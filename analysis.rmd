---
title: "Determinants of phonetic word duration in ten language documentation corpora"
author:
  - "Jan Strunk (University of Cologne)"
  - "Frank Seifart (Dynamique Du Langage, CNRS & Université de Lyon, University of Cologne, Leibniz-Zentrum Allgemeine Sprachwissenschaft)"
  - "Swintha Danielsen (CIHA, Santa Cruz de la Sierra)"
  - "Iren Hartmann (Leipzig University)"
  - "Brigitte Pakendorf (Dynamique Du Langage, CNRS & Université de Lyon)"
  - "Søren Wichmann (Leiden University, Kazan Federal University, Beijing Language University)"
  - "Alena Witzlack-Makarevich (Hebrew University of Jerusalem)"
  - "Balthasar Bickel (University of Zurich)"
date: "`r format(Sys.time(), '%e-%m-%Y')`"
header-includes:
   - \usepackage{amsmath}
   - \usepackage{appendix}
   - \setmainfont{Charis SIL}
output: 
  pdf_document:
    fig_crop: true
    fig_caption: true
    citation_package: natbib
    latex_engine: lualatex
    keep_tex: true
    number_sections: yes
    toc: yes
    toc_depth: 1
    includes:
      in_header: preamble.tex
  tables: true
classoption: leqno
bibliography: words.bib
biblio-style: unified
---

```{r setup, include=FALSE}
# Set KnitR options
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(comment = "")
knitr::opts_chunk$set(fig.pos = "H")
options(width=180, scipen = 999, digits = 7)

# Load various R libraries
library(lme4)
library(effects)
library(ggplot2)
library(dplyr)
library(tidyr)
library(knitr)
options(kableExtra.auto_format = FALSE)
library(kableExtra)
library(extrafont)
library(gtools)
library(reshape2)
library("MuMIn")
library("piecewiseSEM")
library("r2glmm")
library(maps)
library(mapproj)
library(pander)

# Color-blindness friendly palette with gray
cbPalette <- c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

# Color-blindness friendly palette with white and black:
cbbPalette <- c("#FFFFFF","#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

# Load fonts and set options for diagrams
loadfonts()
knit_hooks$set(crop = hook_pdfcrop, pars = function(before, options, envir) {if(before) {par(family=my.font)} else NULL}) # trick for keeping par across chunks; define my.font below!
opts_chunk$set( fig.path='log_figures/',
                dev = 'cairo_pdf', dev.args=list(bg='transparent'),
                fig.height = 7,
                fig.width = 14,
                message = F,
                warning = F,
                family = "CMU Serif",
                autodep=T,
                cache.comments=F,
                crop=T,
                pars=T)

# Set default font
my.font = 'CMU Serif'   

# Set options for ggplot library
theme_set(theme_bw(base_size = 24) +
           theme(text = element_text(family = my.font),
                 plot.background = element_rect(fill = "transparent", colour = NA)
           ))

def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})

# Set default colors for nouns, verbs, and auxiliaries
pos_fill_scale2 = scale_fill_manual(values = c("nouns" = "#1b9e77", "verbs" = "#d95f02", "auxiliaries" = "#7570b3"))
```

```{r functions, echo=FALSE}

# Function to normalize character + diacritic (in UTF-8) for Bora
# so that a word's length can be calculated correctly
normalize_bora = function(words) {

   result = c()
   
   for (cur_word in words) {
   
     cur_word = gsub("\U0268\U0301", "\U0268", cur_word)
     cur_word = gsub("\U0197\U0301", "\U0197", cur_word)
   
     result = c(result, cur_word)
   }
   
   return(result)
}

# Function to normalize character + diacritic (in UTF-8) for Chintang
# so that a word's length can be calculated correctly
normalize_chintang = function(words) {

   result = c()
   
   for (cur_word in words) {
   
     cur_word = gsub("a\U0303", "\U00E3", cur_word)
     cur_word = gsub("e\U0303", "\U1EBD", cur_word)
     cur_word = gsub("i\U0303", "\U0129", cur_word)
     cur_word = gsub("o\U0303", "\U00F5", cur_word)
     cur_word = gsub("u\U0303", "\U0169", cur_word)
     cur_word = gsub("A\U0303", "\U00C3", cur_word)
     cur_word = gsub("E\U0303", "\U1EBC", cur_word)
     cur_word = gsub("I\U0303", "\U0128", cur_word)
     cur_word = gsub("O\U0303", "\U00D5", cur_word)
     cur_word = gsub("U\U0303", "\U0168", cur_word)
     
     result = c(result, cur_word)
   }
   
   return(result)
}

# Function to normalize character + diacritic (in UTF-8) for Hoocąk
# so that a word's length can be calculated correctly
normalize_hoocak = function(words) {

   result = c()
   
   for (cur_word in words) {
   
     cur_word = gsub("a\U0328", "\U0105", cur_word)
     cur_word = gsub("e\U0328", "\U0119", cur_word)
     cur_word = gsub("i\U0328", "\U012F", cur_word)
     cur_word = gsub("o\U0328", "\U01EB", cur_word)
     cur_word = gsub("u\U0328", "\U0173", cur_word)
     cur_word = gsub("A\U0328", "\U0104", cur_word)
     cur_word = gsub("E\U0328", "\U0118", cur_word)
     cur_word = gsub("I\U0328", "\U012E", cur_word)
     cur_word = gsub("O\U0328", "\U01EA", cur_word)
     cur_word = gsub("U\U0328", "\U0172", cur_word)
     cur_word = gsub("g\U030C", "\U01E7", cur_word)
     cur_word = gsub("s\U030C", "\U0161", cur_word)
     cur_word = gsub("z\U030C", "\U017E", cur_word)
     cur_word = gsub("G\U030C", "\U01E6", cur_word)
     cur_word = gsub("S\U030C", "\U0160", cur_word)
     cur_word = gsub("Z\U030C", "\U017D", cur_word)
        
     result = c(result, cur_word)
   }
   
   return(result)
}

# Funtion to delete punctuation characters
delete.final_punctuation = function (word) {
  return(gsub("[-=,?¿¡.:;\"]$", "", word))
}

# Function to calculate standardized coefficients for linear-mixed effects models
# http://stats.stackexchange.com/questions/123366/lmer-standardized-regression-coefficients
lm.beta.lmer <- function(mod) {
   b <- fixef(mod)[-1]
   sd.x <- apply(getME(mod,"X")[,-1],2,sd)
   sd.y <- sd(getME(mod,"y"))
   b*sd.x/sd.y
}
```

```{r data_preparation, echo=FALSE}

# Load original data frame (see data_preparation.txt for how the data was imported into R and further annotated in R)
load("wordtimes.prepared.Rdata")

# Make sure that strings are encoded as factors with sensible ordering of levels
combined.wordtimes.words$language = factor(combined.wordtimes.words$language,levels=c("Baure","Bora","Chintang","Dutch","English","Even","Hoocąk","Nǁng","Sakha","Texistepec"))

# Create disambiguated word types (word type + gloss + word class)
combined.wordtimes.words$word_type2 = paste(tolower(delete.final_punctuation(combined.wordtimes.words$whole_word_morph)),combined.wordtimes.words$whole_word_gloss,combined.wordtimes.words$ntvr_ps_root.simplified,sep="|")

# Only keep roots (which also contain information about the whole word) and discard all other morphemes
data = droplevels(subset(combined.wordtimes.words,subset=ntvr_ps_root.simplified %in% c("N","V","AUX","PRO","OTHER","FLDPS")))

# Make the first column of a table with statistics about the corpora (words per corpus and overall)
complete_data_words = as.vector(by(data$word.id,INDICES=list(data$language),FUN=function (x) length(unique(x))))
complete_data_words = c(complete_data_words,length(unique(data$word.id)))

# Make a separate subcorpus for Chintang (without pear stories, see paper for the reason!)
chintang_nps = droplevels(subset(data,subset=ntvr_language=="ctn" & !grepl("pear",ntvr_file)))

# Number of words in the Chintang corpus (without pear stories)
chintang_nps_words = length(unique(chintang_nps$word.id))

# Collect information about the number of words per annotation unit (au)
annotation_units = data.frame(words=c(complete_data_words,chintang_nps_words))

# Count the number of annotation units per corpus
complete_data_aus = as.vector(by(data$record.id,INDICES=list(data$language),FUN=function (x) length(unique(x))))
complete_data_aus = c(complete_data_aus,length(unique(data$record.id)))
chintang_nps_aus = length(unique(chintang_nps$record.id))

annotation_units$aus = c(complete_data_aus,chintang_nps_aus)

# Calculate the mean number of words per annotation unit for each subcorpus
annotation_units$mean = annotation_units$words / annotation_units$aus

# Check again, using a different method of calculating words per annotation unit
complete_data_aus2 = as.vector(by(data$words_per_record[!duplicated(data$record.id)],INDICES=list(data$language[!duplicated(data$record.id)]),FUN=mean))
complete_data_aus2 = c(complete_data_aus2,mean(data$words_per_record[!duplicated(data$record.id)]))
chintang_nps_aus2 = mean(chintang_nps$words_per_record[!duplicated(chintang_nps$record.id)])

annotation_units$aus2 = c(complete_data_aus2,chintang_nps_aus2)

# Throw out disfluencies
data = subset(data,subset=fldps=="*")

# Throw out words with missing word frequency information (mostly unclear words, count them as disfluencies)
missing_word_freqs = subset(data,subset=is.na(data$word_freq))
data = subset(data,subset=!is.na(data$word_freq))

# Count the number of words in each corpus (after disfluencies have been thrown out)
complete_data_words_no_disfluencies = as.vector(by(data$word.id,INDICES=list(data$language),FUN=function (x) length(unique(x))))
complete_data_words_no_disfluencies = c(complete_data_words_no_disfluencies,length(unique(data$word.id)))

# Reconstruct steps for Chintang (without pear stories)
chintang_nps = droplevels(subset(chintang_nps,subset=!is.na(chintang_nps$word_freq)))
chintang_nps_words_no_disfluencies = length(unique(chintang_nps$word.id))

# Only keep nouns and verbs (exclude known auxiliaries), discard words of other categories (which are not really comparable between languages anyway)
data = subset(data,subset=data$ntvr_ps_root.simplified3 %in% c("N","V"))

# Count the number of words in each corpus (after keeping only nouns and verbs)
complete_data_words_content_words = as.vector(by(data$word.id,INDICES=list(data$language),FUN=function (x) length(unique(x))))
complete_data_words_content_words = c(complete_data_words_content_words,length(unique(data$word.id)))

# Reconstruct steps for Chintang (without pear stories)
chintang_nps = droplevels(subset(chintang_nps,subset=chintang_nps$ntvr_ps_root.simplified3 %in% c("N","V")))
chintang_nps_words_content_words = length(unique(chintang_nps$word.id))

# Throw out words with ambiguous roots (containing both an N and a V root at the same time)
data = subset(data,subset=!word.id %in% with(data,intersect(word.id[ntvr_ps_root.simplified3=="N"],word.id[ntvr_ps_root.simplified3=="V"])))

# Count the number of words (after excluding words containing both a noun and a verb root)
complete_data_words_no_ambiguous = as.vector(by(data$word.id,INDICES=list(data$language),FUN=function (x) length(unique(x))))
complete_data_words_no_ambiguous = c(complete_data_words_no_ambiguous,length(unique(data$word.id)))

# Reconstruct steps for Chintang (without pear stories)
chintang_nps = droplevels(subset(chintang_nps,subset=!word.id %in% with(chintang_nps,intersect(word.id[ntvr_ps_root.simplified3=="N"],word.id[ntvr_ps_root.simplified3=="V"]))))
chintang_nps_words_no_ambiguous = length(unique(chintang_nps$word.id))

# Only keep one entry per word (in case a word contains multiple roots, for example, because it is a compound)
data = subset(data,subset=!duplicated(word.id))

# Count the number of words per corpus (with only one root per word left)
complete_data_words_no_duplicated = as.vector(by(data$word.id,INDICES=list(data$language),FUN=function (x) length(unique(x))))
complete_data_words_no_duplicated = c(complete_data_words_no_duplicated,length(unique(data$word.id)))

# Reconstruct steps for Chintang (without pear stories)
chintang_nps = droplevels(subset(chintang_nps,subset=!duplicated(word.id)))
chintang_nps_words_no_duplicated = length(unique(chintang_nps$word.id))

# Combine the counts of the number of words per corpus at the different steps of excluding data into one table
data_preparation_table = data.frame(language=c(as.character(sort(unique(data$language))),"\\textbf{total}","Chintang (no pear stories)"),complete=c(complete_data_words,chintang_nps_words), no_disfluencies=c(complete_data_words_no_disfluencies,chintang_nps_words_no_disfluencies), content_words=c(complete_data_words_content_words,chintang_nps_words_content_words), no_ambiguous=c(complete_data_words_no_ambiguous,chintang_nps_words_no_ambiguous), no_duplicated=c(complete_data_words_no_duplicated,chintang_nps_words_no_duplicated),stringsAsFactors = FALSE)

# Add columns for 100 most frequent types (to be filled out later)
data_preparation_table$tokens_100 = NA
data_preparation_table$tokens_100_excluded = NA
data_preparation_table$tokens_100_final = NA

# Gather information about average word length in segments and morphemes for each language
# (do this on the complete data, nothing excluded yet, but make sure each word is only counted once)
word.length.data = subset(combined.wordtimes.words,subset=!duplicated(word.id))
word.length.segments.mean = as.vector(by(word.length.data$chars_per_word,INDICES=list(word.length.data$language),FUN=mean))
word.length.segments.sd = as.vector(by(word.length.data$chars_per_word,INDICES=list(word.length.data$language),FUN=sd))
word.length.segments.median = as.vector(by(word.length.data$chars_per_word,INDICES=list(word.length.data$language),FUN=median))
word.length.morphs.mean = as.vector(by(word.length.data$morphs_per_word,INDICES=list(word.length.data$language),FUN=mean))
word.length.morphs.sd = as.vector(by(word.length.data$morphs_per_word,INDICES=list(word.length.data$language),FUN=sd))
word.length.morphs.median = as.vector(by(word.length.data$morphs_per_word,INDICES=list(word.length.data$language),FUN=median))

# Combine the counts about average word length into a table
word.length.table = data.frame(language=unique(word.length.data$language),segments.mean=word.length.segments.mean,segments.sd=word.length.segments.sd,segments.median=word.length.segments.median,morphs.mean=word.length.morphs.mean,morphs.sd=word.length.morphs.sd,morphs.median=word.length.morphs.median)


# Prepapre the data for the individual languages (subcorpora)

# Baure

# Recount frequencies in the Baure subcorpus
baure = droplevels(subset(data,subset=ntvr_language=="brg"))
baure$word_type2 = paste(tolower(delete.final_punctuation(baure$whole_word_morph)),baure$whole_word_gloss,baure$ntvr_ps_root.simplified,sep="|")

# Backup of Baure data for extracting information about the excluded word types
baure_backup = cbind(baure)

# Blacklists for potential auxiliary elements in Baure
baure_aux_blacklist = c("kwe'|exist|V","kwe' =ji|exist =QUOT|V","kwore'|he.is|V","no= ke -wo =ji|3PL= EV -COP =QUOT|V","pi= ke|2SG= EV|V","ri= ke -wo =ji|3SGf= EV -COP =QUOT|V","ri= wo -wo|3SGf= be -COP|V","ro= ke -wo|3SGm= EV -COP|V","ro= ke -wo =ji|3SGm= EV -COP =QUOT|V","ro= wo -wo|3SGm= be -COP|V","wo -no|be -NMLZ|V","wo -no -wo|be -NMLZ -COP|V")

# Additional nominal blacklists
baure_n_blacklist = c()

# Exclude word types from blacklists for Baure
baure = subset(baure,subset=!(word_type2 %in% baure_aux_blacklist))
baure = subset(baure,subset=!(word_type2 %in% baure_n_blacklist))

# Count word frequencies
baure.word.freqs = table(baure$word_type2)

# Convert them to ranks (ties are averaged)
baure.word.freqs.ranks = sort(rank(-baure.word.freqs, ties.method="average"))

# Convert them to dense ranks (ties are broken randomly)
baure.word.freqs.ranks2 = sort(rank(-baure.word.freqs, ties.method="random"))

# Add a words frequency, frequency rank, log frequency, etc. to the original data frame containing the Baure corpus
baure$word_freq = as.numeric(baure.word.freqs[baure$word_type2])
baure$word_freq_rank = as.numeric(baure.word.freqs.ranks[baure$word_type2])
baure$word_freq_rank2 = as.numeric(baure.word.freqs.ranks2[baure$word_type2])

# Calculate a word's relative frequency within the subcorpus
baure$word_rel_freq = baure$word_freq / complete_data_words[1]

# Calculate the natural logarithm of the raw frequency
baure$word_log_freq = log(baure$word_freq)

# Calculate the natural logarithm of the relative frequency
baure$word_log_rel_freq = log(baure$word_rel_freq)

# Calculate the articulation speed within the annotation unit surrounding a word (excluding the word itself) - including silent pauses
baure$speed_per_record2 = (baure$chars_per_record - baure$chars_per_word) / (baure$seconds_per_record - baure$WordLength)

# Calculate the articulation speed within the annotation unit surrounding a word (excluding the word itself) - excluding silent pauses
baure$speed_per_record.no_pauses2 = (baure$chars_per_record - baure$chars_per_word) / (baure$seconds_per_record.no_pauses - baure$WordLength)

# Calculate the natural logarithm of articulation speed - including silent pauses
baure$log_speed_per_record2 = log(baure$speed_per_record2)

# Calculate the natural logarithm of articulation speed - excluding silent pauses
baure$log_speed_per_record.no_pauses2 = log(baure$speed_per_record.no_pauses2)

# Calculate the natural logarithm of a word's length in orthographic characters
baure$log_word_length = log(baure$WordLength)

# Calculate the document frequency for each words, that is, in how many different texts it appears
baure.doc.freqs = sort(margin.table(table(baure$word_type2,baure$ntvr_file) > 0, 1))

# Calculate ranks of document frequencies
baure.doc.freqs.ranks = sort(rank(-baure.doc.freqs, ties.method="average"))

baure$doc_freq = as.numeric(baure.doc.freqs[baure$word_type2])
baure$doc_freq_rank = as.numeric(baure.doc.freqs.ranks[baure$word_type2])

# Recount raw frequencies, relative frequencies, and document frequencies in the backup data (including all words, even those that are later excluded)
baure.word.freqs_backup = table(baure_backup$word_type2)
baure_backup$word_freq = as.numeric(baure.word.freqs_backup[baure_backup$word_type2])
baure_backup$word_rel_freq = baure_backup$word_freq / complete_data_words[1]
baure.doc.freqs_backup = sort(margin.table(table(baure_backup$word_type2,baure_backup$ntvr_file) > 0, 1))
baure_backup$doc_freq = as.numeric(baure.doc.freqs_backup[baure_backup$word_type2])


# Bora

# Recount frequencies in the Bora subcorpus
bora = droplevels(subset(data,subset=ntvr_language=="boa"))
bora$word_type2 = paste(tolower(delete.final_punctuation(bora$whole_word_morph)),bora$whole_word_gloss,bora$ntvr_ps_root.simplified,sep="|")

# Backup of Bora data for extracting information about the excluded word types
bora_backup = cbind(bora)

# Blacklists for potential auxiliary elements in Bora
bora_aux_blacklist = c("ijcya|be|V","ijcya -hi|be -PRED|V","ijcya -híjcya|be -REP|V","ijcya -me|be -PL|V","ijcya -náa|be -SIM|V","ijcya -ne|be -INAN|V","ijcya -nejcu|be -CLF.side|V","ijcya -tu|be -NEG|V","ijcya -vbe|be -M.SG|V","nee -ne|be -INAN|V","nee -vbe|be -M.SG|V")

# Additional nominal blacklists
bora_n_blacklist = c("llíjchuííhyo|Pucunero|N")

# Exclude word types from blacklists for Bora
bora = subset(bora,subset=!(word_type2 %in% bora_aux_blacklist))
bora = subset(bora,subset=!(word_type2 %in% bora_n_blacklist))

# Count word frequencies
bora.word.freqs = table(bora$word_type2)

# Convert them to ranks (ties are averaged)
bora.word.freqs.ranks = sort(rank(-bora.word.freqs, ties.method="average"))

# Convert them to dense ranks (ties are broken randomly)
bora.word.freqs.ranks2 = sort(rank(-bora.word.freqs, ties.method="random"))

# Add a words frequency, frequency rank, log frequency, etc. to the original data frame containing the Bora corpus
bora$word_freq = as.numeric(bora.word.freqs[bora$word_type2])
bora$word_freq_rank = as.numeric(bora.word.freqs.ranks[bora$word_type2])
bora$word_freq_rank2 = as.numeric(bora.word.freqs.ranks2[bora$word_type2])

# Calculate a word's relative frequency within the subcorpus
bora$word_rel_freq = bora$word_freq / complete_data_words[2]

# Calculate the natural logarithm of the raw frequency
bora$word_log_freq = log(bora$word_freq)

# Calculate the natural logarithm of the relative frequency
bora$word_log_rel_freq = log(bora$word_rel_freq)

# Calculate the articulation speed within the annotation unit surrounding a word (excluding the word itself) - including silent pauses
bora$speed_per_record2 = (bora$chars_per_record - bora$chars_per_word) / (bora$seconds_per_record - bora$WordLength)

# Calculate the articulation speed within the annotation unit surrounding a word (excluding the word itself) - excluding silent pauses
bora$speed_per_record.no_pauses2 = (bora$chars_per_record - bora$chars_per_word) / (bora$seconds_per_record.no_pauses - bora$WordLength)

# Calculate the natural logarithm of articulation speed - including silent pauses
bora$log_speed_per_record2 = log(bora$speed_per_record2)

# Calculate the natural logarithm of articulation speed - excluding silent pauses
bora$log_speed_per_record.no_pauses2 = log(bora$speed_per_record.no_pauses2)

# Calculate the natural logarithm of a word's length in orthographic characters
bora$log_word_length = log(bora$WordLength)

# Calculate the document frequency for each words, that is, in how many different texts it appears
bora.doc.freqs = sort(margin.table(table(bora$word_type2,bora$ntvr_file) > 0, 1))

# Calculate ranks of document frequencies
bora.doc.freqs.ranks = sort(rank(-bora.doc.freqs, ties.method="average"))

bora$doc_freq = as.numeric(bora.doc.freqs[bora$word_type2])
bora$doc_freq_rank = as.numeric(bora.doc.freqs.ranks[bora$word_type2])

# Recount raw frequencies, relative frequencies, and document frequencies in the backup data (including all words, even those that are later excluded)
bora.word.freqs_backup = table(bora_backup$word_type2)
bora_backup$word_freq = as.numeric(bora.word.freqs_backup[bora_backup$word_type2])
bora_backup$word_rel_freq = bora_backup$word_freq / complete_data_words[2]
bora.doc.freqs_backup = sort(margin.table(table(bora_backup$word_type2,bora_backup$ntvr_file) > 0, 1))
bora_backup$doc_freq = as.numeric(bora.doc.freqs_backup[bora_backup$word_type2])


# Chintang

# Recount frequencies in the Chintang subcorpus
chintang = droplevels(subset(data,subset=ntvr_language=="ctn"))
chintang$word_type2 = paste(tolower(delete.final_punctuation(chintang$whole_word_morph)),chintang$whole_word_gloss,chintang$ntvr_ps_root.simplified,sep="|")

# Backup of Chintang data for extracting information about the excluded word types
chintang_backup = cbind(chintang)

# Blacklists for potential auxiliary elements in Chintang
chintang_aux_blacklist = c("kond -no|should -IND.NPST|V","lis -e|be -IND.PST|V","lis -e|be -PST|V","lis -i|be -1/2pS/P|V","lis -no|be -IND.NPST|V","par -ne|should -NPST.PTCP|V","yuŋ -a -yakt -e|be -PST -IPFV -IND.PST|V","yuŋ -a -yakt -e|be.there -PST -IPFV -IND.PST|V","yuŋ -a -yakt -e|be -PST -IPFV -PST|V","yuŋ -no|be.there -IND.NPST|V","yuŋ -no|be -IND.NPST|V","yuŋ -no|be -NPST|V")

# Additional nominal blacklists
chintang_n_blacklist = c("chintaŋ|a.place|N")

# Exclude word types from blacklists for Chintang
chintang = subset(chintang,subset=!(word_type2 %in% chintang_aux_blacklist))
chintang = subset(chintang,subset=!(word_type2 %in% chintang_n_blacklist))

# Count word frequencies
chintang.word.freqs = table(chintang$word_type2)

# Convert them to ranks (ties are averaged)
chintang.word.freqs.ranks = sort(rank(-chintang.word.freqs, ties.method="average"))

# Convert them to dense ranks (ties are broken randomly)
chintang.word.freqs.ranks2 = sort(rank(-chintang.word.freqs, ties.method="random"))

# Add a words frequency, frequency rank, log frequency, etc. to the original data frame containing the Chintang corpus
chintang$word_freq = as.numeric(chintang.word.freqs[chintang$word_type2])
chintang$word_freq_rank = as.numeric(chintang.word.freqs.ranks[chintang$word_type2])
chintang$word_freq_rank2 = as.numeric(chintang.word.freqs.ranks2[chintang$word_type2])

# Calculate a word's relative frequency within the subcorpus
chintang$word_rel_freq = chintang$word_freq / complete_data_words[3]

# Calculate the natural logarithm of the raw frequency
chintang$word_log_freq = log(chintang$word_freq)

# Calculate the natural logarithm of the relative frequency
chintang$word_log_rel_freq = log(chintang$word_rel_freq)

# Calculate the articulation speed within the annotation unit surrounding a word (excluding the word itself) - including silent pauses
chintang$speed_per_record2 = (chintang$chars_per_record - chintang$chars_per_word) / (chintang$seconds_per_record - chintang$WordLength)

# Calculate the articulation speed within the annotation unit surrounding a word (excluding the word itself) - excluding silent pauses
chintang$speed_per_record.no_pauses2 = (chintang$chars_per_record - chintang$chars_per_word) / (chintang$seconds_per_record.no_pauses - chintang$WordLength)

# Calculate the natural logarithm of articulation speed - including silent pauses
chintang$log_speed_per_record2 = log(chintang$speed_per_record2)

# Calculate the natural logarithm of articulation speed - excluding silent pauses
chintang$log_speed_per_record.no_pauses2 = log(chintang$speed_per_record.no_pauses2)

# Calculate the natural logarithm of a word's length in orthographic characters
chintang$log_word_length = log(chintang$WordLength)

# Calculate the document frequency for each words, that is, in how many different texts it appears
chintang.doc.freqs = sort(margin.table(table(chintang$word_type2,chintang$ntvr_file) > 0, 1))

# Calculate ranks of document frequencies
chintang.doc.freqs.ranks = sort(rank(-chintang.doc.freqs, ties.method="average"))

chintang$doc_freq = as.numeric(chintang.doc.freqs[chintang$word_type2])
chintang$doc_freq_rank = as.numeric(chintang.doc.freqs.ranks[chintang$word_type2])

# Recount raw frequencies, relative frequencies, and document frequencies in the backup data (including all words, even those that are later excluded)
chintang.word.freqs_backup = table(chintang_backup$word_type2)
chintang_backup$word_freq = as.numeric(chintang.word.freqs_backup[chintang_backup$word_type2])
chintang_backup$word_rel_freq = chintang_backup$word_freq / complete_data_words[3]
chintang.doc.freqs_backup = sort(margin.table(table(chintang_backup$word_type2,chintang_backup$ntvr_file) > 0, 1))
chintang_backup$doc_freq = as.numeric(chintang.doc.freqs_backup[chintang_backup$word_type2])


# Chintang without pear stories

# Create subcorpus of Chintang without pear stories
chintang2 = droplevels(subset(data,subset=ntvr_language=="ctn" & !grepl("pear",ntvr_file)))
chintang2$word_type2 = paste(tolower(delete.final_punctuation(chintang2$whole_word_morph)),chintang2$whole_word_gloss,chintang2$ntvr_ps_root.simplified,sep="|")

# Backup of Chintang data (without pear stories) for extracting information about the excluded word types
chintang2_backup = cbind(chintang2)

# Blacklists for potential auxiliary elements in Chintang
chintang2_aux_blacklist = c("ch -a|be.there -3s|V","kond -no|have.to -IND.NPST|V","kond -no|should -IND.NPST|V","lis|be|V","lis -e|be -IND.PST|V","lis -e|be -PST|V","lis -i|be -1/2pS/P|V","lis -ma|be -INF|V","lis -no|be -NPST|V","lis -no|be -IND.NPST|V","par -ch -a|should -NPST -3s|V","par -ne|should -NPST.PTCP|V","par -y -a|should -PST -3s|V","u- yuŋ -no|3nsS/A- be -NPST|V","yuŋ -a -yakt -e|be -PST -IPFV -IND.PST|V","yuŋ -a -yakt -e|be.there -PST -IPFV -IND.PST|V","yuŋ -a -yakt -e|be -PST -IPFV -PST|V","yuŋ -no|be.there -IND.NPST|V","yuŋ -no|be -IND.NPST|V","yuŋ -no|be -NPST|V")

# Additional nominal blacklists
chintang2_n_blacklist = c("chintaŋ|a.place|N","jalpa|a.kirant.goddess|N","baisak|first.month.of.nepalese.calendar|N")
chintang2_n_blacklist2 = c("whatchamacallit")

# Exclude word types from blacklists for Chintang
chintang2 = subset(chintang2,subset=!word_type2 %in% chintang2_aux_blacklist)
chintang2 = subset(chintang2,subset=!word_type2 %in% chintang2_n_blacklist)
chintang2 = subset(chintang2,subset=!whole_word_gloss %in% chintang2_n_blacklist2)

# Count word frequencies
chintang2.word.freqs = table(chintang2$word_type2)

# Convert them to ranks (ties are averaged)
chintang2.word.freqs.ranks = sort(rank(-chintang2.word.freqs, ties.method="average"))

# Convert them to dense ranks (ties are broken randomly)
chintang2.word.freqs.ranks2 = sort(rank(-chintang2.word.freqs, ties.method="random"))

# Add a words frequency, frequency rank, log frequency, etc. to the original data frame containing the Chintang corpus
chintang2$word_freq = as.numeric(chintang2.word.freqs[chintang2$word_type2])
chintang2$word_freq_rank = as.numeric(chintang2.word.freqs.ranks[chintang2$word_type2])
chintang2$word_freq_rank2 = as.numeric(chintang2.word.freqs.ranks2[chintang2$word_type2])

# Calculate a word's relative frequency within the subcorpus
chintang2$word_rel_freq = chintang2$word_freq / complete_data_words[3]

# Calculate the natural logarithm of the raw frequency
chintang2$word_log_freq = log(chintang2$word_freq)

# Calculate the natural logarithm of the relative frequency
chintang2$word_log_rel_freq = log(chintang2$word_rel_freq)

# Calculate the articulation speed within the annotation unit surrounding a word (excluding the word itself) - including silent pauses
chintang2$speed_per_record2 = (chintang2$chars_per_record - chintang2$chars_per_word) / (chintang2$seconds_per_record - chintang2$WordLength)

# Calculate the articulation speed within the annotation unit surrounding a word (excluding the word itself) - excluding silent pauses
chintang2$speed_per_record.no_pauses2 = (chintang2$chars_per_record - chintang2$chars_per_word) / (chintang2$seconds_per_record.no_pauses - chintang2$WordLength)

# Calculate the natural logarithm of articulation speed - including silent pauses
chintang2$log_speed_per_record2 = log(chintang2$speed_per_record2)

# Calculate the natural logarithm of articulation speed - excluding silent pauses
chintang2$log_speed_per_record.no_pauses2 = log(chintang2$speed_per_record.no_pauses2)

# Calculate the natural logarithm of a word's length in orthographic characters
chintang2$log_word_length = log(chintang2$WordLength)

# Calculate the document frequency for each words, that is, in how many different texts it appears
chintang2.doc.freqs = sort(margin.table(table(chintang2$word_type2,chintang2$ntvr_file) > 0, 1))

# Calculate ranks of document frequencies
chintang2.doc.freqs.ranks = sort(rank(-chintang2.doc.freqs, ties.method="average"))

chintang2$doc_freq = as.numeric(chintang2.doc.freqs[chintang2$word_type2])
chintang2$doc_freq_rank = as.numeric(chintang2.doc.freqs.ranks[chintang2$word_type2])

# Recount raw frequencies, relative frequencies, and document frequencies in the backup data (including all words, even those that are later excluded)
chintang2.word.freqs_backup = table(chintang2_backup$word_type2)
chintang2_backup$word_freq = as.numeric(chintang2.word.freqs_backup[chintang2_backup$word_type2])
chintang2_backup$word_rel_freq = chintang2_backup$word_freq / complete_data_words[3]
chintang2.doc.freqs_backup = sort(margin.table(table(chintang2_backup$word_type2,chintang2_backup$ntvr_file) > 0, 1))
chintang2_backup$doc_freq = as.numeric(chintang2.doc.freqs_backup[chintang2_backup$word_type2])


# Dutch

# Create subcorpus of Dutch
dutch = droplevels(subset(data,subset=ntvr_language=="nld"))
dutch$word_type2 = paste(tolower(delete.final_punctuation(dutch$whole_word_morph)),dutch$whole_word_gloss,dutch$ntvr_ps_root.simplified,sep="|")

# Backup of Dutch for extracting information about the excluded word types
dutch_backup = cbind(dutch)

# Blacklists for potential auxiliary elements in Dutch
dutch_aux_blacklist = c("ben|WW(pv,tgw,ev)|V","bent|WW(pv,tgw,met-t)|V","doe|WW(pv,tgw,ev)|V","doen|WW(inf,vrij,zonder)|V","doen|WW(pv,tgw,mv)|V","doet|WW(pv,tgw,met-t)|V","gedaan|WW(vd,vrij,zonder)|V","gehad|WW(vd,vrij,zonder)|V","geweest|WW(vd,vrij,zonder)|V","had|WW(pv,verl,ev)|V","hadden|WW(pv,verl,mv)|V","heb|WW(pv,tgw,ev)|V","hebben|WW(pv,tgw,mv)|V","hebben|WW(inf,vrij,zonder)|V","hebt|WW(pv,tgw,met-t)|V","heeft|WW(pv,tgw,met-t)|V","hoeft|WW(pv,tgw,met-t)|V","is|WW(pv,tgw,ev)|V","kan|WW(pv,tgw,ev)|V","kun|WW(pv,tgw,ev)|V","kunnen|WW(pv,tgw,mv)|V","kunnen|WW(inf,vrij,zonder)|V","kon|WW(pv,verl,ev)|V","kunt|WW(pv,tgw,met-t)|V","mag|WW(pv,tgw,ev)|V","moest|WW(pv,verl,ev)|V","moesten|WW(pv,verl,mv)|V","moet|WW(pv,tgw,ev)|V","moeten|WW(pv,tgw,mv)|V","moeten|WW(inf,vrij,zonder)|V","'s|WW(pv,tgw,ev)|V","waren|WW(pv,verl,mv)|V","was|WW(pv,verl,ev)|V","wil|WW(pv,tgw,ev)|V","willen|WW(pv,tgw,mv)|V","worden|WW(inf,vrij,zonder)|V","worden|WW(pv,tgw,mv)|V","wordt|WW(pv,tgw,met-t)|V","zal|WW(pv,tgw,ev)|V","zijn|WW(pv,tgw,mv)|V","zijn|WW(inf,vrij,zonder)|V","zou|WW(pv,verl,ev)|V","zullen|WW(pv,tgw,mv)|V")

# Additional nominal blacklists
dutch_n_blacklist = c()

# Exclude word types from blacklists for Dutch
dutch = subset(dutch,subset=!(word_type2 %in% dutch_aux_blacklist))
dutch = subset(dutch,subset=!(word_type2 %in% dutch_n_blacklist))

# Count word frequencies
dutch.word.freqs = table(dutch$word_type2)

# Convert them to ranks (ties are averaged)
dutch.word.freqs.ranks = sort(rank(-dutch.word.freqs, ties.method="average"))

# Convert them to dense ranks (ties are broken randomly)
dutch.word.freqs.ranks2 = sort(rank(-dutch.word.freqs, ties.method="random"))

# Add a words frequency, frequency rank, log frequency, etc. to the original data frame containing the Dutch corpus
dutch$word_freq = as.numeric(dutch.word.freqs[dutch$word_type2])
dutch$word_freq_rank = as.numeric(dutch.word.freqs.ranks[dutch$word_type2])
dutch$word_freq_rank2 = as.numeric(dutch.word.freqs.ranks2[dutch$word_type2])

# Calculate a word's relative frequency within the subcorpus
dutch$word_rel_freq = dutch$word_freq / complete_data_words[4]

# Calculate the natural logarithm of the raw frequency
dutch$word_log_freq = log(dutch$word_freq)

# Calculate the natural logarithm of the relative frequency
dutch$word_log_rel_freq = log(dutch$word_rel_freq)

# Calculate the articulation speed within the annotation unit surrounding a word (excluding the word itself) - including silent pauses
dutch$speed_per_record2 = (dutch$chars_per_record - dutch$chars_per_word) / (dutch$seconds_per_record - dutch$WordLength)

# Calculate the articulation speed within the annotation unit surrounding a word (excluding the word itself) - excluding silent pauses
dutch$speed_per_record.no_pauses2 = (dutch$chars_per_record - dutch$chars_per_word) / (dutch$seconds_per_record.no_pauses - dutch$WordLength)

# Calculate the natural logarithm of articulation speed - including silent pauses
dutch$log_speed_per_record2 = log(dutch$speed_per_record2)

# Calculate the natural logarithm of articulation speed - excluding silent pauses
dutch$log_speed_per_record.no_pauses2 = log(dutch$speed_per_record.no_pauses2)

# Calculate the natural logarithm of a word's length in orthographic characters
dutch$log_word_length = log(dutch$WordLength)

# Calculate the document frequency for each words, that is, in how many different texts it appears
dutch.doc.freqs = sort(margin.table(table(dutch$word_type2,dutch$ntvr_file) > 0, 1))

# Calculate ranks of document frequencies
dutch.doc.freqs.ranks = sort(rank(-dutch.doc.freqs, ties.method="average"))

dutch$doc_freq = as.numeric(dutch.doc.freqs[dutch$word_type2])
dutch$doc_freq_rank = as.numeric(dutch.doc.freqs.ranks[dutch$word_type2])

# Recount raw frequencies, relative frequencies, and document frequencies in the backup data (including all words, even those that are later excluded)
dutch.word.freqs_backup = table(dutch_backup$word_type2)
dutch_backup$word_freq = as.numeric(dutch.word.freqs_backup[dutch_backup$word_type2])
dutch_backup$word_rel_freq = dutch_backup$word_freq / complete_data_words[4]
dutch.doc.freqs_backup = sort(margin.table(table(dutch_backup$word_type2,dutch_backup$ntvr_file) > 0, 1))
dutch_backup$doc_freq = as.numeric(dutch.doc.freqs_backup[dutch_backup$word_type2])


# English

# Create subcorpus of English
english =  droplevels(subset(data,subset=ntvr_language=="eng"))
english$word_type2 = paste(tolower(delete.final_punctuation(english$whole_word_morph)),english$whole_word_gloss,english$ntvr_ps_root.simplified,sep="|")

# Backup of English for extracting information about the excluded word types
english_backup = cbind(english)

# Blacklists for potential auxiliary elements in English
english_aux_blacklist = c("am|be.PRES|V","are|be.PRES|V","be|be.INF|V","been|be.PPART|V","be -ing|be -PROG|V","did|do.PAST|V","do|do.INF|V","does|do.3sg.PRES|V","get|get.INF|V","got|get.PAST|V","get -ting|get -PROG|V","had|have.PAST|V","has|have.3sg.PRES|V","have|have.INF|V","is|be.3sg.PRES|V","was|be.PAST|V","were|be.PAST|V","'m|be.PRES|V","'re|be.PRES|V","'ve|have.INF|V")

# Additional nominal blacklists
english_n_blacklist = c("dallas|dallas|N","i|i|N","it|it|N","t|t|N","texas|texas|N")

# Exclude word types from blacklists for English
english = subset(english,subset=!(word_type2 %in% english_aux_blacklist))
english = subset(english,subset=!(word_type2 %in% english_n_blacklist))

# Count word frequencies
english.word.freqs = table(english$word_type2)

# Convert them to ranks (ties are averaged)
english.word.freqs.ranks = sort(rank(-english.word.freqs, ties.method="average"))

# Convert them to dense ranks (ties are broken randomly)
english.word.freqs.ranks2 = sort(rank(-english.word.freqs, ties.method="random"))

# Add a words frequency, frequency rank, log frequency, etc. to the original data frame containing the English corpus
english$word_freq = as.numeric(english.word.freqs[english$word_type2])
english$word_freq_rank = as.numeric(english.word.freqs.ranks[english$word_type2])
english$word_freq_rank2 = as.numeric(english.word.freqs.ranks2[english$word_type2])

# Calculate a word's relative frequency within the subcorpus
english$word_rel_freq = english$word_freq / complete_data_words[5]

# Calculate the natural logarithm of the raw frequency
english$word_log_freq = log(english$word_freq)

# Calculate the natural logarithm of the relative frequency
english$word_log_rel_freq = log(english$word_rel_freq)

# Calculate the articulation speed within the annotation unit surrounding a word (excluding the word itself) - including silent pauses
english$speed_per_record2 = (english$chars_per_record - english$chars_per_word) / (english$seconds_per_record - english$WordLength)

# Calculate the articulation speed within the annotation unit surrounding a word (excluding the word itself) - excluding silent pauses
english$speed_per_record.no_pauses2 = (english$chars_per_record - english$chars_per_word) / (english$seconds_per_record.no_pauses - english$WordLength)

# Calculate the natural logarithm of articulation speed - including silent pauses
english$log_speed_per_record2 = log(english$speed_per_record2)

# Calculate the natural logarithm of articulation speed - excluding silent pauses
english$log_speed_per_record.no_pauses2 = log(english$speed_per_record.no_pauses2)

# Calculate the natural logarithm of a word's length in orthographic characters
english$log_word_length = log(english$WordLength)

# Calculate the document frequency for each words, that is, in how many different texts it appears
english.doc.freqs = sort(margin.table(table(english$word_type2,english$ntvr_file) > 0, 1))

# Calculate ranks of document frequencies
english.doc.freqs.ranks = sort(rank(-english.doc.freqs, ties.method="average"))

english$doc_freq = as.numeric(english.doc.freqs[english$word_type2])
english$doc_freq_rank = as.numeric(english.doc.freqs.ranks[english$word_type2])

# Recount raw frequencies, relative frequencies, and document frequencies in the backup data (including all words, even those that are later excluded)
english.word.freqs_backup = table(english_backup$word_type2)
english_backup$word_freq = as.numeric(english.word.freqs_backup[english_backup$word_type2])
english_backup$word_rel_freq = english_backup$word_freq / complete_data_words[5]
english.doc.freqs_backup = sort(margin.table(table(english_backup$word_type2,english_backup$ntvr_file) > 0, 1))
english_backup$doc_freq = as.numeric(english.doc.freqs_backup[english_backup$word_type2])


# Even

# Create subcorpus of Even
even = droplevels(subset(data,subset=ntvr_language=="eve"))
even$word_type2 = paste(tolower(delete.final_punctuation(even$whole_word_morph)),even$whole_word_gloss,even$ntvr_ps_root.simplified,sep="|")

# Backup of Even for extracting information about the excluded word types
even_backup = cbind(even)

# Blacklists for potential auxiliary elements in Even
even_aux_blacklist = c("bi -če|be -pf.ptc|V","bi -če -dʒi -n(i)|be -pf.ptc -fut -3sg|V","bi -če -l|be -pf.ptc -pl|V","bi -de -n(i)|be -purp.cvb -poss.3sg|V","bi -dʒi-n(i)|be -fut -3sg|V","bi -gr(e) -n(i)|be -hab -3sg|V","bi -gr(e) -če|be -hab -pf.ptc|V","bi -ŋsi -j|be -impf.cvb -prfl.sg|V","bi -r|be -neg.cvb|V","bi -r(e)|be -nonfut|V","bi -rek -e -n(i)|be -cond.cvb -ep -poss.3sg|V","bi -r(e) -m|be -nonfut -1sg|V","bi -r(e) -n(i)|be -nonfut -3sg|V","bi -r(e) -nri|be -nonfut -2sg|V","bi -r(e) -p|be -nonfut -1pl.in|V","bi -ri -n(i)|be -pst -poss.3sg|V","bi -ri -ten|be -pst -poss.3pl|V","bi -ri -t(i)|be -pst -poss.1pl.in|V","bi -ri -w|be -pst -poss.1sg|V","bi -weːč -ri -w|be -gnr -pst -poss.1sg|V","ọː -če|become -pf.ptc|V","ọː -če -l|become -pf.ptc -pl|V","ọː -dʒi -n(i)|become -fut -3sg|V","ọː -n(i)|become -3sg|V","ọː -niken|become -sim.cvb|V","ọː -rek -e -n(i)|become -cond.cvb -ep -poss.3sg|V")

# Additional nominal blacklists
even_n_blacklist = c("bọčịlịkan|Bochilikan|N","omčeni|Emcheni|N","stepanov|Stepanov|N")

# Exclude word types from blacklists for Even
even = subset(even,subset=!(word_type2 %in% even_aux_blacklist))
even = subset(even,subset=!(word_type2 %in% even_n_blacklist))

# Count word frequencies
even.word.freqs = table(even$word_type2)

# Convert them to ranks (ties are averaged)
even.word.freqs.ranks = sort(rank(-even.word.freqs, ties.method="average"))

# Convert them to dense ranks (ties are broken randomly)
even.word.freqs.ranks2 = sort(rank(-even.word.freqs, ties.method="random"))

# Add a words frequency, frequency rank, log frequency, etc. to the original data frame containing the Even corpus
even$word_freq = as.numeric(even.word.freqs[even$word_type2])
even$word_freq_rank = as.numeric(even.word.freqs.ranks[even$word_type2])
even$word_freq_rank2 = as.numeric(even.word.freqs.ranks2[even$word_type2])

# Calculate a word's relative frequency within the subcorpus
even$word_rel_freq = even$word_freq / complete_data_words[6]

# Calculate the natural logarithm of the raw frequency
even$word_log_freq = log(even$word_freq)

# Calculate the natural logarithm of the relative frequency
even$word_log_rel_freq = log(even$word_rel_freq)

# Calculate the articulation speed within the annotation unit surrounding a word (excluding the word itself) - including silent pauses
even$speed_per_record2 = (even$chars_per_record - even$chars_per_word) / (even$seconds_per_record - even$WordLength)

# Calculate the articulation speed within the annotation unit surrounding a word (excluding the word itself) - excluding silent pauses
even$speed_per_record.no_pauses2 = (even$chars_per_record - even$chars_per_word) / (even$seconds_per_record.no_pauses - even$WordLength)

# Calculate the natural logarithm of articulation speed - including silent pauses
even$log_speed_per_record2 = log(even$speed_per_record2)

# Calculate the natural logarithm of articulation speed - excluding silent pauses
even$log_speed_per_record.no_pauses2 = log(even$speed_per_record.no_pauses2)

# Calculate the natural logarithm of a word's length in orthographic characters
even$log_word_length = log(even$WordLength)

# Calculate the document frequency for each words, that is, in how many different texts it appears
even.doc.freqs = sort(margin.table(table(even$word_type2,even$ntvr_file) > 0, 1))

# Calculate ranks of document frequencies
even.doc.freqs.ranks = sort(rank(-even.doc.freqs, ties.method="average"))

even$doc_freq = as.numeric(even.doc.freqs[even$word_type2])
even$doc_freq_rank = as.numeric(even.doc.freqs.ranks[even$word_type2])

# Recount raw frequencies, relative frequencies, and document frequencies in the backup data (including all words, even those that are later excluded)
even.word.freqs_backup = table(even_backup$word_type2)
even_backup$word_freq = as.numeric(even.word.freqs_backup[even_backup$word_type2])
even_backup$word_rel_freq = even_backup$word_freq / complete_data_words[6]
even.doc.freqs_backup = sort(margin.table(table(even_backup$word_type2,even_backup$ntvr_file) > 0, 1))
even_backup$doc_freq = as.numeric(even.doc.freqs_backup[even_backup$word_type2])


# Hoocąk

# Create subcorpus of Hoocąk
hoocak = droplevels(subset(data,subset=ntvr_language=="win"))
hoocak$word_type2 = paste(tolower(delete.final_punctuation(hoocak$whole_word_morph)),hoocak$whole_word_gloss,hoocak$ntvr_ps_root.simplified,sep="|")

# Backup of Hoocąk for extracting information about the excluded word types
hoocak_backup = cbind(hoocak)

# Blacklists for potential auxiliary elements in Hoocąk
hoocak_aux_blacklist = c("haa|make/CAUS(OBJ.3SG)\\1E.A|V","haa=ra|have.kin(OBJ.3SG)\\1E.A=NMLZ|V","haa=ra|have.kin\\1E.A=NMLZ|V","hii|make/CAUS(SBJ.3SG&OBJ.3SG)|V","hii-ire|make/CAUS(OBJ.3SG)-SBJ.3PL|V","wa-haa=ra|OBJ.3PL-have.kin\\1E.A=NMLZ|V","wa-hii|OBJ.3PL-make/CAUS(SBJ.3SG)|V")

hoocak_aux_blacklist2 = c("1E.A-be/PROG","have.NTL(SBJ.3SG&OBJ.3SG)","OBJ.3PL-have.NTL\\1E.A-PL=NMLZ")

# Additional nominal blacklists
hoocak_n_blacklist = c()

# Exclude word types from blacklists for Hoocąk
hoocak = subset(hoocak,subset=!(word_type2 %in% hoocak_aux_blacklist))

# Problems with character encoding during matching
hoocak = subset(hoocak,subset=!(ntvr_ps_root.simplified3=="V" & whole_word_gloss %in% hoocak_aux_blacklist2))
hoocak = subset(hoocak,subset=!(word_type2 %in% hoocak_n_blacklist))

# Count word frequencies
hoocak.word.freqs = table(hoocak$word_type2)

# Convert them to ranks (ties are averaged)
hoocak.word.freqs.ranks = sort(rank(-hoocak.word.freqs, ties.method="average"))

# Convert them to dense ranks (ties are broken randomly)
hoocak.word.freqs.ranks2 = sort(rank(-hoocak.word.freqs, ties.method="random"))

# Add a words frequency, frequency rank, log frequency, etc. to the original data frame containing the Hoocąk corpus
hoocak$word_freq = as.numeric(hoocak.word.freqs[hoocak$word_type2])
hoocak$word_freq_rank = as.numeric(hoocak.word.freqs.ranks[hoocak$word_type2])
hoocak$word_freq_rank2 = as.numeric(hoocak.word.freqs.ranks2[hoocak$word_type2])

# Calculate a word's relative frequency within the subcorpus
hoocak$word_rel_freq = hoocak$word_freq / complete_data_words[7]

# Calculate the natural logarithm of the raw frequency
hoocak$word_log_freq = log(hoocak$word_freq)

# Calculate the natural logarithm of the relative frequency
hoocak$word_log_rel_freq = log(hoocak$word_rel_freq)

# Calculate the articulation speed within the annotation unit surrounding a word (excluding the word itself) - including silent pauses
hoocak$speed_per_record2 = (hoocak$chars_per_record - hoocak$chars_per_word) / (hoocak$seconds_per_record - hoocak$WordLength)

# Calculate the articulation speed within the annotation unit surrounding a word (excluding the word itself) - excluding silent pauses
hoocak$speed_per_record.no_pauses2 = (hoocak$chars_per_record - hoocak$chars_per_word) / (hoocak$seconds_per_record.no_pauses - hoocak$WordLength)

# Calculate the natural logarithm of articulation speed - including silent pauses
hoocak$log_speed_per_record2 = log(hoocak$speed_per_record2)

# Calculate the natural logarithm of articulation speed - excluding silent pauses
hoocak$log_speed_per_record.no_pauses2 = log(hoocak$speed_per_record.no_pauses2)

# Calculate the natural logarithm of a word's length in orthographic characters
hoocak$log_word_length = log(hoocak$WordLength)

# Calculate the document frequency for each words, that is, in how many different texts it appears
hoocak.doc.freqs = sort(margin.table(table(hoocak$word_type2,hoocak$ntvr_file) > 0, 1))

# Calculate ranks of document frequencies
hoocak.doc.freqs.ranks = sort(rank(-hoocak.doc.freqs, ties.method="average"))

hoocak$doc_freq = as.numeric(hoocak.doc.freqs[hoocak$word_type2])
hoocak$doc_freq_rank = as.numeric(hoocak.doc.freqs.ranks[hoocak$word_type2])

# Recount raw frequencies, relative frequencies, and document frequencies in the backup data (including all words, even those that are later excluded)
hoocak.word.freqs_backup = table(hoocak_backup$word_type2)
hoocak_backup$word_freq = as.numeric(hoocak.word.freqs_backup[hoocak_backup$word_type2])
hoocak_backup$word_rel_freq = hoocak_backup$word_freq / complete_data_words[7]
hoocak.doc.freqs_backup = sort(margin.table(table(hoocak_backup$word_type2,hoocak_backup$ntvr_file) > 0, 1))
hoocak_backup$doc_freq = as.numeric(hoocak.doc.freqs_backup[hoocak_backup$word_type2])


# Nǁng

# Create subcorpus of Nǁng
nuu = droplevels(subset(data,subset=ntvr_language=="ngh"))
nuu$word_type2 = paste(tolower(delete.final_punctuation(nuu$whole_word_morph)),nuu$whole_word_gloss,nuu$ntvr_ps_root.simplified,sep="|")

# Backup of Nǁng for extracting information about the excluded word types
nuu_backup = cbind(nuu)

# Blacklists for potential auxiliary elements in Nǁng
nuu_aux_blacklist = c()

# Additional nominal blacklists
nuu_n_blacklist = c()

# Exclude word types from blacklists for Nǁng
nuu = subset(nuu,subset=!(word_type2 %in% nuu_aux_blacklist))
nuu = subset(nuu,subset=!(word_type2 %in% nuu_n_blacklist))

# Count word frequencies
nuu.word.freqs = table(nuu$word_type2)

# Convert them to ranks (ties are averaged)
nuu.word.freqs.ranks = sort(rank(-nuu.word.freqs, ties.method="average"))

# Convert them to dense ranks (ties are broken randomly)
nuu.word.freqs.ranks2 = sort(rank(-nuu.word.freqs, ties.method="random"))

# Add a words frequency, frequency rank, log frequency, etc. to the original data frame containing the Hoocąk corpus
nuu$word_freq = as.numeric(nuu.word.freqs[nuu$word_type2])
nuu$word_freq_rank = as.numeric(nuu.word.freqs.ranks[nuu$word_type2])
nuu$word_freq_rank2 = as.numeric(nuu.word.freqs.ranks2[nuu$word_type2])

# Calculate a word's relative frequency within the subcorpus
nuu$word_rel_freq = nuu$word_freq / complete_data_words[8]

# Calculate the natural logarithm of the raw frequency
nuu$word_log_freq = log(nuu$word_freq)

# Calculate the natural logarithm of the relative frequency
nuu$word_log_rel_freq = log(nuu$word_rel_freq)

# Calculate the articulation speed within the annotation unit surrounding a word (excluding the word itself) - including silent pauses
nuu$speed_per_record2 = (nuu$chars_per_record - nuu$chars_per_word) / (nuu$seconds_per_record - nuu$WordLength)

# Calculate the articulation speed within the annotation unit surrounding a word (excluding the word itself) - excluding silent pauses
nuu$speed_per_record.no_pauses2 = (nuu$chars_per_record - nuu$chars_per_word) / (nuu$seconds_per_record.no_pauses - nuu$WordLength)

# Calculate the natural logarithm of articulation speed - including silent pauses
nuu$log_speed_per_record2 = log(nuu$speed_per_record2)

# Calculate the natural logarithm of articulation speed - excluding silent pauses
nuu$log_speed_per_record.no_pauses2 = log(nuu$speed_per_record.no_pauses2)

# Calculate the natural logarithm of a word's length in orthographic characters
nuu$log_word_length = log(nuu$WordLength)

# Calculate the document frequency for each words, that is, in how many different texts it appears
nuu.doc.freqs = sort(margin.table(table(nuu$word_type2,nuu$ntvr_file) > 0, 1))

# Calculate ranks of document frequencies
nuu.doc.freqs.ranks = sort(rank(-nuu.doc.freqs, ties.method="average"))

nuu$doc_freq = as.numeric(nuu.doc.freqs[nuu$word_type2])
nuu$doc_freq_rank = as.numeric(nuu.doc.freqs.ranks[nuu$word_type2])

# Recount raw frequencies, relative frequencies, and document frequencies in the backup data (including all words, even those that are later excluded)
nuu.word.freqs_backup = table(nuu_backup$word_type2)
nuu_backup$word_freq = as.numeric(nuu.word.freqs_backup[nuu_backup$word_type2])
nuu_backup$word_rel_freq = nuu_backup$word_freq / complete_data_words[8]
nuu.doc.freqs_backup = sort(margin.table(table(nuu_backup$word_type2,nuu_backup$ntvr_file) > 0, 1))
nuu_backup$doc_freq = as.numeric(nuu.doc.freqs_backup[nuu_backup$word_type2])


# Sakha

# Create subcorpus of Sakha
sakha = droplevels(subset(data,subset=ntvr_language=="sah"))
sakha$word_type2 = paste(tolower(delete.final_punctuation(sakha$whole_word_morph)),sakha$whole_word_gloss,sakha$ntvr_ps_root.simplified,sep="|")

# Backup of Sakha for extracting information about the excluded word types
sakha_backup = cbind(sakha)

# Blacklists for potential auxiliary elements in Sakha
sakha_aux_blacklist = c("baːr|existence|V","baːr -lar|existence -PL|V","buol -a|AUX -IPF.CVB|V","buol -an|AUX -PF.CVB|V","buol -ar|AUX -PRSPT|V","buol -ia|AUX -FUT.3SG|V","buol -ta|AUX -PST.3SG|V","buol -tar|AUX -COND|V","buol -taχ -ina|AUX -MDL -COND.3SG|V","buol -taχ -(t)a|AUX -MDL -POSS.3SG|V","er -taχ -pina|be -MDL -COND.1SG|V","e -bit|be -PSTPT|V","e -bit -(t)a|be -PSTPT -POSS.3SG|V","e -ta|be -PST.3SG|V","e -ti -bit|be -PST -1PL|V","e -ti -(i)m|be -PST -POSS.1SG|V","kebis -ar -lar|PFV -PRSPT -PL|V","kïaj -an|be.able -PF.CVB|V","naːda|have.to.R|V","suoχ|non-existence|V","suoχ -(t)a|non-existence -POSS.3SG|V")

# Additional nominal blacklists
sakha_n_blacklist = c("ʤokuːskaj -ga|Y. -DAT|N")

# Exclude word types from blacklists for Sakha
sakha = subset(sakha,subset=!(word_type2 %in% sakha_aux_blacklist))
sakha = subset(sakha,subset=!(word_type2 %in% sakha_n_blacklist))

# Count word frequencies
sakha.word.freqs = table(sakha$word_type2)

# Convert them to ranks (ties are averaged)
sakha.word.freqs.ranks = sort(rank(-sakha.word.freqs, ties.method="average"))

# Convert them to dense ranks (ties are broken randomly)
sakha.word.freqs.ranks2 = sort(rank(-sakha.word.freqs, ties.method="random"))

# Add a words frequency, frequency rank, log frequency, etc. to the original data frame containing the Hoocąk corpus
sakha$word_freq = as.numeric(sakha.word.freqs[sakha$word_type2])
sakha$word_freq_rank = as.numeric(sakha.word.freqs.ranks[sakha$word_type2])
sakha$word_freq_rank2 = as.numeric(sakha.word.freqs.ranks2[sakha$word_type2])

# Calculate a word's relative frequency within the subcorpus
sakha$word_rel_freq = sakha$word_freq / complete_data_words[10]

# Calculate the natural logarithm of the raw frequency
sakha$word_log_freq = log(sakha$word_freq)

# Calculate the natural logarithm of the relative frequency
sakha$word_log_rel_freq = log(sakha$word_rel_freq)

# Calculate the articulation speed within the annotation unit surrounding a word (excluding the word itself) - including silent pauses
sakha$speed_per_record2 = (sakha$chars_per_record - sakha$chars_per_word) / (sakha$seconds_per_record - sakha$WordLength)

# Calculate the articulation speed within the annotation unit surrounding a word (excluding the word itself) - excluding silent pauses
sakha$speed_per_record.no_pauses2 = (sakha$chars_per_record - sakha$chars_per_word) / (sakha$seconds_per_record.no_pauses - sakha$WordLength)

# Calculate the natural logarithm of articulation speed - including silent pauses
sakha$log_speed_per_record2 = log(sakha$speed_per_record2)

# Calculate the natural logarithm of articulation speed - excluding silent pauses
sakha$log_speed_per_record.no_pauses2 = log(sakha$speed_per_record.no_pauses2)

# Calculate the natural logarithm of a word's length in orthographic characters
sakha$log_word_length = log(sakha$WordLength)

# Calculate the document frequency for each words, that is, in how many different texts it appears
sakha.doc.freqs = sort(margin.table(table(sakha$word_type2,sakha$ntvr_file) > 0, 1))

# Calculate ranks of document frequencies
sakha.doc.freqs.ranks = sort(rank(-sakha.doc.freqs, ties.method="average"))

sakha$doc_freq = as.numeric(sakha.doc.freqs[sakha$word_type2])
sakha$doc_freq_rank = as.numeric(sakha.doc.freqs.ranks[sakha$word_type2])

# Recount raw frequencies, relative frequencies, and document frequencies in the backup data (including all words, even those that are later excluded)
sakha.word.freqs_backup = table(sakha_backup$word_type2)
sakha_backup$word_freq = as.numeric(sakha.word.freqs_backup[sakha_backup$word_type2])
sakha_backup$word_rel_freq = sakha_backup$word_freq / complete_data_words[10]
sakha.doc.freqs_backup = sort(margin.table(table(sakha_backup$word_type2,sakha_backup$ntvr_file) > 0, 1))
sakha_backup$doc_freq = as.numeric(sakha.doc.freqs_backup[sakha_backup$word_type2])


# Texistepec Popoluca

# Create subcorpus of Texistepec Popoluca
texistepec = droplevels(subset(data,subset=ntvr_language=="poq"))
texistepec$word_type2 = paste(tolower(delete.final_punctuation(texistepec$whole_word_morph)),texistepec$whole_word_gloss,texistepec$ntvr_ps_root.simplified,sep="|")

# Backup of Texistepec Popoluca for extracting information about the excluded word types
texistepec_backup = cbind(texistepec)

# Blacklists for potential auxiliary elements in Texistepec Popoluca
texistepec_aux_blacklist = c("'êch -e'|estar -DEP|V","k- 'êch|1:B- estar|V","n- 'êch -da'|1/3- estar -CAUS|V","ny- 'êch -da'|2/3- estar -CAUS|V","ø- 'êch|3:B- estar|V","y- 'êch|3:A- estar|V","y- 'êch -da'|3/3- estar -CAUS|V","y- 'êch -da' -pä'|3/3- estar -CAUS -REL|V","ø- 'êch -yaj -e'|3:B- estar -3:PL -DEP|V")

# Additional nominal blacklists
texistepec_n_blacklist = c("flojo|Flojo|N","jwaŋ|Juan|N","y- wêñ|3:A- REFL|N")

# Exclude word types from blacklists for Texistepec Popoluca
texistepec = subset(texistepec,subset=!(word_type2 %in% texistepec_aux_blacklist))
texistepec = subset(texistepec,subset=!(word_type2 %in% texistepec_n_blacklist))

# Count word frequencies
texistepec.word.freqs = table(texistepec$word_type2)

# Convert them to ranks (ties are averaged)
texistepec.word.freqs.ranks = sort(rank(-texistepec.word.freqs, ties.method="average"))

# Convert them to dense ranks (ties are broken randomly)
texistepec.word.freqs.ranks2 = sort(rank(-texistepec.word.freqs, ties.method="random"))

# Add a words frequency, frequency rank, log frequency, etc. to the original data frame containing the Hoocąk corpus
texistepec$word_freq = as.numeric(texistepec.word.freqs[texistepec$word_type2])
texistepec$word_freq_rank = as.numeric(texistepec.word.freqs.ranks[texistepec$word_type2])
texistepec$word_freq_rank2 = as.numeric(texistepec.word.freqs.ranks2[texistepec$word_type2])

# Calculate a word's relative frequency within the subcorpus
texistepec$word_rel_freq = texistepec$word_freq / complete_data_words[9]

# Calculate the natural logarithm of the raw frequency
texistepec$word_log_freq = log(texistepec$word_freq)

# Calculate the natural logarithm of the relative frequency
texistepec$word_log_rel_freq = log(texistepec$word_rel_freq)

# Calculate the articulation speed within the annotation unit surrounding a word (excluding the word itself) - including silent pauses
texistepec$speed_per_record2 = (texistepec$chars_per_record - texistepec$chars_per_word) / (texistepec$seconds_per_record - texistepec$WordLength)

# Calculate the articulation speed within the annotation unit surrounding a word (excluding the word itself) - excluding silent pauses
texistepec$speed_per_record.no_pauses2 = (texistepec$chars_per_record - texistepec$chars_per_word) / (texistepec$seconds_per_record.no_pauses - texistepec$WordLength)

# Calculate the natural logarithm of articulation speed - including silent pauses
texistepec$log_speed_per_record2 = log(texistepec$speed_per_record2)

# Calculate the natural logarithm of articulation speed - excluding silent pauses
texistepec$log_speed_per_record.no_pauses2 = log(texistepec$speed_per_record.no_pauses2)

# Calculate the natural logarithm of a word's length in orthographic characters
texistepec$log_word_length = log(texistepec$WordLength)

# Calculate the document frequency for each words, that is, in how many different texts it appears
texistepec.doc.freqs = sort(margin.table(table(texistepec$word_type2,texistepec$ntvr_file) > 0, 1))

# Calculate ranks of document frequencies
texistepec.doc.freqs.ranks = sort(rank(-texistepec.doc.freqs, ties.method="average"))

texistepec$doc_freq = as.numeric(texistepec.doc.freqs[texistepec$word_type2])
texistepec$doc_freq_rank = as.numeric(texistepec.doc.freqs.ranks[texistepec$word_type2])

# Recount raw frequencies, relative frequencies, and document frequencies in the backup data (including all words, even those that are later excluded)
texistepec.word.freqs_backup = table(texistepec_backup$word_type2)
texistepec_backup$word_freq = as.numeric(texistepec.word.freqs_backup[texistepec_backup$word_type2])
texistepec_backup$word_rel_freq = texistepec_backup$word_freq / complete_data_words[9]
texistepec.doc.freqs_backup = sort(margin.table(table(texistepec_backup$word_type2,texistepec_backup$ntvr_file) > 0, 1))
texistepec_backup$doc_freq = as.numeric(texistepec.doc.freqs_backup[texistepec_backup$word_type2])


# Prepare data structures for results of bivariate tests
results.word_duration = expand.grid(language=c("Baure","Bora","Chintang","Chintang (no pear stories)","Dutch","English","Even","Hoocąk","Nǁng","Sakha","Texistepec"),parameter=c("spearman.rho","spearman.S","spearman.p","spearman.p.stars"))
results.word_duration$result = NA

# Prepare data structures for results of multivariate modeling
results.word_duration.lmer = expand.grid(language=c("Baure","Bora","Chintang","Chintang (no pear stories)","Dutch","English","Even","Hoocąk","Nǁng","Sakha","Texistepec"),parameter=c("n","m.r.2","c.r.2","coefficient","beta","m.r.2.change","c.r.2.change","semi.partial.r2","chisq","df","p","p.stars"))
results.word_duration.lmer$result = NA
```


```{r analysis_baure, echo=FALSE}

# Create a table of the 100 most frequent word types in Baure
baure_unique = unique(baure[,c("word_type","word_type2","whole_word_gloss","ntvr_ps_root.simplified","word_freq","word_rel_freq","word_log_freq","word_log_rel_freq","word_freq_rank","word_freq_rank2","doc_freq","doc_freq_rank")])
baure_100 = head(baure_unique[order(baure_unique$word_freq_rank,baure_unique$doc_freq_rank),],100)

# Set row names to null
rownames(baure_100) = NULL

# Calculate the mean and standard deviation of word duration and articulation speed per word using a for loop
# Fill these calculated values into the table of the 100 most frequent word types
baure_100$mean_duration_per_word = NA
baure_100$sd_duration_per_word = NA
baure_100$mean_speed_per_word = NA
baure_100$sd_speed_per_word = NA
for (word_type2 in unique(baure_100$word_type2)) {
  baure_100$mean_speed_per_word[baure_100$word_type2==word_type2] = mean(baure$speed_per_word[baure$word_type2==word_type2],na.rm=T)
  baure_100$sd_speed_per_word[baure_100$word_type2==word_type2] = sd(baure$speed_per_word[baure$word_type2==word_type2],na.rm=T)
  baure_100$mean_duration_per_word[baure_100$word_type2==word_type2] = mean(baure$WordLength[baure$word_type2==word_type2],na.rm=T)
  baure_100$sd_duration_per_word[baure_100$word_type2==word_type2] = sd(baure$WordLength[baure$word_type2==word_type2],na.rm=T)
}

# Mininum frequency
baure_min_frequency = min(baure_100$word_freq)

# Make a list of the 100 most frequent word types
baure_100_word_types = baure_100$word_type2

# Collect a data set containing all tokens (instances) of these 100 most frequent word types in the corpus
baure_100_tokens = droplevels(subset(baure,subset=word_type2 %in% baure_100_word_types))

# Add another column to the data preparation table ("word types manually excluded")
data_preparation_table$tokens_100_excluded[data_preparation_table$language=="Baure"] = length(baure_100_tokens$word)

# Exclude tokens contained in single-word annotation units
baure_100_tokens = droplevels(subset(baure_100_tokens,subset=words_per_record>1))
baure_100_tokens = droplevels(subset(baure_100_tokens,subset=chars_per_record>chars_per_word))

# Add another column to the data preparation table ("no one-word utterances (final data set)")
data_preparation_table$tokens_100_final[data_preparation_table$language=="Baure"] = length(baure_100_tokens$word)

# Delete column in table of the 100 most frequent words that is no longer needed
baure_100$word_type2 = NULL

# Similarly for excluded words (that are occur on one of the blacklists)
baure_unique_backup = unique(baure_backup[,c("word_type","word_type2","whole_word_gloss","ntvr_ps_root.simplified","word_freq","word_rel_freq","doc_freq")])
baure_excluded = subset(baure_unique_backup[order(-baure_unique_backup$word_freq,-baure_unique_backup$doc_freq),],subset=word_type2 %in% baure_aux_blacklist | word_type2 %in% baure_n_blacklist)

# Just for statistics, count the number of instances (tokens) of the 100 most frequent words without excluding any specific words
baure_100_backup = head(baure_unique_backup[order(-baure_unique_backup$word_freq,-baure_unique_backup$doc_freq),],100)
baure_100_word_types_backup = baure_100_backup$word_type2
baure_100_tokens_backup = droplevels(subset(baure_backup,subset=word_type2 %in% baure_100_word_types_backup))

# Add a column to the data preparation table ("100 most frequent word types")
data_preparation_table$tokens_100[data_preparation_table$language=="Baure"] = length(baure_100_tokens_backup$word)

# Only keep those excluded words that are at least as frequent as the 100th most frequent word)
baure_excluded = subset(baure_excluded,subset=word_freq >= baure_min_frequency)

# Set rownames to null
rownames(baure_excluded) = NULL

# Calculate the mean and standard deviation of word duration and articulation speed per word using a for loop
# Fill these calculated values into the table of excluded word types
baure_excluded$mean_duration_per_word = NA
baure_excluded$sd_duration_per_word = NA
baure_excluded$mean_speed_per_word = NA
baure_excluded$sd_speed_per_word = NA
for (word_type2 in unique(baure_excluded$word_type2)) {
  baure_excluded$mean_speed_per_word[baure_excluded$word_type2==word_type2] = mean(baure_backup$speed_per_word[baure_backup$word_type2==word_type2],na.rm=T)
  baure_excluded$sd_speed_per_word[baure_excluded$word_type2==word_type2] = sd(baure_backup$speed_per_word[baure_backup$word_type2==word_type2],na.rm=T)
  baure_excluded$mean_duration_per_word[baure_excluded$word_type2==word_type2] = mean(baure_backup$WordLength[baure_backup$word_type2==word_type2],na.rm=T)
  baure_excluded$sd_duration_per_word[baure_excluded$word_type2==word_type2] = sd(baure_backup$WordLength[baure_backup$word_type2==word_type2],na.rm=T)
}

# Delete column in table of excluded word types that is no longer needed
baure_excluded$word_type2 = NULL
```

```{r baure_correlation_test, echo=FALSE}

# Carry out a bivariate correlation test using Spearman's rank correlation coefficient
baure.cor.test = cor.test(baure_100$word_log_rel_freq,baure_100$mean_duration_per_word,method="spearman")

# Save the results in the table prepared for the bivariate analysis results
results.word_duration$result[results.word_duration$language=="Baure" & results.word_duration$parameter=="spearman.rho"] = baure.cor.test$estimate
results.word_duration$result[results.word_duration$language=="Baure" & results.word_duration$parameter=="spearman.S"] = baure.cor.test$statistic
results.word_duration$result[results.word_duration$language=="Baure" & results.word_duration$parameter=="spearman.p"] = baure.cor.test$p.value
results.word_duration$result[results.word_duration$language=="Baure" & results.word_duration$parameter=="spearman.p.stars"] = stars.pval(baure.cor.test$p.value)
results.word_duration$result[results.word_duration$language=="Baure" & results.word_duration$parameter=="spearman.p.stars" & results.word_duration$result==" "] = "n.s."
```

```{r baure_duration_model, echo=FALSE, size="footnotesize"}

# Convert text names and word types into factors
baure_100_tokens$ntvr_file = factor(baure_100_tokens$ntvr_file)
baure_100_tokens$word_type2 = factor(baure_100_tokens$word_type2)

# Train a linear mixed-effects model of log word duration predicted by log relative frequency, word length in characters, the number of morphemes per word, its position within the utterance,
# its part-of-speech (noun or verb), the articulation speed in the surrounding utterance and random factors for speaker (ELANParticipant) and text (ntvr_file)
baure_duration_model = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=baure_100_tokens, REML=TRUE)

# Create a table of model parameters for the Baure linear mixed-effects model
# Include raw coefficients
baure_duration_result = as.data.frame(coef(summary(baure_duration_model)))

# standardized beta coefficients
baure_duration_result$beta = c(NA, lm.beta.lmer(baure_duration_model))

# prepare table cells for change in marginal and conditional R² as well as semi-partial R²
baure_duration_result$m.r.2.change = NA
baure_duration_result$c.r.2.change = NA
baure_duration_result$semi.partial.r2 = NA

# Add sensible names to the coefficients
rownames(baure_duration_result) = c("(Intercept)","log word frequency","word length","number of morphemes","position","word class = verb (vs. noun)","log speech rate")

# Perform drop1 log-likelihood ratio tests for all fixed factors in the model
baure_duration_result_drop1 = drop1(baure_duration_model,test="Chisq")
baure_duration_result$chisq = baure_duration_result_drop1$LRT
baure_duration_result$df = baure_duration_result_drop1$Df
baure_duration_result$p = baure_duration_result_drop1$"Pr(Chi)"

# Add significance stars to the results of the log-likelihood ratio tests
baure_duration_result$stars = stars.pval(baure_duration_result$p)
baure_duration_result$stars[1] = NA
baure_duration_result$stars = sub(" ", "n.s.", baure_duration_result$stars)
baure_duration_result$stars = sub("^\\.$", "n.s.", baure_duration_result$stars)

# Train a similar model but exclude word frequency as a fixed effect
baure_duration_model.no_word_freq = lmer(log_word_length ~ chars_per_word + morphs_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=baure_100_tokens, REML=TRUE)

# Train a similar model but exclude word length as a fixed effect
baure_duration_model.no_word_length = lmer(log_word_length ~ word_log_rel_freq + morphs_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=baure_100_tokens, REML=TRUE)

# Train a similar model but exclude the number of morphemes as a fixed effect
baure_duration_model.no_morphs = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=baure_100_tokens, REML=TRUE)

# Train a similar model but exclude word position as a fixed effect
baure_duration_model.no_position = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=baure_100_tokens, REML=TRUE)

# Train a similar model but exclude part-of-speech as a fixed effect
baure_duration_model.no_pos = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + position_percentage + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=baure_100_tokens, REML=TRUE)

# Train a similar model but exclude articulation rate in the context as a fixed effect
baure_duration_model.no_speech_rate = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + position_percentage + ntvr_ps_root.simplified + (1|ELANParticipant) + (1|ntvr_file), data=baure_100_tokens, REML=TRUE)

# Calculate the marginal and conditional R² of the complete model
baure_duration_m.r.2 = r.squaredGLMM(baure_duration_model)[1]
baure_duration_c.r.2 = r.squaredGLMM(baure_duration_model)[2]

# Calculate semi-partial R² for the fixed effects in the complete model
baure_duration_model.r2beta = r2beta(model=baure_duration_model,method="nsj",partial=T)

# Calculate the percentage change in marginal and conditional R² as well as the semi-partial R² for each fixed effect in the model
baure_duration_result["log word frequency","m.r.2.change"] = (baure_duration_m.r.2 - r.squaredGLMM(baure_duration_model.no_word_freq)[1]) / baure_duration_m.r.2 * 100
baure_duration_result["log word frequency","c.r.2.change"] = (baure_duration_c.r.2 - r.squaredGLMM(baure_duration_model.no_word_freq)[2]) / baure_duration_c.r.2 * 100
baure_duration_result["log word frequency","semi.partial.r2"] = baure_duration_model.r2beta$Rsq[baure_duration_model.r2beta$Effect=="word_log_rel_freq"]
baure_duration_result["word length","m.r.2.change"] = (baure_duration_m.r.2 - r.squaredGLMM(baure_duration_model.no_word_length)[1]) / baure_duration_m.r.2 * 100
baure_duration_result["word length","c.r.2.change"] = (baure_duration_c.r.2 - r.squaredGLMM(baure_duration_model.no_word_length)[2]) / baure_duration_c.r.2 * 100
baure_duration_result["word length","semi.partial.r2"] = baure_duration_model.r2beta$Rsq[baure_duration_model.r2beta$Effect=="chars_per_word"]
baure_duration_result["number of morphemes","m.r.2.change"] = (baure_duration_m.r.2 - r.squaredGLMM(baure_duration_model.no_morphs)[1]) / baure_duration_m.r.2 * 100
baure_duration_result["number of morphemes","c.r.2.change"] = (baure_duration_c.r.2 - r.squaredGLMM(baure_duration_model.no_morphs)[2]) / baure_duration_c.r.2 * 100
baure_duration_result["number of morphemes","semi.partial.r2"] = baure_duration_model.r2beta$Rsq[baure_duration_model.r2beta$Effect=="morphs_per_word"]
baure_duration_result["position","m.r.2.change"] = (baure_duration_m.r.2 - r.squaredGLMM(baure_duration_model.no_position)[1]) / baure_duration_m.r.2 * 100
baure_duration_result["position","c.r.2.change"] = (baure_duration_c.r.2 - r.squaredGLMM(baure_duration_model.no_position)[2]) / baure_duration_c.r.2 * 100
baure_duration_result["position","semi.partial.r2"] = baure_duration_model.r2beta$Rsq[baure_duration_model.r2beta$Effect=="position_percentage"]
baure_duration_result["word class = verb (vs. noun)","m.r.2.change"] = (baure_duration_m.r.2 - r.squaredGLMM(baure_duration_model.no_pos)[1]) / baure_duration_m.r.2 * 100
baure_duration_result["word class = verb (vs. noun)","c.r.2.change"] = (baure_duration_c.r.2 - r.squaredGLMM(baure_duration_model.no_pos)[2]) / baure_duration_c.r.2 * 100
baure_duration_result["word class = verb (vs. noun)","semi.partial.r2"] = baure_duration_model.r2beta$Rsq[baure_duration_model.r2beta$Effect=="ntvr_ps_root.simplifiedV"]
baure_duration_result["log speech rate","m.r.2.change"] = (baure_duration_m.r.2 - r.squaredGLMM(baure_duration_model.no_speech_rate)[1]) / baure_duration_m.r.2 * 100
baure_duration_result["log speech rate","c.r.2.change"] = (baure_duration_c.r.2 - r.squaredGLMM(baure_duration_model.no_speech_rate)[2]) / baure_duration_c.r.2 * 100
baure_duration_result["log speech rate","semi.partial.r2"] = baure_duration_model.r2beta$Rsq[baure_duration_model.r2beta$Effect=="log_speed_per_record2"]

# Save the results concerning the main predictor of interest, namely, word frequency, in table for cross-linguistic comparison
results.word_duration.lmer$result[results.word_duration.lmer$language=="Baure" & results.word_duration.lmer$parameter=="coefficient"] = baure_duration_result["log word frequency","Estimate"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Baure" & results.word_duration.lmer$parameter=="beta"] = baure_duration_result["log word frequency","beta"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Baure" & results.word_duration.lmer$parameter=="chisq"] = baure_duration_result["log word frequency","chisq"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Baure" & results.word_duration.lmer$parameter=="df"] = baure_duration_result["log word frequency","df"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Baure" & results.word_duration.lmer$parameter=="p"] = baure_duration_result["log word frequency","p"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Baure" & results.word_duration.lmer$parameter=="p.stars"] = baure_duration_result["log word frequency","stars"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Baure" & results.word_duration.lmer$parameter=="m.r.2.change"] = baure_duration_result["log word frequency","m.r.2.change"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Baure" & results.word_duration.lmer$parameter=="c.r.2.change"] = baure_duration_result["log word frequency","c.r.2.change"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Baure" & results.word_duration.lmer$parameter=="semi.partial.r2"] = baure_duration_result["log word frequency","semi.partial.r2"]

# Create a table of the random effects in the model
baure_duration_random = as.data.frame(summary(baure_duration_model)$varcor)
colnames(baure_duration_random)[1] = "random_effect"
baure_duration_random$groups = c(summary(baure_duration_model)$ngrps,NA)
baure_duration_random = baure_duration_random[c("random_effect","groups","sdcor")]

# Give the random effects sensible names
baure_duration_random$random_effect[baure_duration_random$random_effect=="ELANParticipant"] = "speaker"
baure_duration_random$random_effect[baure_duration_random$random_effect=="ntvr_file"] = "text"

# And order them consistently
ordering = c(order(baure_duration_random$random_effect)[2:3],order(baure_duration_random$random_effect)[1])
baure_duration_random = baure_duration_random[ordering,]

# Add columns for various tests of the random effects
baure_duration_random$m.r.2.change = NA
baure_duration_random$c.r.2.change = NA
baure_duration_random$chisq = NA
baure_duration_random$df = NA
baure_duration_random$p = NA
baure_duration_random$sig = NA
 
# Train a similar model but exclude speaker as a random effect
baure_duration_model.no_speaker = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ntvr_file), data=baure_100_tokens, REML=TRUE)

# Train a similar model but exclude text as a random effect
baure_duration_model.no_text = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant), data=baure_100_tokens, REML=TRUE)

# Compare model without speaker as a random effect to full model and calculate log-likelihood ratio test as well as percentage change in marginal and conditional R²
comparison = anova(baure_duration_model,baure_duration_model.no_speaker,test="Chisq")
baure_duration_random$chisq[baure_duration_random$random_effect=="speaker"] = comparison$Chisq[2]
baure_duration_random$df[baure_duration_random$random_effect=="speaker"] = comparison$"Chi Df"[2]
baure_duration_random$p[baure_duration_random$random_effect=="speaker"] = comparison$"Pr(>Chisq)"[2]
baure_duration_random$sig[baure_duration_random$random_effect=="speaker"] = sub("^\\.$","n.s.",sub(" ","n.s.",stars.pval(comparison$"Pr(>Chisq)"[2])))
baure_duration_random$m.r.2.change[baure_duration_random$random_effect=="speaker"] = (baure_duration_m.r.2 - r.squaredGLMM(baure_duration_model.no_speaker)[1]) / baure_duration_m.r.2 * 100
baure_duration_random$c.r.2.change[baure_duration_random$random_effect=="speaker"] = (baure_duration_c.r.2 - r.squaredGLMM(baure_duration_model.no_speaker)[2]) / baure_duration_c.r.2 * 100

# Compare model without text as a random effect to full model and calculate log-likelihood ratio test as well as percentage change in marginal and conditional R²
comparison = anova(baure_duration_model,baure_duration_model.no_text,test="Chisq")
baure_duration_random$chisq[baure_duration_random$random_effect=="text"] = comparison$Chisq[2]
baure_duration_random$df[baure_duration_random$random_effect=="text"] = comparison$"Chi Df"[2]
baure_duration_random$p[baure_duration_random$random_effect=="text"] = comparison$"Pr(>Chisq)"[2]
baure_duration_random$sig[baure_duration_random$random_effect=="text"] = sub("^\\.$","n.s.",sub(" ","n.s.",stars.pval(comparison$"Pr(>Chisq)"[2])))
baure_duration_random$m.r.2.change[baure_duration_random$random_effect=="text"] = (baure_duration_m.r.2 - r.squaredGLMM(baure_duration_model.no_text)[1]) / baure_duration_m.r.2 * 100
baure_duration_random$c.r.2.change[baure_duration_random$random_effect=="text"] = (baure_duration_c.r.2 - r.squaredGLMM(baure_duration_model.no_text)[2]) / baure_duration_c.r.2 * 100

# Also save the number of tokens included in the model as well as the marginal and conditional R² of the complete model in the table for cross-linguistic comparison
results.word_duration.lmer$result[results.word_duration.lmer$language=="Baure" & results.word_duration.lmer$parameter=="n"] = summary(baure_duration_model)$devcomp$dims[1]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Baure" & results.word_duration.lmer$parameter=="m.r.2"] = r.squaredGLMM(baure_duration_model)[1]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Baure" & results.word_duration.lmer$parameter=="c.r.2"] = r.squaredGLMM(baure_duration_model)[2]
```



```{r analysis_bora, echo=FALSE}

# Make a list of the 100 most frequent word types
bora_unique = unique(bora[,c("word_type","word_type2","whole_word_gloss","ntvr_ps_root.simplified","word_freq","word_rel_freq","word_log_freq","word_log_rel_freq","word_freq_rank","word_freq_rank2","doc_freq","doc_freq_rank")])
bora_100 = head(bora_unique[order(bora_unique$word_freq_rank,bora_unique$doc_freq_rank),],100)

# Set row names to null
rownames(bora_100) = NULL

# Calculate the mean and standard deviation of word duration and articulation speed per word using a for loop
# Fill these calculated values into the table of the 100 most frequent word types
bora_100$mean_duration_per_word = NA
bora_100$sd_duration_per_word = NA
bora_100$mean_speed_per_word = NA
bora_100$sd_speed_per_word = NA
for (word_type2 in unique(bora_100$word_type2)) {
  bora_100$mean_speed_per_word[bora_100$word_type2==word_type2] = mean(bora$speed_per_word[bora$word_type2==word_type2],na.rm=T)
  bora_100$sd_speed_per_word[bora_100$word_type2==word_type2] = sd(bora$speed_per_word[bora$word_type2==word_type2],na.rm=T)
  bora_100$mean_duration_per_word[bora_100$word_type2==word_type2] = mean(bora$WordLength[bora$word_type2==word_type2],na.rm=T)
  bora_100$sd_duration_per_word[bora_100$word_type2==word_type2] = sd(bora$WordLength[bora$word_type2==word_type2],na.rm=T)
}

# Mininum frequency
bora_min_frequency = min(bora_100$word_freq)

# Collect a data set containing all tokens (instances) of these 100 most frequent word types in the corpus
bora_100_word_types = paste(bora_100$word_type,bora_100$whole_word_gloss,bora_100$ntvr_ps_root.simplified,sep="|")
bora_100_tokens = droplevels(subset(bora,subset=word_type2 %in% bora_100_word_types))

# Add another column to the data preparation table ("word types manually excluded")
data_preparation_table$tokens_100_excluded[data_preparation_table$language=="Bora"] = length(bora_100_tokens$word)

# Exclude tokens contained in single-word annotation units
bora_100_tokens = droplevels(subset(bora_100_tokens,subset=words_per_record>1))
bora_100_tokens = droplevels(subset(bora_100_tokens,subset=chars_per_record>chars_per_word))

# Add another column to the data preparation table ("no one-word utterances (final data set)")
data_preparation_table$tokens_100_final[data_preparation_table$language=="Bora"] = length(bora_100_tokens$word)

# Delete column in table of the 100 most frequent words that is no longer needed
bora_100$word_type2 = NULL

# Similarly for excluded words (that are occur on one of the blacklists)
bora_unique_backup = unique(bora_backup[,c("word_type","word_type2","whole_word_gloss","ntvr_ps_root.simplified","word_freq","word_rel_freq","doc_freq")])
bora_excluded = subset(bora_unique_backup[order(-bora_unique_backup$word_freq,-bora_unique_backup$doc_freq),],subset=word_type2 %in% bora_aux_blacklist | word_type2 %in% bora_n_blacklist)

# Just for statistics, count the number of instances (tokens) of the 100 most frequent words without excluding any specific words
bora_100_backup = head(bora_unique_backup[order(-bora_unique_backup$word_freq,-bora_unique_backup$doc_freq),],100)
bora_100_word_types_backup = bora_100_backup$word_type2
bora_100_tokens_backup = droplevels(subset(bora_backup,subset=word_type2 %in% bora_100_word_types_backup))

# Add a column to the data preparation table ("100 most frequent word types")
data_preparation_table$tokens_100[data_preparation_table$language=="Bora"] = length(bora_100_tokens_backup$word)

# Only keep those excluded words that are at least as frequent as the 100th most frequent word)
bora_excluded = subset(bora_excluded,subset=word_freq >= bora_min_frequency)

# Set rownames to null
rownames(bora_excluded) = NULL

# Calculate the mean and standard deviation of word duration and articulation speed per word using a for loop
# Fill these calculated values into the table of excluded word types
bora_excluded$mean_duration_per_word = NA
bora_excluded$sd_duration_per_word = NA
bora_excluded$mean_speed_per_word = NA
bora_excluded$sd_speed_per_word = NA
for (word_type2 in unique(bora_excluded$word_type2)) {
  bora_excluded$mean_speed_per_word[bora_excluded$word_type2==word_type2] = mean(bora_backup$speed_per_word[bora_backup$word_type2==word_type2],na.rm=T)
  bora_excluded$sd_speed_per_word[bora_excluded$word_type2==word_type2] = sd(bora_backup$speed_per_word[bora_backup$word_type2==word_type2],na.rm=T)
  bora_excluded$mean_duration_per_word[bora_excluded$word_type2==word_type2] = mean(bora_backup$WordLength[bora_backup$word_type2==word_type2],na.rm=T)
  bora_excluded$sd_duration_per_word[bora_excluded$word_type2==word_type2] = sd(bora_backup$WordLength[bora_backup$word_type2==word_type2],na.rm=T)
}

# Delete column in table of excluded word types that is no longer needed
bora_excluded$word_type2 = NULL
```

```{r bora_correlation_test, echo=FALSE}

# Carry out a bivariate correlation test using Spearman's rank correlation coefficient
bora.cor.test = cor.test(bora_100$word_log_rel_freq,bora_100$mean_duration_per_word,method="spearman")

# Save the results in the table prepared for the bivariate analysis results
results.word_duration$result[results.word_duration$language=="Bora" & results.word_duration$parameter=="spearman.rho"] = bora.cor.test$estimate
results.word_duration$result[results.word_duration$language=="Bora" & results.word_duration$parameter=="spearman.S"] = bora.cor.test$statistic
results.word_duration$result[results.word_duration$language=="Bora" & results.word_duration$parameter=="spearman.p"] = bora.cor.test$p.value
results.word_duration$result[results.word_duration$language=="Bora" & results.word_duration$parameter=="spearman.p.stars"] = stars.pval(bora.cor.test$p.value)
results.word_duration$result[results.word_duration$language=="Bora" & results.word_duration$parameter=="spearman.p.stars" & results.word_duration$result==" "] = "n.s."
```

```{r bora_duration_model, echo=FALSE, size="footnotesize"}

# Convert text names and word types into factors
bora_100_tokens$ntvr_file = factor(bora_100_tokens$ntvr_file)
bora_100_tokens$word_type2 = factor(bora_100_tokens$word_type2)

# Train a linear mixed-effects model of log word duration predicted by log relative frequency, word length in characters, the number of morphemes per word, its position within the utterance,
# its part-of-speech (noun or verb), the articulation speed in the surrounding utterance and random factors for speaker (ELANParticipant) and text (ntvr_file)
bora_duration_model = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=bora_100_tokens, REML=TRUE)

# Create a table of model parameters for the Baure linear mixed-effects model
# Include raw coefficients
bora_duration_result = as.data.frame(coef(summary(bora_duration_model)))

# standardized beta coefficients
bora_duration_result$beta = c(NA, lm.beta.lmer(bora_duration_model))

# prepare table cells for change in marginal and conditional R² as well as semi-partial R²
bora_duration_result$m.r.2.change = NA
bora_duration_result$c.r.2.change = NA
bora_duration_result$semi.partial.r2 = NA

# Add sensible names to the coefficients
rownames(bora_duration_result) = c("(Intercept)","log word frequency","word length","number of morphemes","position","word class = verb (vs. noun)","log speech rate")

# Perform drop1 log-likelihood ratio tests for all fixed factors in the model
bora_duration_result_drop1 = drop1(bora_duration_model,test="Chisq")
bora_duration_result$chisq = bora_duration_result_drop1$LRT
bora_duration_result$df = bora_duration_result_drop1$Df
bora_duration_result$p = bora_duration_result_drop1$"Pr(Chi)"

# Add significance stars to the results of the log-likelihood ratio tests
bora_duration_result$stars = stars.pval(bora_duration_result$p)
bora_duration_result$stars[1] = NA
bora_duration_result$stars = sub(" ", "n.s.", bora_duration_result$stars)
bora_duration_result$stars = sub("^\\.$", "n.s.", bora_duration_result$stars)

# Train a similar model but exclude word frequency as a fixed effect
bora_duration_model.no_word_freq = lmer(log_word_length ~ chars_per_word + morphs_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=bora_100_tokens, REML=TRUE)

# Train a similar model but exclude word length as a fixed effect
bora_duration_model.no_word_length = lmer(log_word_length ~ word_log_rel_freq + morphs_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=bora_100_tokens, REML=TRUE)

# Train a similar model but exclude the number of morphemes as a fixed effect
bora_duration_model.no_morphs = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=bora_100_tokens, REML=TRUE)

# Train a similar model but exclude word position as a fixed effect
bora_duration_model.no_position = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=bora_100_tokens, REML=TRUE)

# Train a similar model but exclude part-of-speech as a fixed effect
bora_duration_model.no_pos = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + position_percentage + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file) , data=bora_100_tokens, REML=TRUE)

# Train a similar model but exclude articulation rate in the context as a fixed effect
bora_duration_model.no_speech_rate = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + position_percentage + ntvr_ps_root.simplified + (1|ELANParticipant) + (1|ntvr_file), data=bora_100_tokens, REML=TRUE)

# Calculate the marginal and conditional R² of the complete model
bora_duration_m.r.2 = r.squaredGLMM(bora_duration_model)[1]
bora_duration_c.r.2 = r.squaredGLMM(bora_duration_model)[2]

# Calculate semi-partial R² for the fixed effects in the complete model
bora_duration_model.r2beta = r2beta(model=bora_duration_model,method="nsj",partial=T)

# Calculate the percentage change in marginal and conditional R² as well as the semi-partial R² for each fixed effect in the model
bora_duration_result["log word frequency","m.r.2.change"] = (bora_duration_m.r.2 - r.squaredGLMM(bora_duration_model.no_word_freq)[1]) / bora_duration_m.r.2 * 100
bora_duration_result["log word frequency","c.r.2.change"] = (bora_duration_c.r.2 - r.squaredGLMM(bora_duration_model.no_word_freq)[2]) / bora_duration_c.r.2 * 100
bora_duration_result["log word frequency","semi.partial.r2"] = bora_duration_model.r2beta$Rsq[bora_duration_model.r2beta$Effect=="word_log_rel_freq"]
bora_duration_result["word length","m.r.2.change"] = (bora_duration_m.r.2 - r.squaredGLMM(bora_duration_model.no_word_length)[1]) / bora_duration_m.r.2 * 100
bora_duration_result["word length","c.r.2.change"] = (bora_duration_c.r.2 - r.squaredGLMM(bora_duration_model.no_word_length)[2]) / bora_duration_c.r.2 * 100
bora_duration_result["word length","semi.partial.r2"] = bora_duration_model.r2beta$Rsq[bora_duration_model.r2beta$Effect=="chars_per_word"]
bora_duration_result["number of morphemes","m.r.2.change"] = (bora_duration_m.r.2 - r.squaredGLMM(bora_duration_model.no_morphs)[1]) / bora_duration_m.r.2 * 100
bora_duration_result["number of morphemes","c.r.2.change"] = (bora_duration_c.r.2 - r.squaredGLMM(bora_duration_model.no_morphs)[2]) / bora_duration_c.r.2 * 100
bora_duration_result["number of morphemes","semi.partial.r2"] = bora_duration_model.r2beta$Rsq[bora_duration_model.r2beta$Effect=="morphs_per_word"]
bora_duration_result["position","m.r.2.change"] = (bora_duration_m.r.2 - r.squaredGLMM(bora_duration_model.no_position)[1]) / bora_duration_m.r.2 * 100
bora_duration_result["position","c.r.2.change"] = (bora_duration_c.r.2 - r.squaredGLMM(bora_duration_model.no_position)[2]) / bora_duration_c.r.2 * 100
bora_duration_result["position","semi.partial.r2"] = bora_duration_model.r2beta$Rsq[bora_duration_model.r2beta$Effect=="position_percentage"]
bora_duration_result["word class = verb (vs. noun)","m.r.2.change"] = (bora_duration_m.r.2 - r.squaredGLMM(bora_duration_model.no_pos)[1]) / bora_duration_m.r.2 * 100
bora_duration_result["word class = verb (vs. noun)","c.r.2.change"] = (bora_duration_c.r.2 - r.squaredGLMM(bora_duration_model.no_pos)[2]) / bora_duration_c.r.2 * 100
bora_duration_result["word class = verb (vs. noun)","semi.partial.r2"] = bora_duration_model.r2beta$Rsq[bora_duration_model.r2beta$Effect=="ntvr_ps_root.simplifiedV"]
bora_duration_result["log speech rate","m.r.2.change"] = (bora_duration_m.r.2 - r.squaredGLMM(bora_duration_model.no_speech_rate)[1]) / bora_duration_m.r.2 * 100
bora_duration_result["log speech rate","c.r.2.change"] = (bora_duration_c.r.2 - r.squaredGLMM(bora_duration_model.no_speech_rate)[2]) / bora_duration_c.r.2 * 100
bora_duration_result["log speech rate","semi.partial.r2"] = bora_duration_model.r2beta$Rsq[bora_duration_model.r2beta$Effect=="log_speed_per_record2"]

# Save the results concerning the main predictor of interest, namely, word frequency, in table for cross-linguistic comparison
results.word_duration.lmer$result[results.word_duration.lmer$language=="Bora" & results.word_duration.lmer$parameter=="coefficient"] = bora_duration_result["log word frequency","Estimate"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Bora" & results.word_duration.lmer$parameter=="beta"] = bora_duration_result["log word frequency","beta"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Bora" & results.word_duration.lmer$parameter=="chisq"] = bora_duration_result["log word frequency","chisq"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Bora" & results.word_duration.lmer$parameter=="df"] = bora_duration_result["log word frequency","df"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Bora" & results.word_duration.lmer$parameter=="p"] = bora_duration_result["log word frequency","p"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Bora" & results.word_duration.lmer$parameter=="p.stars"] = bora_duration_result["log word frequency","stars"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Bora" & results.word_duration.lmer$parameter=="m.r.2.change"] = bora_duration_result["log word frequency","m.r.2.change"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Bora" & results.word_duration.lmer$parameter=="c.r.2.change"] = bora_duration_result["log word frequency","c.r.2.change"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Bora" & results.word_duration.lmer$parameter=="semi.partial.r2"] = bora_duration_result["log word frequency","semi.partial.r2"]

# Create a table of the random effects in the model
bora_duration_random = as.data.frame(summary(bora_duration_model)$varcor)
colnames(bora_duration_random)[1] = "random_effect"
bora_duration_random$groups = c(summary(bora_duration_model)$ngrps,NA)
bora_duration_random = bora_duration_random[c("random_effect","groups","sdcor")]

# Give the random effects sensible names
bora_duration_random$random_effect[bora_duration_random$random_effect=="ELANParticipant"] = "speaker"
bora_duration_random$random_effect[bora_duration_random$random_effect=="ntvr_file"] = "text"

# And order them consistently
ordering = c(order(bora_duration_random$random_effect)[2:3],order(bora_duration_random$random_effect)[1])
bora_duration_random = bora_duration_random[ordering,]

# Add columns for various tests of the random effects
bora_duration_random$m.r.2.change = NA
bora_duration_random$c.r.2.change = NA
bora_duration_random$chisq = NA
bora_duration_random$df = NA
bora_duration_random$p = NA
bora_duration_random$sig = NA

# Train a similar model but exclude speaker as a random effect
bora_duration_model.no_speaker = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ntvr_file), data=bora_100_tokens, REML=TRUE)

# Train a similar model but exclude text as a random effect
bora_duration_model.no_text = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant), data=bora_100_tokens, REML=TRUE)

# Compare model without speaker as a random effect to full model and calculate log-likelihood ratio test as well as percentage change in marginal and conditional R²
comparison = anova(bora_duration_model,bora_duration_model.no_speaker,test="Chisq")
bora_duration_random$chisq[bora_duration_random$random_effect=="speaker"] = comparison$Chisq[2]
bora_duration_random$df[bora_duration_random$random_effect=="speaker"] = comparison$"Chi Df"[2]
bora_duration_random$p[bora_duration_random$random_effect=="speaker"] = comparison$"Pr(>Chisq)"[2]
bora_duration_random$sig[bora_duration_random$random_effect=="speaker"] = sub("^\\.$","n.s.",sub(" ","n.s.",stars.pval(comparison$"Pr(>Chisq)"[2])))
bora_duration_random$m.r.2.change[bora_duration_random$random_effect=="speaker"] = (bora_duration_m.r.2 - r.squaredGLMM(bora_duration_model.no_speaker)[1]) / bora_duration_m.r.2 * 100
bora_duration_random$c.r.2.change[bora_duration_random$random_effect=="speaker"] = (bora_duration_c.r.2 - r.squaredGLMM(bora_duration_model.no_speaker)[2]) / bora_duration_c.r.2 * 100

# Compare model without text as a random effect to full model and calculate log-likelihood ratio test as well as percentage change in marginal and conditional R²
comparison = anova(bora_duration_model,bora_duration_model.no_text,test="Chisq")
bora_duration_random$chisq[bora_duration_random$random_effect=="text"] = comparison$Chisq[2]
bora_duration_random$df[bora_duration_random$random_effect=="text"] = comparison$"Chi Df"[2]
bora_duration_random$p[bora_duration_random$random_effect=="text"] = comparison$"Pr(>Chisq)"[2]
bora_duration_random$sig[bora_duration_random$random_effect=="text"] = sub("^\\.$","n.s.",sub(" ","n.s.",stars.pval(comparison$"Pr(>Chisq)"[2])))
bora_duration_random$m.r.2.change[bora_duration_random$random_effect=="text"] = (bora_duration_m.r.2 - r.squaredGLMM(bora_duration_model.no_text)[1]) / bora_duration_m.r.2 * 100
bora_duration_random$c.r.2.change[bora_duration_random$random_effect=="text"] = (bora_duration_c.r.2 - r.squaredGLMM(bora_duration_model.no_text)[2]) / bora_duration_c.r.2 * 100

# Also save the number of tokens included in the model as well as the marginal and conditional R² of the complete model in the table for cross-linguistic comparison
results.word_duration.lmer$result[results.word_duration.lmer$language=="Bora" & results.word_duration.lmer$parameter=="n"] = summary(bora_duration_model)$devcomp$dims[1]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Bora" & results.word_duration.lmer$parameter=="m.r.2"] = r.squaredGLMM(bora_duration_model)[1]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Bora" & results.word_duration.lmer$parameter=="c.r.2"] = r.squaredGLMM(bora_duration_model)[2]
```


```{r analysis_chintang, echo=FALSE}

# Make a list of the 100 most frequent word types
chintang_unique = unique(chintang[,c("word_type","word_type2","whole_word_gloss","ntvr_ps_root.simplified","word_freq","word_rel_freq","word_log_freq","word_log_rel_freq","word_freq_rank","word_freq_rank2","doc_freq","doc_freq_rank")])
chintang_100 = head(chintang_unique[order(chintang_unique$word_freq_rank,chintang_unique$doc_freq_rank),],100)

# Set row names to null
rownames(chintang_100) = NULL

# Calculate the mean and standard deviation of word duration and articulation speed per word using a for loop
# Fill these calculated values into the table of the 100 most frequent word types
chintang_100$mean_duration_per_word = NA
chintang_100$sd_duration_per_word = NA
chintang_100$mean_speed_per_word = NA
chintang_100$sd_speed_per_word = NA
for (word_type2 in unique(chintang_100$word_type2)) {
  chintang_100$mean_speed_per_word[chintang_100$word_type2==word_type2] = mean(chintang$speed_per_word[chintang$word_type2==word_type2],na.rm=T)
  chintang_100$sd_speed_per_word[chintang_100$word_type2==word_type2] = sd(chintang$speed_per_word[chintang$word_type2==word_type2],na.rm=T)
  chintang_100$mean_duration_per_word[chintang_100$word_type2==word_type2] = mean(chintang$WordLength[chintang$word_type2==word_type2],na.rm=T)
  chintang_100$sd_duration_per_word[chintang_100$word_type2==word_type2] = sd(chintang$WordLength[chintang$word_type2==word_type2],na.rm=T)
}

# Mininum frequency
chintang_min_frequency = min(chintang_100$word_freq)

# Collect a data set containing all tokens (instances) of these 100 most frequent word types in the corpus
chintang_100_word_types = paste(chintang_100$word_type,chintang_100$whole_word_gloss,chintang_100$ntvr_ps_root.simplified,sep="|")
chintang_100_tokens = droplevels(subset(chintang,subset=word_type2 %in% chintang_100_word_types))

# Add another column to the data preparation table ("word types manually excluded")
data_preparation_table$tokens_100_excluded[data_preparation_table$language=="Chintang"] = length(chintang_100_tokens$word)

# Exclude tokens contained in single-word annotation units
chintang_100_tokens = droplevels(subset(chintang_100_tokens,subset=words_per_record>1))
chintang_100_tokens = droplevels(subset(chintang_100_tokens,subset=chars_per_record>chars_per_word))

# Add another column to the data preparation table ("no one-word utterances (final data set)")
data_preparation_table$tokens_100_final[data_preparation_table$language=="Chintang"] = length(chintang_100_tokens$word)

# Delete column in table of the 100 most frequent words that is no longer needed
chintang_100$word_type2 = NULL

# Similarly for excluded words (that are occur on one of the blacklists)
chintang_unique_backup = unique(chintang_backup[,c("word_type","word_type2","whole_word_gloss","ntvr_ps_root.simplified","word_freq","word_rel_freq","doc_freq")])
chintang_excluded = subset(chintang_unique_backup[order(-chintang_unique_backup$word_freq,-chintang_unique_backup$doc_freq),],subset=word_type2 %in% chintang_aux_blacklist | word_type2 %in% chintang_n_blacklist)

# Just for statistics, count the number of instances (tokens) of the 100 most frequent words without excluding any specific words
chintang_100_backup = head(chintang_unique_backup[order(-chintang_unique_backup$word_freq,-chintang_unique_backup$doc_freq),],100)
chintang_100_word_types_backup = chintang_100_backup$word_type2
chintang_100_tokens_backup = droplevels(subset(chintang_backup,subset=word_type2 %in% chintang_100_word_types_backup))

# Add a column to the data preparation table ("100 most frequent word types")
data_preparation_table$tokens_100[data_preparation_table$language=="Chintang"] = length(chintang_100_tokens_backup$word)

# Only keep those excluded words that are at least as frequent as the 100th most frequent word)
chintang_excluded = subset(chintang_excluded,subset=word_freq >= chintang_min_frequency)

# Set rownames to null
rownames(chintang_excluded) = NULL

# Calculate the mean and standard deviation of word duration and articulation speed per word using a for loop
# Fill these calculated values into the table of excluded word types
chintang_excluded$mean_duration_per_word = NA
chintang_excluded$sd_duration_per_word = NA
chintang_excluded$mean_speed_per_word = NA
chintang_excluded$sd_speed_per_word = NA
for (word_type2 in unique(chintang_excluded$word_type2)) {
  chintang_excluded$mean_speed_per_word[chintang_excluded$word_type2==word_type2] = mean(chintang_backup$speed_per_word[chintang_backup$word_type2==word_type2],na.rm=T)
  chintang_excluded$sd_speed_per_word[chintang_excluded$word_type2==word_type2] = sd(chintang_backup$speed_per_word[chintang_backup$word_type2==word_type2],na.rm=T)
  chintang_excluded$mean_duration_per_word[chintang_excluded$word_type2==word_type2] = mean(chintang_backup$WordLength[chintang_backup$word_type2==word_type2],na.rm=T)
  chintang_excluded$sd_duration_per_word[chintang_excluded$word_type2==word_type2] = sd(chintang_backup$WordLength[chintang_backup$word_type2==word_type2],na.rm=T)
}

# Delete column in table of excluded word types that is no longer needed
chintang_excluded$word_type2 = NULL
```

```{r chintang_correlation_test, echo=FALSE}
# Carry out a bivariate correlation test using Spearman's rank correlation coefficient
chintang.cor.test = cor.test(chintang_100$word_log_rel_freq,chintang_100$mean_duration_per_word,method="spearman")

# Save the results in the table prepared for the bivariate analysis results
results.word_duration$result[results.word_duration$language=="Chintang" & results.word_duration$parameter=="spearman.rho"] = chintang.cor.test$estimate
results.word_duration$result[results.word_duration$language=="Chintang" & results.word_duration$parameter=="spearman.S"] = chintang.cor.test$statistic
results.word_duration$result[results.word_duration$language=="Chintang" & results.word_duration$parameter=="spearman.p"] = chintang.cor.test$p.value
results.word_duration$result[results.word_duration$language=="Chintang" & results.word_duration$parameter=="spearman.p.stars"] = stars.pval(chintang.cor.test$p.value)
results.word_duration$result[results.word_duration$language=="Chintang" & results.word_duration$parameter=="spearman.p.stars" & results.word_duration$result==" "] = "n.s."
```

```{r chintang_duration_model, echo=FALSE, size="footnotesize"}

# Convert text names and word types into factors
chintang_100_tokens$ntvr_file = factor(chintang_100_tokens$ntvr_file)
chintang_100_tokens$word_type2 = factor(chintang_100_tokens$word_type2)

# Train a linear mixed-effects model of log word duration predicted by log relative frequency, word length in characters, the number of morphemes per word, its position within the utterance,
# its part-of-speech (noun or verb), the articulation speed in the surrounding utterance and random factors for speaker (ELANParticipant) and text (ntvr_file)
chintang_duration_model = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=chintang_100_tokens, REML=TRUE)

# Create a table of model parameters for the Baure linear mixed-effects model
# Include raw coefficients
chintang_duration_result = as.data.frame(coef(summary(chintang_duration_model)))

# standardized beta coefficients
chintang_duration_result$beta = c(NA, lm.beta.lmer(chintang_duration_model))

# prepare table cells for change in marginal and conditional R² as well as semi-partial R²
chintang_duration_result$m.r.2.change = NA
chintang_duration_result$c.r.2.change = NA
chintang_duration_result$semi.partial.r2 = NA

# Add sensible names to the coefficients
rownames(chintang_duration_result) = c("(Intercept)","log word frequency","word length","number of morphemes","position","word class = verb (vs. noun)","log speech rate")

# Perform drop1 log-likelihood ratio tests for all fixed factors in the model
chintang_duration_result_drop1 = drop1(chintang_duration_model,test="Chisq")
chintang_duration_result$chisq = chintang_duration_result_drop1$LRT
chintang_duration_result$df = chintang_duration_result_drop1$Df
chintang_duration_result$p = chintang_duration_result_drop1$"Pr(Chi)"

# Add significance stars to the results of the log-likelihood ratio tests
chintang_duration_result$stars = stars.pval(chintang_duration_result$p)
chintang_duration_result$stars[1] = NA
chintang_duration_result$stars = sub(" ", "n.s.", chintang_duration_result$stars)
chintang_duration_result$stars = sub("^\\.$", "n.s.", chintang_duration_result$stars)

# Train a similar model but exclude word frequency as a fixed effect
chintang_duration_model.no_word_freq = lmer(log_word_length ~ chars_per_word + morphs_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=chintang_100_tokens, REML=TRUE)

# Train a similar model but exclude word length as a fixed effect
chintang_duration_model.no_word_length = lmer(log_word_length ~ word_log_rel_freq + morphs_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=chintang_100_tokens, REML=TRUE)

# Train a similar model but exclude the number of morphemes as a fixed effect
chintang_duration_model.no_morphs = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=chintang_100_tokens, REML=TRUE)

# Train a similar model but exclude word position as a fixed effect
chintang_duration_model.no_position = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=chintang_100_tokens, REML=TRUE)

# Train a similar model but exclude part-of-speech as a fixed effect
chintang_duration_model.no_pos = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + position_percentage + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=chintang_100_tokens, REML=TRUE)

# Train a similar model but exclude articulation rate in the context as a fixed effect
chintang_duration_model.no_speech_rate = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + position_percentage + ntvr_ps_root.simplified + (1|ELANParticipant) + (1|ntvr_file), data=chintang_100_tokens, REML=TRUE)

# Calculate the marginal and conditional R² of the complete model
chintang_duration_m.r.2 = r.squaredGLMM(chintang_duration_model)[1]
chintang_duration_c.r.2 = r.squaredGLMM(chintang_duration_model)[2]

# Calculate semi-partial R² for the fixed effects in the complete model
chintang_duration_model.r2beta = r2beta(model=chintang_duration_model,method="nsj",partial=T)

# Calculate the percentage change in marginal and conditional R² as well as the semi-partial R² for each fixed effect in the model
chintang_duration_result["log word frequency","m.r.2.change"] = (chintang_duration_m.r.2 - r.squaredGLMM(chintang_duration_model.no_word_freq)[1]) / chintang_duration_m.r.2 * 100
chintang_duration_result["log word frequency","c.r.2.change"] = (chintang_duration_c.r.2 - r.squaredGLMM(chintang_duration_model.no_word_freq)[2]) / chintang_duration_c.r.2 * 100
chintang_duration_result["log word frequency","semi.partial.r2"] = chintang_duration_model.r2beta$Rsq[chintang_duration_model.r2beta$Effect=="word_log_rel_freq"]
chintang_duration_result["word length","m.r.2.change"] = (chintang_duration_m.r.2 - r.squaredGLMM(chintang_duration_model.no_word_length)[1]) / chintang_duration_m.r.2 * 100
chintang_duration_result["word length","c.r.2.change"] = (chintang_duration_c.r.2 - r.squaredGLMM(chintang_duration_model.no_word_length)[2]) / chintang_duration_c.r.2 * 100
chintang_duration_result["word length","semi.partial.r2"] = chintang_duration_model.r2beta$Rsq[chintang_duration_model.r2beta$Effect=="chars_per_word"]
chintang_duration_result["number of morphemes","m.r.2.change"] = (chintang_duration_m.r.2 - r.squaredGLMM(chintang_duration_model.no_morphs)[1]) / chintang_duration_m.r.2 * 100
chintang_duration_result["number of morphemes","c.r.2.change"] = (chintang_duration_c.r.2 - r.squaredGLMM(chintang_duration_model.no_morphs)[2]) / chintang_duration_c.r.2 * 100
chintang_duration_result["number of morphemes","semi.partial.r2"] = chintang_duration_model.r2beta$Rsq[chintang_duration_model.r2beta$Effect=="morphs_per_word"]
chintang_duration_result["position","m.r.2.change"] = (chintang_duration_m.r.2 - r.squaredGLMM(chintang_duration_model.no_position)[1]) / chintang_duration_m.r.2 * 100
chintang_duration_result["position","c.r.2.change"] = (chintang_duration_c.r.2 - r.squaredGLMM(chintang_duration_model.no_position)[2]) / chintang_duration_c.r.2 * 100
chintang_duration_result["position","semi.partial.r2"] = chintang_duration_model.r2beta$Rsq[chintang_duration_model.r2beta$Effect=="position_percentage"]
chintang_duration_result["word class = verb (vs. noun)","m.r.2.change"] = (chintang_duration_m.r.2 - r.squaredGLMM(chintang_duration_model.no_pos)[1]) / chintang_duration_m.r.2 * 100
chintang_duration_result["word class = verb (vs. noun)","c.r.2.change"] = (chintang_duration_c.r.2 - r.squaredGLMM(chintang_duration_model.no_pos)[2]) / chintang_duration_c.r.2 * 100
chintang_duration_result["word class = verb (vs. noun)","semi.partial.r2"] = chintang_duration_model.r2beta$Rsq[chintang_duration_model.r2beta$Effect=="ntvr_ps_root.simplifiedV"]
chintang_duration_result["log speech rate","m.r.2.change"] = (chintang_duration_m.r.2 - r.squaredGLMM(chintang_duration_model.no_speech_rate)[1]) / chintang_duration_m.r.2 * 100
chintang_duration_result["log speech rate","c.r.2.change"] = (chintang_duration_c.r.2 - r.squaredGLMM(chintang_duration_model.no_speech_rate)[2]) / chintang_duration_c.r.2 * 100
chintang_duration_result["log speech rate","semi.partial.r2"] = chintang_duration_model.r2beta$Rsq[chintang_duration_model.r2beta$Effect=="log_speed_per_record2"]

# Save the results concerning the main predictor of interest, namely, word frequency, in table for cross-linguistic comparison
results.word_duration.lmer$result[results.word_duration.lmer$language=="Chintang" & results.word_duration.lmer$parameter=="coefficient"] = chintang_duration_result["log word frequency","Estimate"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Chintang" & results.word_duration.lmer$parameter=="beta"] = chintang_duration_result["log word frequency","beta"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Chintang" & results.word_duration.lmer$parameter=="chisq"] = chintang_duration_result["log word frequency","chisq"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Chintang" & results.word_duration.lmer$parameter=="df"] = chintang_duration_result["log word frequency","df"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Chintang" & results.word_duration.lmer$parameter=="p"] = chintang_duration_result["log word frequency","p"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Chintang" & results.word_duration.lmer$parameter=="p.stars"] = chintang_duration_result["log word frequency","stars"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Chintang" & results.word_duration.lmer$parameter=="m.r.2.change"] = chintang_duration_result["log word frequency","m.r.2.change"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Chintang" & results.word_duration.lmer$parameter=="c.r.2.change"] = chintang_duration_result["log word frequency","c.r.2.change"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Chintang" & results.word_duration.lmer$parameter=="semi.partial.r2"] = chintang_duration_result["log word frequency","semi.partial.r2"]

# Create a table of the random effects in the model
chintang_duration_random = as.data.frame(summary(chintang_duration_model)$varcor)
colnames(chintang_duration_random)[1] = "random_effect"
chintang_duration_random$groups = c(summary(chintang_duration_model)$ngrps,NA)
chintang_duration_random = chintang_duration_random[c("random_effect","groups","sdcor")]

# Give the random effects sensible names
chintang_duration_random$random_effect[chintang_duration_random$random_effect=="ELANParticipant"] = "speaker"
chintang_duration_random$random_effect[chintang_duration_random$random_effect=="ntvr_file"] = "text"

# And order them consistently
ordering = c(order(chintang_duration_random$random_effect)[2:3],order(chintang_duration_random$random_effect)[1])
chintang_duration_random = chintang_duration_random[ordering,]

# Add columns for various tests of the random effects
chintang_duration_random$m.r.2.change = NA
chintang_duration_random$c.r.2.change = NA
chintang_duration_random$chisq = NA
chintang_duration_random$df = NA
chintang_duration_random$p = NA
chintang_duration_random$sig = NA
  
# Train a similar model but exclude speaker as a random effect
chintang_duration_model.no_speaker = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ntvr_file), data=chintang_100_tokens, REML=TRUE)

# Train a similar model but exclude text as a random effect
chintang_duration_model.no_text = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant), data=chintang_100_tokens, REML=TRUE)

# Compare model without speaker as a random effect to full model and calculate log-likelihood ratio test as well as percentage change in marginal and conditional R²
comparison = anova(chintang_duration_model,chintang_duration_model.no_speaker,test="Chisq")
chintang_duration_random$chisq[chintang_duration_random$random_effect=="speaker"] = comparison$Chisq[2]
chintang_duration_random$df[chintang_duration_random$random_effect=="speaker"] = comparison$"Chi Df"[2]
chintang_duration_random$p[chintang_duration_random$random_effect=="speaker"] = comparison$"Pr(>Chisq)"[2]
chintang_duration_random$sig[chintang_duration_random$random_effect=="speaker"] = sub("^\\.$","n.s.",sub(" ","n.s.",stars.pval(comparison$"Pr(>Chisq)"[2])))
chintang_duration_random$m.r.2.change[chintang_duration_random$random_effect=="speaker"] = (chintang_duration_m.r.2 - r.squaredGLMM(chintang_duration_model.no_speaker)[1]) / chintang_duration_m.r.2 * 100
chintang_duration_random$c.r.2.change[chintang_duration_random$random_effect=="speaker"] = (chintang_duration_c.r.2 - r.squaredGLMM(chintang_duration_model.no_speaker)[2]) / chintang_duration_c.r.2 * 100

# Compare model without text as a random effect to full model and calculate log-likelihood ratio test as well as percentage change in marginal and conditional R²
comparison = anova(chintang_duration_model,chintang_duration_model.no_text,test="Chisq")
chintang_duration_random$chisq[chintang_duration_random$random_effect=="text"] = comparison$Chisq[2]
chintang_duration_random$df[chintang_duration_random$random_effect=="text"] = comparison$"Chi Df"[2]
chintang_duration_random$p[chintang_duration_random$random_effect=="text"] = comparison$"Pr(>Chisq)"[2]
chintang_duration_random$sig[chintang_duration_random$random_effect=="text"] = sub("^\\.$","n.s.",sub(" ","n.s.",stars.pval(comparison$"Pr(>Chisq)"[2])))
chintang_duration_random$m.r.2.change[chintang_duration_random$random_effect=="text"] = (chintang_duration_m.r.2 - r.squaredGLMM(chintang_duration_model.no_text)[1]) / chintang_duration_m.r.2 * 100
chintang_duration_random$c.r.2.change[chintang_duration_random$random_effect=="text"] = (chintang_duration_c.r.2 - r.squaredGLMM(chintang_duration_model.no_text)[2]) / chintang_duration_c.r.2 * 100

# Also save the number of tokens included in the model as well as the marginal and conditional R² of the complete model in the table for cross-linguistic comparison
results.word_duration.lmer$result[results.word_duration.lmer$language=="Chintang" & results.word_duration.lmer$parameter=="n"] = summary(chintang_duration_model)$devcomp$dims[1]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Chintang" & results.word_duration.lmer$parameter=="m.r.2"] = r.squaredGLMM(chintang_duration_model)[1]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Chintang" & results.word_duration.lmer$parameter=="c.r.2"] = r.squaredGLMM(chintang_duration_model)[2]
```

```{r analysis_chintang_without_pear_stories, echo=FALSE}

# Make a list of the 100 most frequent word types
chintang2_unique = unique(chintang2[,c("word_type","word_type2","whole_word_gloss","ntvr_ps_root.simplified","word_freq","word_rel_freq","word_log_freq","word_log_rel_freq","word_freq_rank","word_freq_rank2","doc_freq","doc_freq_rank")])
chintang2_100 = head(chintang2_unique[order(chintang2_unique$word_freq_rank,chintang2_unique$doc_freq_rank),],100)

# Set row names to null
rownames(chintang2_100) = NULL

# Calculate the mean and standard deviation of word duration and articulation speed per word using a for loop
# Fill these calculated values into the table of the 100 most frequent word types
chintang2_100$mean_duration_per_word = NA
chintang2_100$sd_duration_per_word = NA
chintang2_100$mean_speed_per_word = NA
chintang2_100$sd_speed_per_word = NA
for (word_type2 in unique(chintang2_100$word_type2)) {
  chintang2_100$mean_speed_per_word[chintang2_100$word_type2==word_type2] = mean(chintang2$speed_per_word[chintang2$word_type2==word_type2],na.rm=T)
  chintang2_100$sd_speed_per_word[chintang2_100$word_type2==word_type2] = sd(chintang2$speed_per_word[chintang2$word_type2==word_type2],na.rm=T)
  chintang2_100$mean_duration_per_word[chintang2_100$word_type2==word_type2] = mean(chintang2$WordLength[chintang2$word_type2==word_type2],na.rm=T)
  chintang2_100$sd_duration_per_word[chintang2_100$word_type2==word_type2] = sd(chintang2$WordLength[chintang2$word_type2==word_type2],na.rm=T)
}

# Mininum frequency
chintang2_min_frequency = min(chintang2_100$word_freq)

# Collect a data set containing all tokens (instances) of these 100 most frequent word types in the corpus
chintang2_100_word_types = paste(chintang2_100$word_type,chintang2_100$whole_word_gloss,chintang2_100$ntvr_ps_root.simplified,sep="|")
chintang2_100_tokens = droplevels(subset(chintang2,subset=word_type2 %in% chintang2_100_word_types))

# Add another column to the data preparation table ("word types manually excluded")
data_preparation_table$tokens_100_excluded[data_preparation_table$language=="Chintang (no pear stories)"] = length(chintang2_100_tokens$word)

# Exclude tokens contained in single-word annotation units
chintang2_100_tokens = droplevels(subset(chintang2_100_tokens,subset=words_per_record>1))
chintang2_100_tokens = droplevels(subset(chintang2_100_tokens,subset=chars_per_record>chars_per_word))

# Add another column to the data preparation table ("no one-word utterances (final data set)")
data_preparation_table$tokens_100_final[data_preparation_table$language=="Chintang (no pear stories)"] = length(chintang2_100_tokens$word)

# Delete column in table of the 100 most frequent words that is no longer needed
chintang2_100$word_type2 = NULL

# Similarly for excluded words (that are occur on one of the blacklists)
chintang2_unique_backup = unique(chintang2_backup[,c("word_type","word_type2","whole_word_gloss","ntvr_ps_root.simplified","word_freq","word_rel_freq","doc_freq")])
chintang2_excluded = subset(chintang2_unique_backup[order(-chintang2_unique_backup$word_freq,-chintang2_unique_backup$doc_freq),],subset=word_type2 %in% chintang_aux_blacklist | word_type2 %in% chintang_n_blacklist)

# Just for statistics, count the number of instances (tokens) of the 100 most frequent words without excluding any specific words
chintang2_100_backup = head(chintang2_unique_backup[order(-chintang2_unique_backup$word_freq,-chintang2_unique_backup$doc_freq),],100)
chintang2_100_word_types_backup = chintang2_100_backup$word_type2
chintang2_100_tokens_backup = droplevels(subset(chintang2_backup,subset=word_type2 %in% chintang2_100_word_types_backup))

# Add a column to the data preparation table ("100 most frequent word types")
data_preparation_table$tokens_100[data_preparation_table$language=="Chintang (no pear stories)"] = length(chintang2_100_tokens_backup$word)

# Only keep those excluded words that are at least as frequent as the 100th most frequent word)
chintang2_excluded = subset(chintang2_excluded,subset=word_freq >= chintang2_min_frequency)

# Set rownames to null
rownames(chintang2_excluded) = NULL

# Calculate the mean and standard deviation of word duration and articulation speed per word using a for loop
# Fill these calculated values into the table of excluded word types
chintang2_excluded$mean_duration_per_word = NA
chintang2_excluded$sd_duration_per_word = NA
chintang2_excluded$mean_speed_per_word = NA
chintang2_excluded$sd_speed_per_word = NA
for (word_type2 in unique(chintang2_excluded$word_type2)) {
  chintang2_excluded$mean_speed_per_word[chintang2_excluded$word_type2==word_type2] = mean(chintang2_backup$speed_per_word[chintang2_backup$word_type2==word_type2],na.rm=T)
  chintang2_excluded$sd_speed_per_word[chintang2_excluded$word_type2==word_type2] = sd(chintang2_backup$speed_per_word[chintang2_backup$word_type2==word_type2],na.rm=T)
  chintang2_excluded$mean_duration_per_word[chintang2_excluded$word_type2==word_type2] = mean(chintang2_backup$WordLength[chintang2_backup$word_type2==word_type2],na.rm=T)
  chintang2_excluded$sd_duration_per_word[chintang2_excluded$word_type2==word_type2] = sd(chintang2_backup$WordLength[chintang2_backup$word_type2==word_type2],na.rm=T)
}

# Delete column in table of excluded word types that is no longer needed
chintang2_excluded$word_type2 = NULL
```

```{r chintang_without_pear_stories_correlation_test, echo=FALSE}

# Carry out a bivariate correlation test using Spearman's rank correlation coefficient
chintang2.cor.test = cor.test(chintang2_100$word_log_rel_freq,chintang2_100$mean_duration_per_word,method="spearman")

# Save the results in the table prepared for the bivariate analysis results
results.word_duration$result[results.word_duration$language=="Chintang (no pear stories)" & results.word_duration$parameter=="spearman.rho"] = chintang2.cor.test$estimate
results.word_duration$result[results.word_duration$language=="Chintang (no pear stories)" & results.word_duration$parameter=="spearman.S"] = chintang2.cor.test$statistic
results.word_duration$result[results.word_duration$language=="Chintang (no pear stories)" & results.word_duration$parameter=="spearman.p"] = chintang2.cor.test$p.value
results.word_duration$result[results.word_duration$language=="Chintang (no pear stories)" & results.word_duration$parameter=="spearman.p.stars"] = stars.pval(chintang2.cor.test$p.value)
results.word_duration$result[results.word_duration$language=="Chintang (no pear stories)" & results.word_duration$parameter=="spearman.p.stars" & results.word_duration$result==" "] = "n.s."
```

```{r chintang_without_pear_stories_duration_model, echo=FALSE, size="footnotesize"}

# Convert text names and word types into factors
chintang2_100_tokens$ntvr_file = factor(chintang2_100_tokens$ntvr_file)
chintang2_100_tokens$word_type2 = factor(chintang2_100_tokens$word_type2)

# Train a linear mixed-effects model of log word duration predicted by log relative frequency, word length in characters, the number of morphemes per word, its position within the utterance,
# its part-of-speech (noun or verb), the articulation speed in the surrounding utterance and random factors for speaker (ELANParticipant) and text (ntvr_file)
chintang2_duration_model = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=chintang2_100_tokens, REML=TRUE)

# Create a table of model parameters for the Baure linear mixed-effects model
# Include raw coefficients
chintang2_duration_result = as.data.frame(coef(summary(chintang2_duration_model)))

# standardized beta coefficients
chintang2_duration_result$beta = c(NA, lm.beta.lmer(chintang2_duration_model))

# prepare table cells for change in marginal and conditional R² as well as semi-partial R²
chintang2_duration_result$m.r.2.change = NA
chintang2_duration_result$c.r.2.change = NA
chintang2_duration_result$semi.partial.r2 = NA

# Add sensible names to the coefficients
rownames(chintang2_duration_result) = c("(Intercept)","log word frequency","word length","number of morphemes","position","word class = verb (vs. noun)","log speech rate")

# Perform drop1 log-likelihood ratio tests for all fixed factors in the model
chintang2_duration_result_drop1 = drop1(chintang2_duration_model,test="Chisq")
chintang2_duration_result$chisq = chintang2_duration_result_drop1$LRT
chintang2_duration_result$df = chintang2_duration_result_drop1$Df
chintang2_duration_result$p = chintang2_duration_result_drop1$"Pr(Chi)"

# Add significance stars to the results of the log-likelihood ratio tests
chintang2_duration_result$stars = stars.pval(chintang2_duration_result$p)
chintang2_duration_result$stars[1] = NA
chintang2_duration_result$stars = sub(" ", "n.s.", chintang2_duration_result$stars)
chintang2_duration_result$stars = sub("^\\.$", "n.s.", chintang2_duration_result$stars)

# Train a similar model but exclude word frequency as a fixed effect
chintang2_duration_model.no_word_freq = lmer(log_word_length ~ chars_per_word + morphs_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=chintang2_100_tokens, REML=TRUE)

# Train a similar model but exclude word length as a fixed effect
chintang2_duration_model.no_word_length = lmer(log_word_length ~ word_log_rel_freq + morphs_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=chintang2_100_tokens, REML=TRUE)

# Train a similar model but exclude the number of morphemes as a fixed effect
chintang2_duration_model.no_morphs = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=chintang2_100_tokens, REML=TRUE)

# Train a similar model but exclude word position as a fixed effect
chintang2_duration_model.no_position = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=chintang2_100_tokens, REML=TRUE)

# Train a similar model but exclude part-of-speech as a fixed effect
chintang2_duration_model.no_pos = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + position_percentage + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=chintang2_100_tokens, REML=TRUE)

# Train a similar model but exclude articulation rate in the context as a fixed effect
chintang2_duration_model.no_speech_rate = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + position_percentage + ntvr_ps_root.simplified + (1|ELANParticipant) + (1|ntvr_file), data=chintang2_100_tokens, REML=TRUE)

# Calculate the marginal and conditional R² of the complete model
chintang2_duration_m.r.2 = r.squaredGLMM(chintang2_duration_model)[1]
chintang2_duration_c.r.2 = r.squaredGLMM(chintang2_duration_model)[2]

# Calculate semi-partial R² for the fixed effects in the complete model
chintang2_duration_model.r2beta = r2beta(model=chintang2_duration_model,method="nsj",partial=T)

# Calculate the percentage change in marginal and conditional R² as well as the semi-partial R² for each fixed effect in the model
chintang2_duration_result["log word frequency","m.r.2.change"] = (chintang2_duration_m.r.2 - r.squaredGLMM(chintang2_duration_model.no_word_freq)[1]) / chintang2_duration_m.r.2 * 100
chintang2_duration_result["log word frequency","c.r.2.change"] = (chintang2_duration_c.r.2 - r.squaredGLMM(chintang2_duration_model.no_word_freq)[2]) / chintang2_duration_c.r.2 * 100
chintang2_duration_result["log word frequency","semi.partial.r2"] = chintang2_duration_model.r2beta$Rsq[chintang2_duration_model.r2beta$Effect=="word_log_rel_freq"]
chintang2_duration_result["word length","m.r.2.change"] = (chintang2_duration_m.r.2 - r.squaredGLMM(chintang2_duration_model.no_word_length)[1]) / chintang2_duration_m.r.2 * 100
chintang2_duration_result["word length","c.r.2.change"] = (chintang2_duration_c.r.2 - r.squaredGLMM(chintang2_duration_model.no_word_length)[2]) / chintang2_duration_c.r.2 * 100
chintang2_duration_result["word length","semi.partial.r2"] = chintang2_duration_model.r2beta$Rsq[chintang2_duration_model.r2beta$Effect=="chars_per_word"]
chintang2_duration_result["number of morphemes","m.r.2.change"] = (chintang2_duration_m.r.2 - r.squaredGLMM(chintang2_duration_model.no_morphs)[1]) / chintang2_duration_m.r.2 * 100
chintang2_duration_result["number of morphemes","c.r.2.change"] = (chintang2_duration_c.r.2 - r.squaredGLMM(chintang2_duration_model.no_morphs)[2]) / chintang2_duration_c.r.2 * 100
chintang2_duration_result["number of morphemes","semi.partial.r2"] = chintang2_duration_model.r2beta$Rsq[chintang2_duration_model.r2beta$Effect=="morphs_per_word"]
chintang2_duration_result["position","m.r.2.change"] = (chintang2_duration_m.r.2 - r.squaredGLMM(chintang2_duration_model.no_position)[1]) / chintang2_duration_m.r.2 * 100
chintang2_duration_result["position","c.r.2.change"] = (chintang2_duration_c.r.2 - r.squaredGLMM(chintang2_duration_model.no_position)[2]) / chintang2_duration_c.r.2 * 100
chintang2_duration_result["position","semi.partial.r2"] = chintang2_duration_model.r2beta$Rsq[chintang2_duration_model.r2beta$Effect=="position_percentage"]
chintang2_duration_result["word class = verb (vs. noun)","m.r.2.change"] = (chintang2_duration_m.r.2 - r.squaredGLMM(chintang2_duration_model.no_pos)[1]) / chintang2_duration_m.r.2 * 100
chintang2_duration_result["word class = verb (vs. noun)","c.r.2.change"] = (chintang2_duration_c.r.2 - r.squaredGLMM(chintang2_duration_model.no_pos)[2]) / chintang2_duration_c.r.2 * 100
chintang2_duration_result["word class = verb (vs. noun)","semi.partial.r2"] = chintang2_duration_model.r2beta$Rsq[chintang2_duration_model.r2beta$Effect=="ntvr_ps_root.simplifiedV"]
chintang2_duration_result["log speech rate","m.r.2.change"] = (chintang2_duration_m.r.2 - r.squaredGLMM(chintang2_duration_model.no_speech_rate)[1]) / chintang2_duration_m.r.2 * 100
chintang2_duration_result["log speech rate","c.r.2.change"] = (chintang2_duration_c.r.2 - r.squaredGLMM(chintang2_duration_model.no_speech_rate)[2]) / chintang2_duration_c.r.2 * 100
chintang2_duration_result["log speech rate","semi.partial.r2"] = chintang2_duration_model.r2beta$Rsq[chintang2_duration_model.r2beta$Effect=="log_speed_per_record2"]

# Save the results concerning the main predictor of interest, namely, word frequency, in table for cross-linguistic comparison
results.word_duration.lmer$result[results.word_duration.lmer$language=="Chintang (no pear stories)" & results.word_duration.lmer$parameter=="coefficient"] = chintang2_duration_result["log word frequency","Estimate"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Chintang (no pear stories)" & results.word_duration.lmer$parameter=="beta"] = chintang2_duration_result["log word frequency","beta"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Chintang (no pear stories)" & results.word_duration.lmer$parameter=="chisq"] = chintang2_duration_result["log word frequency","chisq"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Chintang (no pear stories)" & results.word_duration.lmer$parameter=="df"] = chintang2_duration_result["log word frequency","df"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Chintang (no pear stories)" & results.word_duration.lmer$parameter=="p"] = chintang2_duration_result["log word frequency","p"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Chintang (no pear stories)" & results.word_duration.lmer$parameter=="p.stars"] = chintang2_duration_result["log word frequency","stars"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Chintang (no pear stories)" & results.word_duration.lmer$parameter=="m.r.2.change"] = chintang2_duration_result["log word frequency","m.r.2.change"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Chintang (no pear stories)" & results.word_duration.lmer$parameter=="c.r.2.change"] = chintang2_duration_result["log word frequency","c.r.2.change"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Chintang (no pear stories)" & results.word_duration.lmer$parameter=="semi.partial.r2"] = chintang2_duration_result["log word frequency","semi.partial.r2"]

# Create a table of the random effects in the model
chintang2_duration_random = as.data.frame(summary(chintang2_duration_model)$varcor)
colnames(chintang2_duration_random)[1] = "random_effect"
chintang2_duration_random$groups = c(summary(chintang2_duration_model)$ngrps,NA)
chintang2_duration_random = chintang2_duration_random[c("random_effect","groups","sdcor")]

# Give the random effects sensible names
chintang2_duration_random$random_effect[chintang2_duration_random$random_effect=="ELANParticipant"] = "speaker"
chintang2_duration_random$random_effect[chintang2_duration_random$random_effect=="ntvr_file"] = "text"

# And order them consistently
ordering = c(order(chintang2_duration_random$random_effect)[2:3],order(chintang2_duration_random$random_effect)[1])
chintang2_duration_random = chintang2_duration_random[ordering,]

# Add columns for various tests of the random effects
chintang2_duration_random$m.r.2.change = NA
chintang2_duration_random$c.r.2.change = NA
chintang2_duration_random$chisq = NA
chintang2_duration_random$df = NA
chintang2_duration_random$p = NA
chintang2_duration_random$sig = NA
  
# Train a similar model but exclude speaker as a random effect
chintang2_duration_model.no_speaker = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ntvr_file), data=chintang2_100_tokens, REML=TRUE)

# Train a similar model but exclude text as a random effect
chintang2_duration_model.no_text = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant), data=chintang2_100_tokens, REML=TRUE)

# Compare model without speaker as a random effect to full model and calculate log-likelihood ratio test as well as percentage change in marginal and conditional R²
comparison = anova(chintang2_duration_model,chintang2_duration_model.no_speaker,test="Chisq")
chintang2_duration_random$chisq[chintang2_duration_random$random_effect=="speaker"] = comparison$Chisq[2]
chintang2_duration_random$df[chintang2_duration_random$random_effect=="speaker"] = comparison$"Chi Df"[2]
chintang2_duration_random$p[chintang2_duration_random$random_effect=="speaker"] = comparison$"Pr(>Chisq)"[2]
chintang2_duration_random$sig[chintang2_duration_random$random_effect=="speaker"] = sub("^\\.$","n.s.",sub(" ","n.s.",stars.pval(comparison$"Pr(>Chisq)"[2])))
chintang2_duration_random$m.r.2.change[chintang2_duration_random$random_effect=="speaker"] = (chintang2_duration_m.r.2 - r.squaredGLMM(chintang2_duration_model.no_speaker)[1]) / chintang2_duration_m.r.2 * 100
chintang2_duration_random$c.r.2.change[chintang2_duration_random$random_effect=="speaker"] = (chintang2_duration_c.r.2 - r.squaredGLMM(chintang2_duration_model.no_speaker)[2]) / chintang2_duration_c.r.2 * 100

# Compare model without text as a random effect to full model and calculate log-likelihood ratio test as well as percentage change in marginal and conditional R²
comparison = anova(chintang2_duration_model,chintang2_duration_model.no_text,test="Chisq")
chintang2_duration_random$chisq[chintang2_duration_random$random_effect=="text"] = comparison$Chisq[2]
chintang2_duration_random$df[chintang2_duration_random$random_effect=="text"] = comparison$"Chi Df"[2]
chintang2_duration_random$p[chintang2_duration_random$random_effect=="text"] = comparison$"Pr(>Chisq)"[2]
chintang2_duration_random$sig[chintang2_duration_random$random_effect=="text"] = sub("^\\.$","n.s.",sub(" ","n.s.",stars.pval(comparison$"Pr(>Chisq)"[2])))
chintang2_duration_random$m.r.2.change[chintang2_duration_random$random_effect=="text"] = (chintang2_duration_m.r.2 - r.squaredGLMM(chintang2_duration_model.no_text)[1]) / chintang2_duration_m.r.2 * 100
chintang2_duration_random$c.r.2.change[chintang2_duration_random$random_effect=="text"] = (chintang2_duration_c.r.2 - r.squaredGLMM(chintang2_duration_model.no_text)[2]) / chintang2_duration_c.r.2 * 100

# Also save the number of tokens included in the model as well as the marginal and conditional R² of the complete model in the table for cross-linguistic comparison
results.word_duration.lmer$result[results.word_duration.lmer$language=="Chintang (no pear stories)" & results.word_duration.lmer$parameter=="n"] = summary(chintang2_duration_model)$devcomp$dims[1]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Chintang (no pear stories)" & results.word_duration.lmer$parameter=="m.r.2"] = r.squaredGLMM(chintang2_duration_model)[1]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Chintang (no pear stories)" & results.word_duration.lmer$parameter=="c.r.2"] = r.squaredGLMM(chintang2_duration_model)[2]
```

```{r analysis_dutch, echo=FALSE}

# Make a list of the 100 most frequent word types
dutch_unique = unique(dutch[,c("word_type","word_type2","whole_word_gloss","ntvr_ps_root.simplified","word_freq","word_rel_freq","word_log_freq","word_log_rel_freq","word_freq_rank","word_freq_rank2","doc_freq","doc_freq_rank")])
dutch_100 = head(dutch_unique[order(dutch_unique$word_freq_rank,dutch_unique$doc_freq_rank),],100)

# Set row names to null
rownames(dutch_100) = NULL

# Calculate the mean and standard deviation of word duration and articulation speed per word using a for loop
# Fill these calculated values into the table of the 100 most frequent word types
dutch_100$mean_duration_per_word = NA
dutch_100$sd_duration_per_word = NA
dutch_100$mean_speed_per_word = NA
dutch_100$sd_speed_per_word = NA
for (word_type2 in unique(dutch_100$word_type2)) {
  dutch_100$mean_speed_per_word[dutch_100$word_type2==word_type2] = mean(dutch$speed_per_word[dutch$word_type2==word_type2],na.rm=T)
  dutch_100$sd_speed_per_word[dutch_100$word_type2==word_type2] = sd(dutch$speed_per_word[dutch$word_type2==word_type2],na.rm=T)
  dutch_100$mean_duration_per_word[dutch_100$word_type2==word_type2] = mean(dutch$WordLength[dutch$word_type2==word_type2],na.rm=T)
  dutch_100$sd_duration_per_word[dutch_100$word_type2==word_type2] = sd(dutch$WordLength[dutch$word_type2==word_type2],na.rm=T)
}

# Mininum frequency
dutch_min_frequency = min(dutch_100$word_freq)

# Collect a data set containing all tokens (instances) of these 100 most frequent word types in the corpus
dutch_100_word_types = paste(dutch_100$word_type,dutch_100$whole_word_gloss,dutch_100$ntvr_ps_root.simplified,sep="|")
dutch_100_tokens = droplevels(subset(dutch,subset=word_type2 %in% dutch_100_word_types))

# Add another column to the data preparation table ("word types manually excluded")
data_preparation_table$tokens_100_excluded[data_preparation_table$language=="Dutch"] = length(dutch_100_tokens$word)

# Exclude tokens contained in single-word annotation units
dutch_100_tokens = droplevels(subset(dutch_100_tokens,subset=words_per_record>1))
dutch_100_tokens = droplevels(subset(dutch_100_tokens,subset=chars_per_record>chars_per_word))

# Add another column to the data preparation table ("no one-word utterances (final data set)")
data_preparation_table$tokens_100_final[data_preparation_table$language=="Dutch"] = length(dutch_100_tokens$word)

# Delete column in table of the 100 most frequent words that is no longer needed
dutch_100$word_type2 = NULL

# Similarly for excluded words (that are occur on one of the blacklists)
dutch_unique_backup = unique(dutch_backup[,c("word_type","word_type2","whole_word_gloss","ntvr_ps_root.simplified","word_freq","word_rel_freq","doc_freq")])
dutch_excluded = subset(dutch_unique_backup[order(-dutch_unique_backup$word_freq,-dutch_unique_backup$doc_freq),],subset=word_type2 %in% dutch_aux_blacklist | word_type2 %in% dutch_n_blacklist)

# Just for statistics, count the number of instances (tokens) of the 100 most frequent words without excluding any specific words
dutch_100_backup = head(dutch_unique_backup[order(-dutch_unique_backup$word_freq,-dutch_unique_backup$doc_freq),],100)
dutch_100_word_types_backup = dutch_100_backup$word_type2
dutch_100_tokens_backup = droplevels(subset(dutch_backup,subset=word_type2 %in% dutch_100_word_types_backup))

# Add a column to the data preparation table ("100 most frequent word types")
data_preparation_table$tokens_100[data_preparation_table$language=="Dutch"] = length(dutch_100_tokens_backup$word)

# Only keep those excluded words that are at least as frequent as the 100th most frequent word)
dutch_excluded = subset(dutch_excluded,subset=word_freq >= dutch_min_frequency)

# Set rownames to null
rownames(dutch_excluded) = NULL

# Calculate the mean and standard deviation of word duration and articulation speed per word using a for loop
# Fill these calculated values into the table of excluded word types
dutch_excluded$mean_duration_per_word = NA
dutch_excluded$sd_duration_per_word = NA
dutch_excluded$mean_speed_per_word = NA
dutch_excluded$sd_speed_per_word = NA
for (word_type2 in unique(dutch_excluded$word_type2)) {
  dutch_excluded$mean_speed_per_word[dutch_excluded$word_type2==word_type2] = mean(dutch_backup$speed_per_word[dutch_backup$word_type2==word_type2],na.rm=T)
  dutch_excluded$sd_speed_per_word[dutch_excluded$word_type2==word_type2] = sd(dutch_backup$speed_per_word[dutch_backup$word_type2==word_type2],na.rm=T)
  dutch_excluded$mean_duration_per_word[dutch_excluded$word_type2==word_type2] = mean(dutch_backup$WordLength[dutch_backup$word_type2==word_type2],na.rm=T)
  dutch_excluded$sd_duration_per_word[dutch_excluded$word_type2==word_type2] = sd(dutch_backup$WordLength[dutch_backup$word_type2==word_type2],na.rm=T)
}

# Delete column in table of excluded word types that is no longer needed
dutch_excluded$word_type2 = NULL
```

```{r dutch_correlation_test, echo=FALSE}

# Carry out a bivariate correlation test using Spearman's rank correlation coefficient
dutch.cor.test = cor.test(dutch_100$word_log_rel_freq,dutch_100$mean_duration_per_word,method="spearman")

# Save the results in the table prepared for the bivariate analysis results
results.word_duration$result[results.word_duration$language=="Dutch" & results.word_duration$parameter=="spearman.rho"] = dutch.cor.test$estimate
results.word_duration$result[results.word_duration$language=="Dutch" & results.word_duration$parameter=="spearman.S"] = dutch.cor.test$statistic
results.word_duration$result[results.word_duration$language=="Dutch" & results.word_duration$parameter=="spearman.p"] = dutch.cor.test$p.value
results.word_duration$result[results.word_duration$language=="Dutch" & results.word_duration$parameter=="spearman.p.stars"] = stars.pval(dutch.cor.test$p.value)
results.word_duration$result[results.word_duration$language=="Dutch" & results.word_duration$parameter=="spearman.p.stars" & results.word_duration$result==" "] = "n.s."
```

```{r dutch_duration_model, echo=FALSE, size="footnotesize"}

# Convert text names and word types into factors
dutch_100_tokens$ntvr_file = factor(dutch_100_tokens$ntvr_file)
dutch_100_tokens$word_type2 = factor(dutch_100_tokens$word_type2)

# Train a linear mixed-effects model of log word duration predicted by log relative frequency, word length in characters, its position within the utterance,
# its part-of-speech (noun or verb), the articulation speed in the surrounding utterance and random factors for speaker (ELANParticipant) and text (ntvr_file)
# We needed to remove morphs_per_word in this version (not annotated correctly, always one)
dutch_duration_model = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=dutch_100_tokens, REML=TRUE)

# Create a table of model parameters for the Baure linear mixed-effects model
# Include raw coefficients
dutch_duration_result = as.data.frame(coef(summary(dutch_duration_model)))

# standardized beta coefficients
dutch_duration_result$beta = c(NA, lm.beta.lmer(dutch_duration_model))

# prepare table cells for change in marginal and conditional R² as well as semi-partial R²
dutch_duration_result$m.r.2.change = NA
dutch_duration_result$c.r.2.change = NA
dutch_duration_result$semi.partial.r2 = NA

# Add sensible names to the coefficients
rownames(dutch_duration_result) = c("(Intercept)","log word frequency","word length","position","word class = verb (vs. noun)","log speech rate")

# Perform drop1 log-likelihood ratio tests for all fixed factors in the model
dutch_duration_result_drop1 = drop1(dutch_duration_model,test="Chisq")
dutch_duration_result$chisq = dutch_duration_result_drop1$LRT
dutch_duration_result$df = dutch_duration_result_drop1$Df
dutch_duration_result$p = dutch_duration_result_drop1$"Pr(Chi)"

# Add significance stars to the results of the log-likelihood ratio tests
dutch_duration_result$stars = stars.pval(dutch_duration_result$p)
dutch_duration_result$stars[1] = NA
dutch_duration_result$stars = sub(" ", "n.s.", dutch_duration_result$stars)
dutch_duration_result$stars = sub("^\\.$", "n.s.", dutch_duration_result$stars)

# Train a similar model but exclude word frequency as a fixed effect
dutch_duration_model.no_word_freq = lmer(log_word_length ~ chars_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=dutch_100_tokens, REML=TRUE)

# Train a similar model but exclude word length as a fixed effect
dutch_duration_model.no_word_length = lmer(log_word_length ~ word_log_rel_freq + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=dutch_100_tokens, REML=TRUE)

# Train a similar model but exclude word position as a fixed effect
dutch_duration_model.no_position = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=dutch_100_tokens, REML=TRUE)

# Train a similar model but exclude part-of-speech as a fixed effect
dutch_duration_model.no_pos = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + position_percentage + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=dutch_100_tokens, REML=TRUE)

# Train a similar model but exclude articulation rate in the context as a fixed effect
dutch_duration_model.no_speech_rate = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + position_percentage + ntvr_ps_root.simplified + (1|ELANParticipant) + (1|ntvr_file), data=dutch_100_tokens, REML=TRUE)

# Calculate the marginal and conditional R² of the complete model
dutch_duration_m.r.2 = r.squaredGLMM(dutch_duration_model)[1]
dutch_duration_c.r.2 = r.squaredGLMM(dutch_duration_model)[2]

# Calculate semi-partial R² for the fixed effects in the complete model
dutch_duration_model.r2beta = r2beta(model=dutch_duration_model,method="nsj",partial=T)

# Calculate the percentage change in marginal and conditional R² as well as the semi-partial R² for each fixed effect in the model
dutch_duration_result["log word frequency","m.r.2.change"] = (dutch_duration_m.r.2 - r.squaredGLMM(dutch_duration_model.no_word_freq)[1]) / dutch_duration_m.r.2 * 100
dutch_duration_result["log word frequency","c.r.2.change"] = (dutch_duration_c.r.2 - r.squaredGLMM(dutch_duration_model.no_word_freq)[2]) / dutch_duration_c.r.2 * 100
dutch_duration_result["log word frequency","semi.partial.r2"] = dutch_duration_model.r2beta$Rsq[dutch_duration_model.r2beta$Effect=="word_log_rel_freq"]
dutch_duration_result["word length","m.r.2.change"] = (dutch_duration_m.r.2 - r.squaredGLMM(dutch_duration_model.no_word_length)[1]) / dutch_duration_m.r.2 * 100
dutch_duration_result["word length","c.r.2.change"] = (dutch_duration_c.r.2 - r.squaredGLMM(dutch_duration_model.no_word_length)[2]) / dutch_duration_c.r.2 * 100
dutch_duration_result["word length","semi.partial.r2"] = dutch_duration_model.r2beta$Rsq[dutch_duration_model.r2beta$Effect=="chars_per_word"]
dutch_duration_result["position","m.r.2.change"] = (dutch_duration_m.r.2 - r.squaredGLMM(dutch_duration_model.no_position)[1]) / dutch_duration_m.r.2 * 100
dutch_duration_result["position","c.r.2.change"] = (dutch_duration_c.r.2 - r.squaredGLMM(dutch_duration_model.no_position)[2]) / dutch_duration_c.r.2 * 100
dutch_duration_result["position","semi.partial.r2"] = dutch_duration_model.r2beta$Rsq[dutch_duration_model.r2beta$Effect=="position_percentage"]
dutch_duration_result["word class = verb (vs. noun)","m.r.2.change"] = (dutch_duration_m.r.2 - r.squaredGLMM(dutch_duration_model.no_pos)[1]) / dutch_duration_m.r.2 * 100
dutch_duration_result["word class = verb (vs. noun)","c.r.2.change"] = (dutch_duration_c.r.2 - r.squaredGLMM(dutch_duration_model.no_pos)[2]) / dutch_duration_c.r.2 * 100
dutch_duration_result["word class = verb (vs. noun)","semi.partial.r2"] = dutch_duration_model.r2beta$Rsq[dutch_duration_model.r2beta$Effect=="ntvr_ps_root.simplifiedV"]
dutch_duration_result["log speech rate","m.r.2.change"] = (dutch_duration_m.r.2 - r.squaredGLMM(dutch_duration_model.no_speech_rate)[1]) / dutch_duration_m.r.2 * 100
dutch_duration_result["log speech rate","c.r.2.change"] = (dutch_duration_c.r.2 - r.squaredGLMM(dutch_duration_model.no_speech_rate)[2]) / dutch_duration_c.r.2 * 100
dutch_duration_result["log speech rate","semi.partial.r2"] = dutch_duration_model.r2beta$Rsq[dutch_duration_model.r2beta$Effect=="log_speed_per_record2"]

# Save the results concerning the main predictor of interest, namely, word frequency, in table for cross-linguistic comparison
results.word_duration.lmer$result[results.word_duration.lmer$language=="Dutch" & results.word_duration.lmer$parameter=="coefficient"] = dutch_duration_result["log word frequency","Estimate"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Dutch" & results.word_duration.lmer$parameter=="beta"] = dutch_duration_result["log word frequency","beta"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Dutch" & results.word_duration.lmer$parameter=="chisq"] = dutch_duration_result["log word frequency","chisq"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Dutch" & results.word_duration.lmer$parameter=="df"] = dutch_duration_result["log word frequency","df"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Dutch" & results.word_duration.lmer$parameter=="p"] = dutch_duration_result["log word frequency","p"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Dutch" & results.word_duration.lmer$parameter=="p.stars"] = dutch_duration_result["log word frequency","stars"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Dutch" & results.word_duration.lmer$parameter=="m.r.2.change"] = dutch_duration_result["log word frequency","m.r.2.change"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Dutch" & results.word_duration.lmer$parameter=="c.r.2.change"] = dutch_duration_result["log word frequency","c.r.2.change"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Dutch" & results.word_duration.lmer$parameter=="semi.partial.r2"] = dutch_duration_result["log word frequency","semi.partial.r2"]

# Create a table of the random effects in the model
dutch_duration_random = as.data.frame(summary(dutch_duration_model)$varcor)
colnames(dutch_duration_random)[1] = "random_effect"
dutch_duration_random$groups = c(summary(dutch_duration_model)$ngrps,NA)
dutch_duration_random = dutch_duration_random[c("random_effect","groups","sdcor")]

# Give the random effects sensible names
dutch_duration_random$random_effect[dutch_duration_random$random_effect=="word_type2"] = "word type"
dutch_duration_random$random_effect[dutch_duration_random$random_effect=="ELANParticipant"] = "speaker"
dutch_duration_random$random_effect[dutch_duration_random$random_effect=="ntvr_file"] = "text"

# And order them consistently
ordering = c(order(dutch_duration_random$random_effect)[2:3],order(dutch_duration_random$random_effect)[1])
dutch_duration_random = dutch_duration_random[ordering,]

# Add columns for various tests of the random effects
dutch_duration_random$m.r.2.change = NA
dutch_duration_random$c.r.2.change = NA
dutch_duration_random$chisq = NA
dutch_duration_random$df = NA
dutch_duration_random$p = NA
dutch_duration_random$sig = NA
  
# Train a similar model but exclude speaker as a random effect
dutch_duration_model.no_speaker = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ntvr_file), data=dutch_100_tokens, REML=TRUE)

# Train a similar model but exclude text as a random effect
dutch_duration_model.no_text = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant), data=dutch_100_tokens, REML=TRUE)

# Compare model without speaker as a random effect to full model and calculate log-likelihood ratio test as well as percentage change in marginal and conditional R²
comparison = anova(dutch_duration_model,dutch_duration_model.no_speaker,test="Chisq")
dutch_duration_random$chisq[dutch_duration_random$random_effect=="speaker"] = comparison$Chisq[2]
dutch_duration_random$df[dutch_duration_random$random_effect=="speaker"] = comparison$"Chi Df"[2]
dutch_duration_random$p[dutch_duration_random$random_effect=="speaker"] = comparison$"Pr(>Chisq)"[2]
dutch_duration_random$sig[dutch_duration_random$random_effect=="speaker"] = sub("^\\.$","n.s.",sub(" ","n.s.",stars.pval(comparison$"Pr(>Chisq)"[2])))
dutch_duration_random$m.r.2.change[dutch_duration_random$random_effect=="speaker"] = (dutch_duration_m.r.2 - r.squaredGLMM(dutch_duration_model.no_speaker)[1]) / dutch_duration_m.r.2 * 100
dutch_duration_random$c.r.2.change[dutch_duration_random$random_effect=="speaker"] = (dutch_duration_c.r.2 - r.squaredGLMM(dutch_duration_model.no_speaker)[2]) / dutch_duration_c.r.2 * 100

# Compare model without text as a random effect to full model and calculate log-likelihood ratio test as well as percentage change in marginal and conditional R²
comparison = anova(dutch_duration_model,dutch_duration_model.no_text,test="Chisq")
dutch_duration_random$chisq[dutch_duration_random$random_effect=="text"] = comparison$Chisq[2]
dutch_duration_random$df[dutch_duration_random$random_effect=="text"] = comparison$"Chi Df"[2]
dutch_duration_random$p[dutch_duration_random$random_effect=="text"] = comparison$"Pr(>Chisq)"[2]
dutch_duration_random$sig[dutch_duration_random$random_effect=="text"] = sub("^\\.$","n.s.",sub(" ","n.s.",stars.pval(comparison$"Pr(>Chisq)"[2])))
dutch_duration_random$m.r.2.change[dutch_duration_random$random_effect=="text"] = (dutch_duration_m.r.2 - r.squaredGLMM(dutch_duration_model.no_text)[1]) / dutch_duration_m.r.2 * 100
dutch_duration_random$c.r.2.change[dutch_duration_random$random_effect=="text"] = (dutch_duration_c.r.2 - r.squaredGLMM(dutch_duration_model.no_text)[2]) / dutch_duration_c.r.2 * 100

# Also save the number of tokens included in the model as well as the marginal and conditional R² of the complete model in the table for cross-linguistic comparison
results.word_duration.lmer$result[results.word_duration.lmer$language=="Dutch" & results.word_duration.lmer$parameter=="n"] = summary(dutch_duration_model)$devcomp$dims[1]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Dutch" & results.word_duration.lmer$parameter=="m.r.2"] = r.squaredGLMM(dutch_duration_model)[1]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Dutch" & results.word_duration.lmer$parameter=="c.r.2"] = r.squaredGLMM(dutch_duration_model)[2]
```


```{r analysis_english, echo=FALSE}

# Make a list of the 100 most frequent word types
english_unique = unique(english[,c("word_type","word_type2","whole_word_gloss","ntvr_ps_root.simplified","word_freq","word_rel_freq","word_log_freq","word_log_rel_freq","word_freq_rank","word_freq_rank2","doc_freq","doc_freq_rank")])
english_100 = head(english_unique[order(english_unique$word_freq_rank,english_unique$doc_freq_rank),],100)

# Set row names to null
rownames(english_100) = NULL

# Calculate the mean and standard deviation of word duration and articulation speed per word using a for loop
# Fill these calculated values into the table of the 100 most frequent word types
english_100$mean_duration_per_word = NA
english_100$sd_duration_per_word = NA
english_100$mean_speed_per_word = NA
english_100$sd_speed_per_word = NA
for (word_type2 in unique(english_100$word_type2)) {
  english_100$mean_speed_per_word[english_100$word_type2==word_type2] = mean(english$speed_per_word[english$word_type2==word_type2],na.rm=T)
  english_100$sd_speed_per_word[english_100$word_type2==word_type2] = sd(english$speed_per_word[english$word_type2==word_type2],na.rm=T)
  english_100$mean_duration_per_word[english_100$word_type2==word_type2] = mean(english$WordLength[english$word_type2==word_type2],na.rm=T)
  english_100$sd_duration_per_word[english_100$word_type2==word_type2] = sd(english$WordLength[english$word_type2==word_type2],na.rm=T)
}

# Mininum frequency
english_min_frequency = min(english_100$word_freq)

# Collect a data set containing all tokens (instances) of these 100 most frequent word types in the corpus
english_100_word_types = paste(english_100$word_type,english_100$whole_word_gloss,english_100$ntvr_ps_root.simplified,sep="|")
english_100_tokens = droplevels(subset(english,subset=word_type2 %in% english_100_word_types))

# Add another column to the data preparation table ("word types manually excluded")
data_preparation_table$tokens_100_excluded[data_preparation_table$language=="English"] = length(english_100_tokens$word)

# Exclude tokens contained in single-word annotation units
english_100_tokens = droplevels(subset(english_100_tokens,subset=words_per_record>1))
english_100_tokens = droplevels(subset(english_100_tokens,subset=chars_per_record>chars_per_word))

# Add another column to the data preparation table ("no one-word utterances (final data set)")
data_preparation_table$tokens_100_final[data_preparation_table$language=="English"] = length(english_100_tokens$word)

# Delete column in table of the 100 most frequent words that is no longer needed
english_100$word_type2 = NULL

# Similarly for excluded words (that are occur on one of the blacklists)
english_unique_backup = unique(english_backup[,c("word_type","word_type2","whole_word_gloss","ntvr_ps_root.simplified","word_freq","word_rel_freq","doc_freq")])
english_excluded = subset(english_unique_backup[order(-english_unique_backup$word_freq,-english_unique_backup$doc_freq),],subset=word_type2 %in% english_aux_blacklist | word_type2 %in% english_n_blacklist)

# Just for statistics, count the number of instances (tokens) of the 100 most frequent words without excluding any specific words
english_100_backup = head(english_unique_backup[order(-english_unique_backup$word_freq,-english_unique_backup$doc_freq),],100)
english_100_word_types_backup = english_100_backup$word_type2
english_100_tokens_backup = droplevels(subset(english_backup,subset=word_type2 %in% english_100_word_types_backup))

# Add a column to the data preparation table ("100 most frequent word types")
data_preparation_table$tokens_100[data_preparation_table$language=="English"] = length(english_100_tokens_backup$word)

# Only keep those excluded words that are at least as frequent as the 100th most frequent word)
english_excluded = subset(english_excluded,subset=word_freq >= english_min_frequency)

# Set rownames to null
rownames(english_excluded) = NULL

# Calculate the mean and standard deviation of word duration and articulation speed per word using a for loop
# Fill these calculated values into the table of excluded word types
english_excluded$mean_duration_per_word = NA
english_excluded$sd_duration_per_word = NA
english_excluded$mean_speed_per_word = NA
english_excluded$sd_speed_per_word = NA
for (word_type2 in unique(english_excluded$word_type2)) {
  english_excluded$mean_speed_per_word[english_excluded$word_type2==word_type2] = mean(english_backup$speed_per_word[english_backup$word_type2==word_type2],na.rm=T)
  english_excluded$sd_speed_per_word[english_excluded$word_type2==word_type2] = sd(english_backup$speed_per_word[english_backup$word_type2==word_type2],na.rm=T)
  english_excluded$mean_duration_per_word[english_excluded$word_type2==word_type2] = mean(english_backup$WordLength[english_backup$word_type2==word_type2],na.rm=T)
  english_excluded$sd_duration_per_word[english_excluded$word_type2==word_type2] = sd(english_backup$WordLength[english_backup$word_type2==word_type2],na.rm=T)
}

# Delete column in table of excluded word types that is no longer needed
english_excluded$word_type2 = NULL
```

```{r english_correlation_test, echo=FALSE}

# Carry out a bivariate correlation test using Spearman's rank correlation coefficient
english.cor.test = cor.test(english_100$word_log_rel_freq,english_100$mean_duration_per_word,method="spearman")

# Save the results in the table prepared for the bivariate analysis results
results.word_duration$result[results.word_duration$language=="English" & results.word_duration$parameter=="spearman.rho"] = english.cor.test$estimate
results.word_duration$result[results.word_duration$language=="English" & results.word_duration$parameter=="spearman.S"] = english.cor.test$statistic
results.word_duration$result[results.word_duration$language=="English" & results.word_duration$parameter=="spearman.p"] = english.cor.test$p.value
results.word_duration$result[results.word_duration$language=="English" & results.word_duration$parameter=="spearman.p.stars"] = stars.pval(english.cor.test$p.value)
results.word_duration$result[results.word_duration$language=="English" & results.word_duration$parameter=="spearman.p.stars" & results.word_duration$result==" "] = "n.s."
```

```{r english_duration_model, echo=FALSE, size="footnotesize"}

# Convert text names and word types into factors
english_100_tokens$ntvr_file = factor(english_100_tokens$ntvr_file)
english_100_tokens$word_type2 = factor(english_100_tokens$word_type2)

# Train a linear mixed-effects model of log word duration predicted by log relative frequency, word length in characters, the number of morphemes per word, its position within the utterance,
# its part-of-speech (noun or verb), the articulation speed in the surrounding utterance and random factors for speaker (ELANParticipant) and text (ntvr_file)
english_duration_model = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=english_100_tokens, REML=TRUE)

# Create a table of model parameters for the Baure linear mixed-effects model
# Include raw coefficients
english_duration_result = as.data.frame(coef(summary(english_duration_model)))

# standardized beta coefficients
english_duration_result$beta = c(NA, lm.beta.lmer(english_duration_model))

# prepare table cells for change in marginal and conditional R² as well as semi-partial R²
english_duration_result$m.r.2.change = NA
english_duration_result$c.r.2.change = NA
english_duration_result$semi.partial.r2 = NA

# Add sensible names to the coefficients
rownames(english_duration_result) = c("(Intercept)","log word frequency","word length","number of morphemes","position","word class = verb (vs. noun)","log speech rate")

# Perform drop1 log-likelihood ratio tests for all fixed factors in the model
english_duration_result_drop1 = drop1(english_duration_model,test="Chisq")
english_duration_result$chisq = english_duration_result_drop1$LRT
english_duration_result$df = english_duration_result_drop1$Df
english_duration_result$p = english_duration_result_drop1$"Pr(Chi)"

# Add significance stars to the results of the log-likelihood ratio tests
english_duration_result$stars = stars.pval(english_duration_result$p)
english_duration_result$stars[1] = NA
english_duration_result$stars = sub(" ", "n.s.", english_duration_result$stars)
english_duration_result$stars = sub("^\\.$", "n.s.", english_duration_result$stars)

# Train a similar model but exclude word frequency as a fixed effect
english_duration_model.no_word_freq = lmer(log_word_length ~ chars_per_word + morphs_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=english_100_tokens, REML=TRUE)

# Train a similar model but exclude word length as a fixed effect
english_duration_model.no_word_length = lmer(log_word_length ~ word_log_rel_freq + morphs_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=english_100_tokens, REML=TRUE)

# Train a similar model but exclude the number of morphemes as a fixed effect
english_duration_model.no_morphs = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=english_100_tokens, REML=TRUE)

# Train a similar model but exclude word position as a fixed effect
english_duration_model.no_position = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=english_100_tokens, REML=TRUE)

# Train a similar model but exclude part-of-speech as a fixed effect
english_duration_model.no_pos = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + position_percentage + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=english_100_tokens, REML=TRUE)

# Train a similar model but exclude articulation rate in the context as a fixed effect
english_duration_model.no_speech_rate = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + position_percentage + ntvr_ps_root.simplified + (1|ELANParticipant) + (1|ntvr_file), data=english_100_tokens, REML=TRUE)

# Calculate the marginal and conditional R² of the complete model
english_duration_m.r.2 = r.squaredGLMM(english_duration_model)[1]
english_duration_c.r.2 = r.squaredGLMM(english_duration_model)[2]

# Calculate semi-partial R² for the fixed effects in the complete model
english_duration_model.r2beta = r2beta(model=english_duration_model,method="nsj",partial=T)

# Calculate the percentage change in marginal and conditional R² as well as the semi-partial R² for each fixed effect in the model
english_duration_result["log word frequency","m.r.2.change"] = (english_duration_m.r.2 - r.squaredGLMM(english_duration_model.no_word_freq)[1]) / english_duration_m.r.2 * 100
english_duration_result["log word frequency","c.r.2.change"] = (english_duration_c.r.2 - r.squaredGLMM(english_duration_model.no_word_freq)[2]) / english_duration_c.r.2 * 100
english_duration_result["log word frequency","semi.partial.r2"] = english_duration_model.r2beta$Rsq[english_duration_model.r2beta$Effect=="word_log_rel_freq"]
english_duration_result["word length","m.r.2.change"] = (english_duration_m.r.2 - r.squaredGLMM(english_duration_model.no_word_length)[1]) / english_duration_m.r.2 * 100
english_duration_result["word length","c.r.2.change"] = (english_duration_c.r.2 - r.squaredGLMM(english_duration_model.no_word_length)[2]) / english_duration_c.r.2 * 100
english_duration_result["word length","semi.partial.r2"] = english_duration_model.r2beta$Rsq[english_duration_model.r2beta$Effect=="chars_per_word"]
english_duration_result["number of morphemes","m.r.2.change"] = (english_duration_m.r.2 - r.squaredGLMM(english_duration_model.no_morphs)[1]) / english_duration_m.r.2 * 100
english_duration_result["number of morphemes","c.r.2.change"] = (english_duration_c.r.2 - r.squaredGLMM(english_duration_model.no_morphs)[2]) / english_duration_c.r.2 * 100
english_duration_result["number of morphemes","semi.partial.r2"] = english_duration_model.r2beta$Rsq[english_duration_model.r2beta$Effect=="morphs_per_word"]
english_duration_result["position","m.r.2.change"] = (english_duration_m.r.2 - r.squaredGLMM(english_duration_model.no_position)[1]) / english_duration_m.r.2 * 100
english_duration_result["position","c.r.2.change"] = (english_duration_c.r.2 - r.squaredGLMM(english_duration_model.no_position)[2]) / english_duration_c.r.2 * 100
english_duration_result["position","semi.partial.r2"] = english_duration_model.r2beta$Rsq[english_duration_model.r2beta$Effect=="position_percentage"]
english_duration_result["word class = verb (vs. noun)","m.r.2.change"] = (english_duration_m.r.2 - r.squaredGLMM(english_duration_model.no_pos)[1]) / english_duration_m.r.2 * 100
english_duration_result["word class = verb (vs. noun)","c.r.2.change"] = (english_duration_c.r.2 - r.squaredGLMM(english_duration_model.no_pos)[2]) / english_duration_c.r.2 * 100
english_duration_result["word class = verb (vs. noun)","semi.partial.r2"] = english_duration_model.r2beta$Rsq[english_duration_model.r2beta$Effect=="ntvr_ps_root.simplifiedV"]
english_duration_result["log speech rate","m.r.2.change"] = (english_duration_m.r.2 - r.squaredGLMM(english_duration_model.no_speech_rate)[1]) / english_duration_m.r.2 * 100
english_duration_result["log speech rate","c.r.2.change"] = (english_duration_c.r.2 - r.squaredGLMM(english_duration_model.no_speech_rate)[2]) / english_duration_c.r.2 * 100
english_duration_result["log speech rate","semi.partial.r2"] = english_duration_model.r2beta$Rsq[english_duration_model.r2beta$Effect=="log_speed_per_record2"]

# Save the results concerning the main predictor of interest, namely, word frequency, in table for cross-linguistic comparison
results.word_duration.lmer$result[results.word_duration.lmer$language=="English" & results.word_duration.lmer$parameter=="coefficient"] = english_duration_result["log word frequency","Estimate"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="English" & results.word_duration.lmer$parameter=="beta"] = english_duration_result["log word frequency","beta"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="English" & results.word_duration.lmer$parameter=="chisq"] = english_duration_result["log word frequency","chisq"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="English" & results.word_duration.lmer$parameter=="df"] = english_duration_result["log word frequency","df"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="English" & results.word_duration.lmer$parameter=="p"] = english_duration_result["log word frequency","p"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="English" & results.word_duration.lmer$parameter=="p.stars"] = english_duration_result["log word frequency","stars"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="English" & results.word_duration.lmer$parameter=="m.r.2.change"] = english_duration_result["log word frequency","m.r.2.change"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="English" & results.word_duration.lmer$parameter=="c.r.2.change"] = english_duration_result["log word frequency","c.r.2.change"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="English" & results.word_duration.lmer$parameter=="semi.partial.r2"] = english_duration_result["log word frequency","semi.partial.r2"]

# Create a table of the random effects in the model
english_duration_random = as.data.frame(summary(english_duration_model)$varcor)
colnames(english_duration_random)[1] = "random_effect"
english_duration_random$groups = c(summary(english_duration_model)$ngrps,NA)
english_duration_random = english_duration_random[c("random_effect","groups","sdcor")]

# Give the random effects sensible names
english_duration_random$random_effect[english_duration_random$random_effect=="ELANParticipant"] = "speaker"
english_duration_random$random_effect[english_duration_random$random_effect=="ntvr_file"] = "text"

# And order them consistently
ordering = c(order(english_duration_random$random_effect)[2:3],order(english_duration_random$random_effect)[1])
english_duration_random = english_duration_random[ordering,]

# Add columns for various tests of the random effects
english_duration_random$m.r.2.change = NA
english_duration_random$c.r.2.change = NA
english_duration_random$chisq = NA
english_duration_random$df = NA
english_duration_random$p = NA
english_duration_random$sig = NA
  
# Train a similar model but exclude speaker as a random effect
english_duration_model.no_speaker = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ntvr_file), data=english_100_tokens, REML=TRUE)

# Train a similar model but exclude text as a random effect
english_duration_model.no_text = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant), data=english_100_tokens, REML=TRUE)

# Compare model without speaker as a random effect to full model and calculate log-likelihood ratio test as well as percentage change in marginal and conditional R²
comparison = anova(english_duration_model,english_duration_model.no_speaker,test="Chisq")
english_duration_random$chisq[english_duration_random$random_effect=="speaker"] = comparison$Chisq[2]
english_duration_random$df[english_duration_random$random_effect=="speaker"] = comparison$"Chi Df"[2]
english_duration_random$p[english_duration_random$random_effect=="speaker"] = comparison$"Pr(>Chisq)"[2]
english_duration_random$sig[english_duration_random$random_effect=="speaker"] = sub("^\\.$","n.s.",sub(" ","n.s.",stars.pval(comparison$"Pr(>Chisq)"[2])))
english_duration_random$m.r.2.change[english_duration_random$random_effect=="speaker"] = (english_duration_m.r.2 - r.squaredGLMM(english_duration_model.no_speaker)[1]) / english_duration_m.r.2 * 100
english_duration_random$c.r.2.change[english_duration_random$random_effect=="speaker"] = (english_duration_c.r.2 - r.squaredGLMM(english_duration_model.no_speaker)[2]) / english_duration_c.r.2 * 100

# Compare model without text as a random effect to full model and calculate log-likelihood ratio test as well as percentage change in marginal and conditional R²
comparison = anova(english_duration_model,english_duration_model.no_text,test="Chisq")
english_duration_random$chisq[english_duration_random$random_effect=="text"] = comparison$Chisq[2]
english_duration_random$df[english_duration_random$random_effect=="text"] = comparison$"Chi Df"[2]
english_duration_random$p[english_duration_random$random_effect=="text"] = comparison$"Pr(>Chisq)"[2]
english_duration_random$sig[english_duration_random$random_effect=="text"] = sub("^\\.$","n.s.",sub(" ","n.s.",stars.pval(comparison$"Pr(>Chisq)"[2])))
english_duration_random$m.r.2.change[english_duration_random$random_effect=="text"] = (english_duration_m.r.2 - r.squaredGLMM(english_duration_model.no_text)[1]) / english_duration_m.r.2 * 100
english_duration_random$c.r.2.change[english_duration_random$random_effect=="text"] = (english_duration_c.r.2 - r.squaredGLMM(english_duration_model.no_text)[2]) / english_duration_c.r.2 * 100

# Also save the number of tokens included in the model as well as the marginal and conditional R² of the complete model in the table for cross-linguistic comparison
results.word_duration.lmer$result[results.word_duration.lmer$language=="English" & results.word_duration.lmer$parameter=="n"] = summary(english_duration_model)$devcomp$dims[1]
results.word_duration.lmer$result[results.word_duration.lmer$language=="English" & results.word_duration.lmer$parameter=="m.r.2"] = r.squaredGLMM(english_duration_model)[1]
results.word_duration.lmer$result[results.word_duration.lmer$language=="English" & results.word_duration.lmer$parameter=="c.r.2"] = r.squaredGLMM(english_duration_model)[2]
```


```{r analysis_even, echo=FALSE}

# Make a list of the 100 most frequent word types
even_unique = unique(even[,c("word_type","word_type2","whole_word_gloss","ntvr_ps_root.simplified","word_freq","word_rel_freq","word_log_freq","word_log_rel_freq","word_freq_rank","word_freq_rank2","doc_freq","doc_freq_rank")])
even_100 = head(even_unique[order(even_unique$word_freq_rank,even_unique$doc_freq_rank),],100)

# Set row names to null
rownames(even_100) = NULL

# Calculate the mean and standard deviation of word duration and articulation speed per word using a for loop
# Fill these calculated values into the table of the 100 most frequent word types
even_100$mean_duration_per_word = NA
even_100$sd_duration_per_word = NA
even_100$mean_speed_per_word = NA
even_100$sd_speed_per_word = NA
for (word_type2 in unique(even_100$word_type2)) {
  even_100$mean_speed_per_word[even_100$word_type2==word_type2] = mean(even$speed_per_word[even$word_type2==word_type2],na.rm=T)
  even_100$sd_speed_per_word[even_100$word_type2==word_type2] = sd(even$speed_per_word[even$word_type2==word_type2],na.rm=T)
  even_100$mean_duration_per_word[even_100$word_type2==word_type2] = mean(even$WordLength[even$word_type2==word_type2],na.rm=T)
  even_100$sd_duration_per_word[even_100$word_type2==word_type2] = sd(even$WordLength[even$word_type2==word_type2],na.rm=T)
}

# Mininum frequency
even_min_frequency = min(even_100$word_freq)

# Collect a data set containing all tokens (instances) of these 100 most frequent word types in the corpus
even_100_word_types = paste(even_100$word_type,even_100$whole_word_gloss,even_100$ntvr_ps_root.simplified,sep="|")
even_100_tokens = droplevels(subset(even,subset=word_type2 %in% even_100_word_types))

# Add another column to the data preparation table ("word types manually excluded")
data_preparation_table$tokens_100_excluded[data_preparation_table$language=="Even"] = length(even_100_tokens$word)

# Exclude tokens contained in single-word annotation units
even_100_tokens = droplevels(subset(even_100_tokens,subset=words_per_record>1))
even_100_tokens = droplevels(subset(even_100_tokens,subset=chars_per_record>chars_per_word))

# Add another column to the data preparation table ("no one-word utterances (final data set)")
data_preparation_table$tokens_100_final[data_preparation_table$language=="Even"] = length(even_100_tokens$word)

# Delete column in table of the 100 most frequent words that is no longer needed
even_100$word_type2 = NULL

# Similarly for excluded words (that are occur on one of the blacklists)
even_unique_backup = unique(even_backup[,c("word_type","word_type2","whole_word_gloss","ntvr_ps_root.simplified","word_freq","word_rel_freq","doc_freq")])
even_excluded = subset(even_unique_backup[order(-even_unique_backup$word_freq,-even_unique_backup$doc_freq),],subset=word_type2 %in% even_aux_blacklist | word_type2 %in% even_n_blacklist)

# Just for statistics, count the number of instances (tokens) of the 100 most frequent words without excluding any specific words
even_100_backup = head(even_unique_backup[order(-even_unique_backup$word_freq,-even_unique_backup$doc_freq),],100)
even_100_word_types_backup = even_100_backup$word_type2
even_100_tokens_backup = droplevels(subset(even_backup,subset=word_type2 %in% even_100_word_types_backup))

# Add a column to the data preparation table ("100 most frequent word types")
data_preparation_table$tokens_100[data_preparation_table$language=="Even"] = length(even_100_tokens_backup$word)

# Only keep those excluded words that are at least as frequent as the 100th most frequent word)
even_excluded = subset(even_excluded,subset=word_freq >= even_min_frequency)

# Set rownames to null
rownames(even_excluded) = NULL

# Calculate the mean and standard deviation of word duration and articulation speed per word using a for loop
# Fill these calculated values into the table of excluded word types
even_excluded$mean_duration_per_word = NA
even_excluded$sd_duration_per_word = NA
even_excluded$mean_speed_per_word = NA
even_excluded$sd_speed_per_word = NA
for (word_type2 in unique(even_excluded$word_type2)) {
  even_excluded$mean_speed_per_word[even_excluded$word_type2==word_type2] = mean(even_backup$speed_per_word[even_backup$word_type2==word_type2],na.rm=T)
  even_excluded$sd_speed_per_word[even_excluded$word_type2==word_type2] = sd(even_backup$speed_per_word[even_backup$word_type2==word_type2],na.rm=T)
  even_excluded$mean_duration_per_word[even_excluded$word_type2==word_type2] = mean(even_backup$WordLength[even_backup$word_type2==word_type2],na.rm=T)
  even_excluded$sd_duration_per_word[even_excluded$word_type2==word_type2] = sd(even_backup$WordLength[even_backup$word_type2==word_type2],na.rm=T)
}

# Delete column in table of excluded word types that is no longer needed
even_excluded$word_type2 = NULL
```

```{r even_correlation_test, echo=FALSE}

# Carry out a bivariate correlation test using Spearman's rank correlation coefficient
even.cor.test = cor.test(even_100$word_log_rel_freq,even_100$mean_duration_per_word,method="spearman")

# Save the results in the table prepared for the bivariate analysis results
results.word_duration$result[results.word_duration$language=="Even" & results.word_duration$parameter=="spearman.rho"] = even.cor.test$estimate
results.word_duration$result[results.word_duration$language=="Even" & results.word_duration$parameter=="spearman.S"] = even.cor.test$statistic
results.word_duration$result[results.word_duration$language=="Even" & results.word_duration$parameter=="spearman.p"] = even.cor.test$p.value
results.word_duration$result[results.word_duration$language=="Even" & results.word_duration$parameter=="spearman.p.stars"] = stars.pval(even.cor.test$p.value)
results.word_duration$result[results.word_duration$language=="Even" & results.word_duration$parameter=="spearman.p.stars" & results.word_duration$result==" "] = "n.s."
```

```{r even_duration_model, echo=FALSE, size="footnotesize"}

# Convert text names and word types into factors
even_100_tokens$ntvr_file = factor(even_100_tokens$ntvr_file)
even_100_tokens$word_type2 = factor(even_100_tokens$word_type2)

# Train a linear mixed-effects model of log word duration predicted by log relative frequency, word length in characters, the number of morphemes per word, its position within the utterance,
# its part-of-speech (noun or verb), the articulation speed in the surrounding utterance and random factors for speaker (ELANParticipant) and text (ntvr_file)
even_duration_model = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=even_100_tokens, REML=TRUE)

# Create a table of model parameters for the Baure linear mixed-effects model
# Include raw coefficients
even_duration_result = as.data.frame(coef(summary(even_duration_model)))

# standardized beta coefficients
even_duration_result$beta = c(NA, lm.beta.lmer(even_duration_model))

# prepare table cells for change in marginal and conditional R² as well as semi-partial R²
even_duration_result$m.r.2.change = NA
even_duration_result$c.r.2.change = NA
even_duration_result$semi.partial.r2 = NA

# Add sensible names to the coefficients
rownames(even_duration_result) = c("(Intercept)","log word frequency","word length","number of morphemes","position","word class = verb (vs. noun)","log speech rate")

# Perform drop1 log-likelihood ratio tests for all fixed factors in the model
even_duration_result_drop1 = drop1(even_duration_model,test="Chisq")
even_duration_result$chisq = even_duration_result_drop1$LRT
even_duration_result$df = even_duration_result_drop1$Df
even_duration_result$p = even_duration_result_drop1$"Pr(Chi)"

# Add significance stars to the results of the log-likelihood ratio tests
even_duration_result$stars = stars.pval(even_duration_result$p)
even_duration_result$stars[1] = NA
even_duration_result$stars = sub(" ", "n.s.", even_duration_result$stars)
even_duration_result$stars = sub("^\\.$", "n.s.", even_duration_result$stars)

# Train a similar model but exclude word frequency as a fixed effect
even_duration_model.no_word_freq = lmer(log_word_length ~ chars_per_word + morphs_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=even_100_tokens, REML=TRUE)

# Train a similar model but exclude word length as a fixed effect
even_duration_model.no_word_length = lmer(log_word_length ~ word_log_rel_freq + morphs_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=even_100_tokens, REML=TRUE)

# Train a similar model but exclude the number of morphemes as a fixed effect
even_duration_model.no_morphs = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=even_100_tokens, REML=TRUE)

# Train a similar model but exclude word position as a fixed effect
even_duration_model.no_position = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=even_100_tokens, REML=TRUE)

# Train a similar model but exclude part-of-speech as a fixed effect
even_duration_model.no_pos = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + position_percentage + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=even_100_tokens, REML=TRUE)

# Train a similar model but exclude articulation rate in the context as a fixed effect
even_duration_model.no_speech_rate = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + position_percentage + ntvr_ps_root.simplified + (1|ELANParticipant) + (1|ntvr_file), data=even_100_tokens, REML=TRUE)

# Calculate the marginal and conditional R² of the complete model
even_duration_m.r.2 = r.squaredGLMM(even_duration_model)[1]
even_duration_c.r.2 = r.squaredGLMM(even_duration_model)[2]

# Calculate semi-partial R² for the fixed effects in the complete model
even_duration_model.r2beta = r2beta(model=even_duration_model,method="nsj",partial=T)

# Calculate the percentage change in marginal and conditional R² as well as the semi-partial R² for each fixed effect in the model
even_duration_result["log word frequency","m.r.2.change"] = (even_duration_m.r.2 - r.squaredGLMM(even_duration_model.no_word_freq)[1]) / even_duration_m.r.2 * 100
even_duration_result["log word frequency","c.r.2.change"] = (even_duration_c.r.2 - r.squaredGLMM(even_duration_model.no_word_freq)[2]) / even_duration_c.r.2 * 100
even_duration_result["log word frequency","semi.partial.r2"] = even_duration_model.r2beta$Rsq[even_duration_model.r2beta$Effect=="word_log_rel_freq"]
even_duration_result["word length","m.r.2.change"] = (even_duration_m.r.2 - r.squaredGLMM(even_duration_model.no_word_length)[1]) / even_duration_m.r.2 * 100
even_duration_result["word length","c.r.2.change"] = (even_duration_c.r.2 - r.squaredGLMM(even_duration_model.no_word_length)[2]) / even_duration_c.r.2 * 100
even_duration_result["word length","semi.partial.r2"] = even_duration_model.r2beta$Rsq[even_duration_model.r2beta$Effect=="chars_per_word"]
even_duration_result["number of morphemes","m.r.2.change"] = (even_duration_m.r.2 - r.squaredGLMM(even_duration_model.no_morphs)[1]) / even_duration_m.r.2 * 100
even_duration_result["number of morphemes","c.r.2.change"] = (even_duration_c.r.2 - r.squaredGLMM(even_duration_model.no_morphs)[2]) / even_duration_c.r.2 * 100
even_duration_result["number of morphemes","semi.partial.r2"] = even_duration_model.r2beta$Rsq[even_duration_model.r2beta$Effect=="morphs_per_word"]
even_duration_result["position","m.r.2.change"] = (even_duration_m.r.2 - r.squaredGLMM(even_duration_model.no_position)[1]) / even_duration_m.r.2 * 100
even_duration_result["position","c.r.2.change"] = (even_duration_c.r.2 - r.squaredGLMM(even_duration_model.no_position)[2]) / even_duration_c.r.2 * 100
even_duration_result["position","semi.partial.r2"] = even_duration_model.r2beta$Rsq[even_duration_model.r2beta$Effect=="position_percentage"]
even_duration_result["word class = verb (vs. noun)","m.r.2.change"] = (even_duration_m.r.2 - r.squaredGLMM(even_duration_model.no_pos)[1]) / even_duration_m.r.2 * 100
even_duration_result["word class = verb (vs. noun)","c.r.2.change"] = (even_duration_c.r.2 - r.squaredGLMM(even_duration_model.no_pos)[2]) / even_duration_c.r.2 * 100
even_duration_result["word class = verb (vs. noun)","semi.partial.r2"] = even_duration_model.r2beta$Rsq[even_duration_model.r2beta$Effect=="ntvr_ps_root.simplifiedV"]
even_duration_result["log speech rate","m.r.2.change"] = (even_duration_m.r.2 - r.squaredGLMM(even_duration_model.no_speech_rate)[1]) / even_duration_m.r.2 * 100
even_duration_result["log speech rate","c.r.2.change"] = (even_duration_c.r.2 - r.squaredGLMM(even_duration_model.no_speech_rate)[2]) / even_duration_c.r.2 * 100
even_duration_result["log speech rate","semi.partial.r2"] = even_duration_model.r2beta$Rsq[even_duration_model.r2beta$Effect=="log_speed_per_record2"]

# Save the results concerning the main predictor of interest, namely, word frequency, in table for cross-linguistic comparison
results.word_duration.lmer$result[results.word_duration.lmer$language=="Even" & results.word_duration.lmer$parameter=="coefficient"] = even_duration_result["log word frequency","Estimate"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Even" & results.word_duration.lmer$parameter=="beta"] = even_duration_result["log word frequency","beta"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Even" & results.word_duration.lmer$parameter=="chisq"] = even_duration_result["log word frequency","chisq"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Even" & results.word_duration.lmer$parameter=="df"] = even_duration_result["log word frequency","df"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Even" & results.word_duration.lmer$parameter=="p"] = even_duration_result["log word frequency","p"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Even" & results.word_duration.lmer$parameter=="p.stars"] = even_duration_result["log word frequency","stars"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Even" & results.word_duration.lmer$parameter=="m.r.2.change"] = even_duration_result["log word frequency","m.r.2.change"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Even" & results.word_duration.lmer$parameter=="c.r.2.change"] = even_duration_result["log word frequency","c.r.2.change"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Even" & results.word_duration.lmer$parameter=="semi.partial.r2"] = even_duration_result["log word frequency","semi.partial.r2"]

# Create a table of the random effects in the model
even_duration_random = as.data.frame(summary(even_duration_model)$varcor)
colnames(even_duration_random)[1] = "random_effect"
even_duration_random$groups = c(summary(even_duration_model)$ngrps,NA)
even_duration_random = even_duration_random[c("random_effect","groups","sdcor")]

# Give the random effects sensible names
even_duration_random$random_effect[even_duration_random$random_effect=="ELANParticipant"] = "speaker"
even_duration_random$random_effect[even_duration_random$random_effect=="ntvr_file"] = "text"

# And order them consistently
ordering = c(order(even_duration_random$random_effect)[2:3],order(even_duration_random$random_effect)[1])
even_duration_random = even_duration_random[ordering,]

# Add columns for various tests of the random effects
even_duration_random$m.r.2.change = NA
even_duration_random$c.r.2.change = NA
even_duration_random$chisq = NA
even_duration_random$df = NA
even_duration_random$p = NA
even_duration_random$sig = NA
  
# Train a similar model but exclude speaker as a random effect
even_duration_model.no_speaker = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ntvr_file), data=even_100_tokens, REML=TRUE)

# Train a similar model but exclude text as a random effect
even_duration_model.no_text = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant), data=even_100_tokens, REML=TRUE)

# Compare model without speaker as a random effect to full model and calculate log-likelihood ratio test as well as percentage change in marginal and conditional R²
comparison = anova(even_duration_model,even_duration_model.no_speaker,test="Chisq")
even_duration_random$chisq[even_duration_random$random_effect=="speaker"] = comparison$Chisq[2]
even_duration_random$df[even_duration_random$random_effect=="speaker"] = comparison$"Chi Df"[2]
even_duration_random$p[even_duration_random$random_effect=="speaker"] = comparison$"Pr(>Chisq)"[2]
even_duration_random$sig[even_duration_random$random_effect=="speaker"] = sub("^\\.$","n.s.",sub(" ","n.s.",stars.pval(comparison$"Pr(>Chisq)"[2])))
even_duration_random$m.r.2.change[even_duration_random$random_effect=="speaker"] = (even_duration_m.r.2 - r.squaredGLMM(even_duration_model.no_speaker)[1]) / even_duration_m.r.2 * 100
even_duration_random$c.r.2.change[even_duration_random$random_effect=="speaker"] = (even_duration_c.r.2 - r.squaredGLMM(even_duration_model.no_speaker)[2]) / even_duration_c.r.2 * 100

# Compare model without text as a random effect to full model and calculate log-likelihood ratio test as well as percentage change in marginal and conditional R²
comparison = anova(even_duration_model,even_duration_model.no_text,test="Chisq")
even_duration_random$chisq[even_duration_random$random_effect=="text"] = comparison$Chisq[2]
even_duration_random$df[even_duration_random$random_effect=="text"] = comparison$"Chi Df"[2]
even_duration_random$p[even_duration_random$random_effect=="text"] = comparison$"Pr(>Chisq)"[2]
even_duration_random$sig[even_duration_random$random_effect=="text"] = sub("^\\.$","n.s.",sub(" ","n.s.",stars.pval(comparison$"Pr(>Chisq)"[2])))
even_duration_random$m.r.2.change[even_duration_random$random_effect=="text"] = (even_duration_m.r.2 - r.squaredGLMM(even_duration_model.no_text)[1]) / even_duration_m.r.2 * 100
even_duration_random$c.r.2.change[even_duration_random$random_effect=="text"] = (even_duration_c.r.2 - r.squaredGLMM(even_duration_model.no_text)[2]) / even_duration_c.r.2 * 100

# Also save the number of tokens included in the model as well as the marginal and conditional R² of the complete model in the table for cross-linguistic comparison
results.word_duration.lmer$result[results.word_duration.lmer$language=="Even" & results.word_duration.lmer$parameter=="n"] = summary(even_duration_model)$devcomp$dims[1]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Even" & results.word_duration.lmer$parameter=="m.r.2"] = r.squaredGLMM(even_duration_model)[1]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Even" & results.word_duration.lmer$parameter=="c.r.2"] = r.squaredGLMM(even_duration_model)[2]
```


```{r analysis_hoocak, echo=FALSE}

# Make a list of the 100 most frequent word types
hoocak_unique = unique(hoocak[,c("word_type","word_type2","whole_word_gloss","ntvr_ps_root.simplified","word_freq","word_rel_freq","word_log_freq","word_log_rel_freq","word_freq_rank","word_freq_rank2","doc_freq","doc_freq_rank")])
hoocak_100 = head(hoocak_unique[order(hoocak_unique$word_freq_rank,hoocak_unique$doc_freq_rank),],100)

# Set row names to null
rownames(hoocak_100) = NULL

# Calculate the mean and standard deviation of word duration and articulation speed per word using a for loop
# Fill these calculated values into the table of the 100 most frequent word types
hoocak_100$mean_duration_per_word = NA
hoocak_100$sd_duration_per_word = NA
hoocak_100$mean_speed_per_word = NA
hoocak_100$sd_speed_per_word = NA
for (word_type2 in unique(hoocak_100$word_type2)) {
  hoocak_100$mean_speed_per_word[hoocak_100$word_type2==word_type2] = mean(hoocak$speed_per_word[hoocak$word_type2==word_type2],na.rm=T)
  hoocak_100$sd_speed_per_word[hoocak_100$word_type2==word_type2] = sd(hoocak$speed_per_word[hoocak$word_type2==word_type2],na.rm=T)
  hoocak_100$mean_duration_per_word[hoocak_100$word_type2==word_type2] = mean(hoocak$WordLength[hoocak$word_type2==word_type2],na.rm=T)
  hoocak_100$sd_duration_per_word[hoocak_100$word_type2==word_type2] = sd(hoocak$WordLength[hoocak$word_type2==word_type2],na.rm=T)
}

# Mininum frequency
hoocak_min_frequency = min(hoocak_100$word_freq)

# Collect a data set containing all tokens (instances) of these 100 most frequent word types in the corpus
hoocak_100_word_types = paste(hoocak_100$word_type,hoocak_100$whole_word_gloss,hoocak_100$ntvr_ps_root.simplified,sep="|")
hoocak_100_tokens = droplevels(subset(hoocak,subset=word_type2 %in% hoocak_100_word_types))

# Add another column to the data preparation table ("word types manually excluded")
data_preparation_table$tokens_100_excluded[data_preparation_table$language=="Hoocąk"] = length(hoocak_100_tokens$word)

# Exclude tokens contained in single-word annotation units
hoocak_100_tokens = droplevels(subset(hoocak_100_tokens,subset=words_per_record>1))
hoocak_100_tokens = droplevels(subset(hoocak_100_tokens,subset=chars_per_record>chars_per_word))

# Add another column to the data preparation table ("no one-word utterances (final data set)")
data_preparation_table$tokens_100_final[data_preparation_table$language=="Hoocąk"] = length(hoocak_100_tokens$word)

# Delete column in table of the 100 most frequent words that is no longer needed
hoocak_100$word_type2 = NULL

# Similarly for excluded words (that are occur on one of the blacklists)
hoocak_unique_backup = unique(hoocak_backup[,c("word_type","word_type2","whole_word_gloss","ntvr_ps_root.simplified","word_freq","word_rel_freq","doc_freq")])
hoocak_excluded = subset(hoocak_unique_backup[order(-hoocak_unique_backup$word_freq,-hoocak_unique_backup$doc_freq),],subset=word_type2 %in% hoocak_aux_blacklist | word_type2 %in% hoocak_n_blacklist | whole_word_gloss %in% hoocak_aux_blacklist2)

# Just for statistics, count the number of instances (tokens) of the 100 most frequent words without excluding any specific words
hoocak_100_backup = head(hoocak_unique_backup[order(-hoocak_unique_backup$word_freq,-hoocak_unique_backup$doc_freq),],100)
hoocak_100_word_types_backup = hoocak_100_backup$word_type2
hoocak_100_tokens_backup = droplevels(subset(hoocak_backup,subset=word_type2 %in% hoocak_100_word_types_backup))

# Add a column to the data preparation table ("100 most frequent word types")
data_preparation_table$tokens_100[data_preparation_table$language=="Hoocąk"] = length(hoocak_100_tokens_backup$word)

# Only keep those excluded words that are at least as frequent as the 100th most frequent word)
hoocak_excluded = subset(hoocak_excluded,subset=word_freq >= hoocak_min_frequency)

# Set rownames to null
rownames(hoocak_excluded) = NULL

# Calculate the mean and standard deviation of word duration and articulation speed per word using a for loop
# Fill these calculated values into the table of excluded word types
hoocak_excluded$mean_duration_per_word = NA
hoocak_excluded$sd_duration_per_word = NA
hoocak_excluded$mean_speed_per_word = NA
hoocak_excluded$sd_speed_per_word = NA
for (word_type2 in unique(hoocak_excluded$word_type2)) {
  hoocak_excluded$mean_speed_per_word[hoocak_excluded$word_type2==word_type2] = mean(hoocak_backup$speed_per_word[hoocak_backup$word_type2==word_type2],na.rm=T)
  hoocak_excluded$sd_speed_per_word[hoocak_excluded$word_type2==word_type2] = sd(hoocak_backup$speed_per_word[hoocak_backup$word_type2==word_type2],na.rm=T)
  hoocak_excluded$mean_duration_per_word[hoocak_excluded$word_type2==word_type2] = mean(hoocak_backup$WordLength[hoocak_backup$word_type2==word_type2],na.rm=T)
  hoocak_excluded$sd_duration_per_word[hoocak_excluded$word_type2==word_type2] = sd(hoocak_backup$WordLength[hoocak_backup$word_type2==word_type2],na.rm=T)
}

# Delete column in table of excluded word types that is no longer needed
hoocak_excluded$word_type2 = NULL
```

```{r hoocak_correlation_test, echo=FALSE}

# Carry out a bivariate correlation test using Spearman's rank correlation coefficient
hoocak.cor.test = cor.test(hoocak_100$word_log_rel_freq,hoocak_100$mean_duration_per_word,method="spearman")

# Save the results in the table prepared for the bivariate analysis results
results.word_duration$result[results.word_duration$language=="Hoocąk" & results.word_duration$parameter=="spearman.rho"] = hoocak.cor.test$estimate
results.word_duration$result[results.word_duration$language=="Hoocąk" & results.word_duration$parameter=="spearman.S"] = hoocak.cor.test$statistic
results.word_duration$result[results.word_duration$language=="Hoocąk" & results.word_duration$parameter=="spearman.p"] = hoocak.cor.test$p.value
results.word_duration$result[results.word_duration$language=="Hoocąk" & results.word_duration$parameter=="spearman.p.stars"] = stars.pval(hoocak.cor.test$p.value)
results.word_duration$result[results.word_duration$language=="Hoocąk" & results.word_duration$parameter=="spearman.p.stars" & results.word_duration$result==" "] = "n.s."
```

```{r hoocak_duration_model, echo=FALSE, size="footnotesize"}

# Convert text names and word types into factors
hoocak_100_tokens$ntvr_file = factor(hoocak_100_tokens$ntvr_file)
hoocak_100_tokens$word_type2 = factor(hoocak_100_tokens$word_type2)

# Train a linear mixed-effects model of log word duration predicted by log relative frequency, word length in characters, the number of morphemes per word, its position within the utterance,
# its part-of-speech (noun or verb), the articulation speed in the surrounding utterance and random factors for speaker (ELANParticipant) and text (ntvr_file)
hoocak_duration_model = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=hoocak_100_tokens, REML=TRUE)

# Create a table of model parameters for the Baure linear mixed-effects model
# Include raw coefficients
hoocak_duration_result = as.data.frame(coef(summary(hoocak_duration_model)))

# standardized beta coefficients
hoocak_duration_result$beta = c(NA, lm.beta.lmer(hoocak_duration_model))

# prepare table cells for change in marginal and conditional R² as well as semi-partial R²
hoocak_duration_result$m.r.2.change = NA
hoocak_duration_result$c.r.2.change = NA
hoocak_duration_result$semi.partial.r2 = NA

# Add sensible names to the coefficients
rownames(hoocak_duration_result) = c("(Intercept)","log word frequency","word length","number of morphemes","position","word class = verb (vs. noun)","log speech rate")

# Perform drop1 log-likelihood ratio tests for all fixed factors in the model
hoocak_duration_result_drop1 = drop1(hoocak_duration_model,test="Chisq")
hoocak_duration_result$chisq = hoocak_duration_result_drop1$LRT
hoocak_duration_result$df = hoocak_duration_result_drop1$Df
hoocak_duration_result$p = hoocak_duration_result_drop1$"Pr(Chi)"

# Add significance stars to the results of the log-likelihood ratio tests
hoocak_duration_result$stars = stars.pval(hoocak_duration_result$p)
hoocak_duration_result$stars[1] = NA
hoocak_duration_result$stars = sub(" ", "n.s.", hoocak_duration_result$stars)
hoocak_duration_result$stars = sub("^\\.$", "n.s.", hoocak_duration_result$stars)

# Train a similar model but exclude word frequency as a fixed effect
hoocak_duration_model.no_word_freq = lmer(log_word_length ~ chars_per_word + morphs_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=hoocak_100_tokens, REML=TRUE)

# Train a similar model but exclude word length as a fixed effect
hoocak_duration_model.no_word_length = lmer(log_word_length ~ word_log_rel_freq + morphs_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=hoocak_100_tokens, REML=TRUE)

# Train a similar model but exclude the number of morphemes as a fixed effect
hoocak_duration_model.no_morphs = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=hoocak_100_tokens, REML=TRUE)

# Train a similar model but exclude word position as a fixed effect
hoocak_duration_model.no_position = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=hoocak_100_tokens, REML=TRUE)

# Train a similar model but exclude part-of-speech as a fixed effect
hoocak_duration_model.no_pos = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + position_percentage + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=hoocak_100_tokens, REML=TRUE)

# Train a similar model but exclude articulation rate in the context as a fixed effect
hoocak_duration_model.no_speech_rate = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + position_percentage + ntvr_ps_root.simplified + (1|ELANParticipant) + (1|ntvr_file), data=hoocak_100_tokens, REML=TRUE)

# Calculate the marginal and conditional R² of the complete model
hoocak_duration_m.r.2 = r.squaredGLMM(hoocak_duration_model)[1]
hoocak_duration_c.r.2 = r.squaredGLMM(hoocak_duration_model)[2]

# Calculate semi-partial R² for the fixed effects in the complete model
hoocak_duration_model.r2beta = r2beta(model=hoocak_duration_model,method="nsj",partial=T)

# Calculate the percentage change in marginal and conditional R² as well as the semi-partial R² for each fixed effect in the model
hoocak_duration_result["log word frequency","m.r.2.change"] = (hoocak_duration_m.r.2 - r.squaredGLMM(hoocak_duration_model.no_word_freq)[1]) / hoocak_duration_m.r.2 * 100
hoocak_duration_result["log word frequency","c.r.2.change"] = (hoocak_duration_c.r.2 - r.squaredGLMM(hoocak_duration_model.no_word_freq)[2]) / hoocak_duration_c.r.2 * 100
hoocak_duration_result["log word frequency","semi.partial.r2"] = hoocak_duration_model.r2beta$Rsq[hoocak_duration_model.r2beta$Effect=="word_log_rel_freq"]
hoocak_duration_result["word length","m.r.2.change"] = (hoocak_duration_m.r.2 - r.squaredGLMM(hoocak_duration_model.no_word_length)[1]) / hoocak_duration_m.r.2 * 100
hoocak_duration_result["word length","c.r.2.change"] = (hoocak_duration_c.r.2 - r.squaredGLMM(hoocak_duration_model.no_word_length)[2]) / hoocak_duration_c.r.2 * 100
hoocak_duration_result["word length","semi.partial.r2"] = hoocak_duration_model.r2beta$Rsq[hoocak_duration_model.r2beta$Effect=="chars_per_word"]
hoocak_duration_result["number of morphemes","m.r.2.change"] = (hoocak_duration_m.r.2 - r.squaredGLMM(hoocak_duration_model.no_morphs)[1]) / hoocak_duration_m.r.2 * 100
hoocak_duration_result["number of morphemes","c.r.2.change"] = (hoocak_duration_c.r.2 - r.squaredGLMM(hoocak_duration_model.no_morphs)[2]) / hoocak_duration_c.r.2 * 100
hoocak_duration_result["number of morphemes","semi.partial.r2"] = hoocak_duration_model.r2beta$Rsq[hoocak_duration_model.r2beta$Effect=="morphs_per_word"]
hoocak_duration_result["position","m.r.2.change"] = (hoocak_duration_m.r.2 - r.squaredGLMM(hoocak_duration_model.no_position)[1]) / hoocak_duration_m.r.2 * 100
hoocak_duration_result["position","c.r.2.change"] = (hoocak_duration_c.r.2 - r.squaredGLMM(hoocak_duration_model.no_position)[2]) / hoocak_duration_c.r.2 * 100
hoocak_duration_result["position","semi.partial.r2"] = hoocak_duration_model.r2beta$Rsq[hoocak_duration_model.r2beta$Effect=="position_percentage"]
hoocak_duration_result["word class = verb (vs. noun)","m.r.2.change"] = (hoocak_duration_m.r.2 - r.squaredGLMM(hoocak_duration_model.no_pos)[1]) / hoocak_duration_m.r.2 * 100
hoocak_duration_result["word class = verb (vs. noun)","c.r.2.change"] = (hoocak_duration_c.r.2 - r.squaredGLMM(hoocak_duration_model.no_pos)[2]) / hoocak_duration_c.r.2 * 100
hoocak_duration_result["word class = verb (vs. noun)","semi.partial.r2"] = hoocak_duration_model.r2beta$Rsq[hoocak_duration_model.r2beta$Effect=="ntvr_ps_root.simplifiedV"]
hoocak_duration_result["log speech rate","m.r.2.change"] = (hoocak_duration_m.r.2 - r.squaredGLMM(hoocak_duration_model.no_speech_rate)[1]) / hoocak_duration_m.r.2 * 100
hoocak_duration_result["log speech rate","c.r.2.change"] = (hoocak_duration_c.r.2 - r.squaredGLMM(hoocak_duration_model.no_speech_rate)[2]) / hoocak_duration_c.r.2 * 100
hoocak_duration_result["log speech rate","semi.partial.r2"] = hoocak_duration_model.r2beta$Rsq[hoocak_duration_model.r2beta$Effect=="log_speed_per_record2"]

# Save the results concerning the main predictor of interest, namely, word frequency, in table for cross-linguistic comparison
results.word_duration.lmer$result[results.word_duration.lmer$language=="Hoocąk" & results.word_duration.lmer$parameter=="coefficient"] = hoocak_duration_result["log word frequency","Estimate"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Hoocąk" & results.word_duration.lmer$parameter=="beta"] = hoocak_duration_result["log word frequency","beta"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Hoocąk" & results.word_duration.lmer$parameter=="chisq"] = hoocak_duration_result["log word frequency","chisq"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Hoocąk" & results.word_duration.lmer$parameter=="df"] = hoocak_duration_result["log word frequency","df"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Hoocąk" & results.word_duration.lmer$parameter=="p"] = hoocak_duration_result["log word frequency","p"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Hoocąk" & results.word_duration.lmer$parameter=="p.stars"] = hoocak_duration_result["log word frequency","stars"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Hoocąk" & results.word_duration.lmer$parameter=="m.r.2.change"] = hoocak_duration_result["log word frequency","m.r.2.change"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Hoocąk" & results.word_duration.lmer$parameter=="c.r.2.change"] = hoocak_duration_result["log word frequency","c.r.2.change"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Hoocąk" & results.word_duration.lmer$parameter=="semi.partial.r2"] = hoocak_duration_result["log word frequency","semi.partial.r2"]

# Create a table of the random effects in the model
hoocak_duration_random = as.data.frame(summary(hoocak_duration_model)$varcor)
colnames(hoocak_duration_random)[1] = "random_effect"
hoocak_duration_random$groups = c(summary(hoocak_duration_model)$ngrps,NA)
hoocak_duration_random = hoocak_duration_random[c("random_effect","groups","sdcor")]

# Give the random effects sensible names
hoocak_duration_random$random_effect[hoocak_duration_random$random_effect=="ELANParticipant"] = "speaker"
hoocak_duration_random$random_effect[hoocak_duration_random$random_effect=="ntvr_file"] = "text"

# And order them consistently
ordering = c(order(hoocak_duration_random$random_effect)[2:3],order(hoocak_duration_random$random_effect)[1])
hoocak_duration_random = hoocak_duration_random[ordering,]

# Add columns for various tests of the random effects
hoocak_duration_random$m.r.2.change = NA
hoocak_duration_random$c.r.2.change = NA
hoocak_duration_random$chisq = NA
hoocak_duration_random$df = NA
hoocak_duration_random$p = NA
hoocak_duration_random$sig = NA
  
# Train a similar model but exclude speaker as a random effect
hoocak_duration_model.no_speaker = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ntvr_file), data=hoocak_100_tokens, REML=TRUE)

# Train a similar model but exclude text as a random effect
hoocak_duration_model.no_text = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant), data=hoocak_100_tokens, REML=TRUE)

# Compare model without speaker as a random effect to full model and calculate log-likelihood ratio test as well as percentage change in marginal and conditional R²
comparison = anova(hoocak_duration_model,hoocak_duration_model.no_speaker,test="Chisq")
hoocak_duration_random$chisq[hoocak_duration_random$random_effect=="speaker"] = comparison$Chisq[2]
hoocak_duration_random$df[hoocak_duration_random$random_effect=="speaker"] = comparison$"Chi Df"[2]
hoocak_duration_random$p[hoocak_duration_random$random_effect=="speaker"] = comparison$"Pr(>Chisq)"[2]
hoocak_duration_random$sig[hoocak_duration_random$random_effect=="speaker"] = sub("^\\.$","n.s.",sub(" ","n.s.",stars.pval(comparison$"Pr(>Chisq)"[2])))
hoocak_duration_random$m.r.2.change[hoocak_duration_random$random_effect=="speaker"] = (hoocak_duration_m.r.2 - r.squaredGLMM(hoocak_duration_model.no_speaker)[1]) / hoocak_duration_m.r.2 * 100
hoocak_duration_random$c.r.2.change[hoocak_duration_random$random_effect=="speaker"] = (hoocak_duration_c.r.2 - r.squaredGLMM(hoocak_duration_model.no_speaker)[2]) / hoocak_duration_c.r.2 * 100

# Compare model without text as a random effect to full model and calculate log-likelihood ratio test as well as percentage change in marginal and conditional R²
comparison = anova(hoocak_duration_model,hoocak_duration_model.no_text,test="Chisq")
hoocak_duration_random$chisq[hoocak_duration_random$random_effect=="text"] = comparison$Chisq[2]
hoocak_duration_random$df[hoocak_duration_random$random_effect=="text"] = comparison$"Chi Df"[2]
hoocak_duration_random$p[hoocak_duration_random$random_effect=="text"] = comparison$"Pr(>Chisq)"[2]
hoocak_duration_random$sig[hoocak_duration_random$random_effect=="text"] = sub("^\\.$","n.s.",sub(" ","n.s.",stars.pval(comparison$"Pr(>Chisq)"[2])))
hoocak_duration_random$m.r.2.change[hoocak_duration_random$random_effect=="text"] = (hoocak_duration_m.r.2 - r.squaredGLMM(hoocak_duration_model.no_text)[1]) / hoocak_duration_m.r.2 * 100
hoocak_duration_random$c.r.2.change[hoocak_duration_random$random_effect=="text"] = (hoocak_duration_c.r.2 - r.squaredGLMM(hoocak_duration_model.no_text)[2]) / hoocak_duration_c.r.2 * 100

# Also save the number of tokens included in the model as well as the marginal and conditional R² of the complete model in the table for cross-linguistic comparison
results.word_duration.lmer$result[results.word_duration.lmer$language=="Hoocąk" & results.word_duration.lmer$parameter=="n"] = summary(hoocak_duration_model)$devcomp$dims[1]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Hoocąk" & results.word_duration.lmer$parameter=="m.r.2"] = r.squaredGLMM(hoocak_duration_model)[1]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Hoocąk" & results.word_duration.lmer$parameter=="c.r.2"] = r.squaredGLMM(hoocak_duration_model)[2]
```


```{r analysis_nuu, echo=FALSE}

# Make a list of the 100 most frequent word types
nuu_unique = unique(nuu[,c("word_type","word_type2","whole_word_gloss","ntvr_ps_root.simplified","word_freq","word_rel_freq","word_log_freq","word_log_rel_freq","word_freq_rank","word_freq_rank2","doc_freq","doc_freq_rank")])
nuu_100 = head(nuu_unique[order(nuu_unique$word_freq_rank,nuu_unique$doc_freq_rank),],100)

# Set row names to null
rownames(nuu_100) = NULL

# Calculate the mean and standard deviation of word duration and articulation speed per word using a for loop
# Fill these calculated values into the table of the 100 most frequent word types
nuu_100$mean_duration_per_word = NA
nuu_100$sd_duration_per_word = NA
nuu_100$mean_speed_per_word = NA
nuu_100$sd_speed_per_word = NA
for (word_type2 in unique(nuu_100$word_type2)) {
  nuu_100$mean_speed_per_word[nuu_100$word_type2==word_type2] = mean(nuu$speed_per_word[nuu$word_type2==word_type2],na.rm=T)
  nuu_100$sd_speed_per_word[nuu_100$word_type2==word_type2] = sd(nuu$speed_per_word[nuu$word_type2==word_type2],na.rm=T)
  nuu_100$mean_duration_per_word[nuu_100$word_type2==word_type2] = mean(nuu$WordLength[nuu$word_type2==word_type2],na.rm=T)
  nuu_100$sd_duration_per_word[nuu_100$word_type2==word_type2] = sd(nuu$WordLength[nuu$word_type2==word_type2],na.rm=T)
}

# Mininum frequency
nuu_min_frequency = min(nuu_100$word_freq)

# Collect a data set containing all tokens (instances) of these 100 most frequent word types in the corpus
nuu_100_word_types = paste(nuu_100$word_type,nuu_100$whole_word_gloss,nuu_100$ntvr_ps_root.simplified,sep="|")
nuu_100_tokens = droplevels(subset(nuu,subset=word_type2 %in% nuu_100_word_types))

# Add another column to the data preparation table ("word types manually excluded")
data_preparation_table$tokens_100[data_preparation_table$language=="Nǁng"] = length(nuu_100_tokens$word)

# Exclude tokens contained in single-word annotation units
nuu_100_tokens = droplevels(subset(nuu_100_tokens,subset=words_per_record>1))
nuu_100_tokens = droplevels(subset(nuu_100_tokens,subset=chars_per_record>chars_per_word))

# Add another column to the data preparation table ("no one-word utterances (final data set)")
data_preparation_table$tokens_100_final[data_preparation_table$language=="Nǁng"] = length(nuu_100_tokens$word)

# Delete column in table of the 100 most frequent words that is no longer needed
nuu_100$word_type2 = NULL

# Similarly for excluded words (that are occur on one of the blacklists)
nuu_unique_backup = unique(nuu_backup[,c("word_type","word_type2","whole_word_gloss","ntvr_ps_root.simplified","word_freq","word_rel_freq","doc_freq")])
nuu_excluded = subset(nuu_unique_backup[order(-nuu_unique_backup$word_freq,-nuu_unique_backup$doc_freq),],subset=word_type %in% nuu_aux_blacklist | word_type %in% nuu_n_blacklist)

# Just for statistics, count the number of instances (tokens) of the 100 most frequent words without excluding any specific words
nuu_100_backup = head(nuu_unique_backup[order(-nuu_unique_backup$word_freq,-nuu_unique_backup$doc_freq),],100)
nuu_100_word_types_backup = nuu_100_backup$word_type2
nuu_100_tokens_backup = droplevels(subset(nuu_backup,subset=word_type2 %in% nuu_100_word_types_backup))

# Add a column to the data preparation table ("100 most frequent word types")
data_preparation_table$tokens_100_excluded[data_preparation_table$language=="Nǁng"] = length(nuu_100_tokens_backup$word)

# Only keep those excluded words that are at least as frequent as the 100th most frequent word)
nuu_excluded = subset(nuu_excluded,subset=word_freq >= nuu_min_frequency)

# Set rownames to null
rownames(nuu_excluded) = NULL

# There are no excluded words in Nǁng
# nuu_excluded$mean_duration_per_word = NA
# nuu_excluded$sd_duration_per_word = NA
# nuu_excluded$mean_speed_per_word = NA
# nuu_excluded$sd_speed_per_word = NA
# for (word_type2 in unique(nuu_excluded$word_type2)) {
#   nuu_excluded$mean_speed_per_word[nuu_excluded$word_type2==word_type2] = mean(nuu_backup$speed_per_word[nuu_backup$word_type2==word_type2],na.rm=T)
#   nuu_excluded$sd_speed_per_word[nuu_excluded$word_type2==word_type2] = sd(nuu_backup$speed_per_word[nuu_backup$word_type2==word_type2],na.rm=T)
#   nuu_excluded$mean_duration_per_word[nuu_excluded$word_type2==word_type2] = mean(nuu_backup$WordLength[nuu_backup$word_type2==word_type2],na.rm=T)
#   nuu_excluded$sd_duration_per_word[nuu_excluded$word_type2==word_type2] = sd(nuu_backup$WordLength[nuu_backup$word_type2==word_type2],na.rm=T)
# }
```

```{r nuu_correlation_test, echo=FALSE}

# Carry out a bivariate correlation test using Spearman's rank correlation coefficient
nuu.cor.test = cor.test(nuu_100$word_log_rel_freq,nuu_100$mean_duration_per_word,method="spearman")

# Save the results in the table prepared for the bivariate analysis results
results.word_duration$result[results.word_duration$language=="Nǁng" & results.word_duration$parameter=="spearman.rho"] = nuu.cor.test$estimate
results.word_duration$result[results.word_duration$language=="Nǁng" & results.word_duration$parameter=="spearman.S"] = nuu.cor.test$statistic
results.word_duration$result[results.word_duration$language=="Nǁng" & results.word_duration$parameter=="spearman.p"] = nuu.cor.test$p.value
results.word_duration$result[results.word_duration$language=="Nǁng" & results.word_duration$parameter=="spearman.p.stars"] = stars.pval(nuu.cor.test$p.value)
results.word_duration$result[results.word_duration$language=="Nǁng" & results.word_duration$parameter=="spearman.p.stars" & results.word_duration$result==" "] = "n.s."
```

```{r nuu_duration_model, echo=FALSE, size="footnotesize"}

# Convert text names and word types into factors
nuu_100_tokens$ntvr_file = factor(nuu_100_tokens$ntvr_file)
nuu_100_tokens$word_type2 = factor(nuu_100_tokens$word_type2)

# Train a linear mixed-effects model of log word duration predicted by log relative frequency, word length in characters, the number of morphemes per word, its position within the utterance,
# its part-of-speech (noun or verb), the articulation speed in the surrounding utterance and random factors for speaker (ELANParticipant) and text (ntvr_file)
nuu_duration_model = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=nuu_100_tokens, REML=TRUE)

# Create a table of model parameters for the Baure linear mixed-effects model
# Include raw coefficients
nuu_duration_result = as.data.frame(coef(summary(nuu_duration_model)))

# standardized beta coefficients
nuu_duration_result$beta = c(NA, lm.beta.lmer(nuu_duration_model))

# prepare table cells for change in marginal and conditional R² as well as semi-partial R²
nuu_duration_result$m.r.2.change = NA
nuu_duration_result$c.r.2.change = NA
nuu_duration_result$semi.partial.r2 = NA

# Add sensible names to the coefficients
rownames(nuu_duration_result) = c("(Intercept)","log word frequency","word length","number of morphemes","position","word class = verb (vs. noun)","log speech rate")

# Perform drop1 log-likelihood ratio tests for all fixed factors in the model
nuu_duration_result_drop1 = drop1(nuu_duration_model,test="Chisq")
nuu_duration_result$chisq = nuu_duration_result_drop1$LRT
nuu_duration_result$df = nuu_duration_result_drop1$Df
nuu_duration_result$p = nuu_duration_result_drop1$"Pr(Chi)"

# Add significance stars to the results of the log-likelihood ratio tests
nuu_duration_result$stars = stars.pval(nuu_duration_result$p)
nuu_duration_result$stars[1] = NA
nuu_duration_result$stars = sub(" ", "n.s.", nuu_duration_result$stars)
nuu_duration_result$stars = sub("^\\.$", "n.s.", nuu_duration_result$stars)

# Train a similar model but exclude word frequency as a fixed effect
nuu_duration_model.no_word_freq = lmer(log_word_length ~ chars_per_word + morphs_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=nuu_100_tokens, REML=TRUE)

# Train a similar model but exclude word length as a fixed effect
nuu_duration_model.no_word_length = lmer(log_word_length ~ word_log_rel_freq + morphs_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=nuu_100_tokens, REML=TRUE)

# Train a similar model but exclude the number of morphemes as a fixed effect
nuu_duration_model.no_morphs = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=nuu_100_tokens, REML=TRUE)

# Train a similar model but exclude word position as a fixed effect
nuu_duration_model.no_position = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=nuu_100_tokens, REML=TRUE)

# Train a similar model but exclude part-of-speech as a fixed effect
nuu_duration_model.no_pos = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + position_percentage + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=nuu_100_tokens, REML=TRUE)

# Train a similar model but exclude articulation rate in the context as a fixed effect
nuu_duration_model.no_speech_rate = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + position_percentage + ntvr_ps_root.simplified + (1|ELANParticipant) + (1|ntvr_file), data=nuu_100_tokens, REML=TRUE)

# Calculate the marginal and conditional R² of the complete model
nuu_duration_m.r.2 = r.squaredGLMM(nuu_duration_model)[1]
nuu_duration_c.r.2 = r.squaredGLMM(nuu_duration_model)[2]

# Calculate semi-partial R² for the fixed effects in the complete model
nuu_duration_model.r2beta = r2beta(model=nuu_duration_model,method="nsj",partial=T)

# Calculate the percentage change in marginal and conditional R² as well as the semi-partial R² for each fixed effect in the model
nuu_duration_result["log word frequency","m.r.2.change"] = (nuu_duration_m.r.2 - r.squaredGLMM(nuu_duration_model.no_word_freq)[1]) / nuu_duration_m.r.2 * 100
nuu_duration_result["log word frequency","c.r.2.change"] = (nuu_duration_c.r.2 - r.squaredGLMM(nuu_duration_model.no_word_freq)[2]) / nuu_duration_c.r.2 * 100
nuu_duration_result["log word frequency","semi.partial.r2"] = nuu_duration_model.r2beta$Rsq[nuu_duration_model.r2beta$Effect=="word_log_rel_freq"]
nuu_duration_result["word length","m.r.2.change"] = (nuu_duration_m.r.2 - r.squaredGLMM(nuu_duration_model.no_word_length)[1]) / nuu_duration_m.r.2 * 100
nuu_duration_result["word length","c.r.2.change"] = (nuu_duration_c.r.2 - r.squaredGLMM(nuu_duration_model.no_word_length)[2]) / nuu_duration_c.r.2 * 100
nuu_duration_result["word length","semi.partial.r2"] = nuu_duration_model.r2beta$Rsq[nuu_duration_model.r2beta$Effect=="chars_per_word"]
nuu_duration_result["number of morphemes","m.r.2.change"] = (nuu_duration_m.r.2 - r.squaredGLMM(nuu_duration_model.no_morphs)[1]) / nuu_duration_m.r.2 * 100
nuu_duration_result["number of morphemes","c.r.2.change"] = (nuu_duration_c.r.2 - r.squaredGLMM(nuu_duration_model.no_morphs)[2]) / nuu_duration_c.r.2 * 100
nuu_duration_result["number of morphemes","semi.partial.r2"] = nuu_duration_model.r2beta$Rsq[nuu_duration_model.r2beta$Effect=="morphs_per_word"]
nuu_duration_result["position","m.r.2.change"] = (nuu_duration_m.r.2 - r.squaredGLMM(nuu_duration_model.no_position)[1]) / nuu_duration_m.r.2 * 100
nuu_duration_result["position","c.r.2.change"] = (nuu_duration_c.r.2 - r.squaredGLMM(nuu_duration_model.no_position)[2]) / nuu_duration_c.r.2 * 100
nuu_duration_result["position","semi.partial.r2"] = nuu_duration_model.r2beta$Rsq[nuu_duration_model.r2beta$Effect=="position_percentage"]
nuu_duration_result["word class = verb (vs. noun)","m.r.2.change"] = (nuu_duration_m.r.2 - r.squaredGLMM(nuu_duration_model.no_pos)[1]) / nuu_duration_m.r.2 * 100
nuu_duration_result["word class = verb (vs. noun)","c.r.2.change"] = (nuu_duration_c.r.2 - r.squaredGLMM(nuu_duration_model.no_pos)[2]) / nuu_duration_c.r.2 * 100
nuu_duration_result["word class = verb (vs. noun)","semi.partial.r2"] = nuu_duration_model.r2beta$Rsq[nuu_duration_model.r2beta$Effect=="ntvr_ps_root.simplifiedV"]
nuu_duration_result["log speech rate","m.r.2.change"] = (nuu_duration_m.r.2 - r.squaredGLMM(nuu_duration_model.no_speech_rate)[1]) / nuu_duration_m.r.2 * 100
nuu_duration_result["log speech rate","c.r.2.change"] = (nuu_duration_c.r.2 - r.squaredGLMM(nuu_duration_model.no_speech_rate)[2]) / nuu_duration_c.r.2 * 100
nuu_duration_result["log speech rate","semi.partial.r2"] = nuu_duration_model.r2beta$Rsq[nuu_duration_model.r2beta$Effect=="log_speed_per_record2"]

# Save the results concerning the main predictor of interest, namely, word frequency, in table for cross-linguistic comparison
results.word_duration.lmer$result[results.word_duration.lmer$language=="Nǁng" & results.word_duration.lmer$parameter=="coefficient"] = nuu_duration_result["log word frequency","Estimate"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Nǁng" & results.word_duration.lmer$parameter=="beta"] = nuu_duration_result["log word frequency","beta"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Nǁng" & results.word_duration.lmer$parameter=="chisq"] = nuu_duration_result["log word frequency","chisq"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Nǁng" & results.word_duration.lmer$parameter=="df"] = nuu_duration_result["log word frequency","df"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Nǁng" & results.word_duration.lmer$parameter=="p"] = nuu_duration_result["log word frequency","p"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Nǁng" & results.word_duration.lmer$parameter=="p.stars"] = nuu_duration_result["log word frequency","stars"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Nǁng" & results.word_duration.lmer$parameter=="m.r.2.change"] = nuu_duration_result["log word frequency","m.r.2.change"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Nǁng" & results.word_duration.lmer$parameter=="c.r.2.change"] = nuu_duration_result["log word frequency","c.r.2.change"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Nǁng" & results.word_duration.lmer$parameter=="semi.partial.r2"] = nuu_duration_result["log word frequency","semi.partial.r2"]

# Create a table of the random effects in the model
nuu_duration_random = as.data.frame(summary(nuu_duration_model)$varcor)
colnames(nuu_duration_random)[1] = "random_effect"
nuu_duration_random$groups = c(summary(nuu_duration_model)$ngrps,NA)
nuu_duration_random = nuu_duration_random[c("random_effect","groups","sdcor")]

# Give the random effects sensible names
nuu_duration_random$random_effect[nuu_duration_random$random_effect=="ELANParticipant"] = "speaker"
nuu_duration_random$random_effect[nuu_duration_random$random_effect=="ntvr_file"] = "text"

# And order them consistently
ordering = c(order(nuu_duration_random$random_effect)[2:3],order(nuu_duration_random$random_effect)[1])
nuu_duration_random = nuu_duration_random[ordering,]

# Add columns for various tests of the random effects
nuu_duration_random$m.r.2.change = NA
nuu_duration_random$c.r.2.change = NA
nuu_duration_random$chisq = NA
nuu_duration_random$df = NA
nuu_duration_random$p = NA
nuu_duration_random$sig = NA
  
# Train a similar model but exclude speaker as a random effect
nuu_duration_model.no_speaker = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ntvr_file), data=nuu_100_tokens, REML=TRUE)

# Train a similar model but exclude text as a random effect
nuu_duration_model.no_text = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant), data=nuu_100_tokens, REML=TRUE)

# Compare model without speaker as a random effect to full model and calculate log-likelihood ratio test as well as percentage change in marginal and conditional R²
comparison = anova(nuu_duration_model,nuu_duration_model.no_speaker,test="Chisq")
nuu_duration_random$chisq[nuu_duration_random$random_effect=="speaker"] = comparison$Chisq[2]
nuu_duration_random$df[nuu_duration_random$random_effect=="speaker"] = comparison$"Chi Df"[2]
nuu_duration_random$p[nuu_duration_random$random_effect=="speaker"] = comparison$"Pr(>Chisq)"[2]
nuu_duration_random$sig[nuu_duration_random$random_effect=="speaker"] = sub("^\\.$","n.s.",sub(" ","n.s.",stars.pval(comparison$"Pr(>Chisq)"[2])))
nuu_duration_random$m.r.2.change[nuu_duration_random$random_effect=="speaker"] = (nuu_duration_m.r.2 - r.squaredGLMM(nuu_duration_model.no_speaker)[1]) / nuu_duration_m.r.2 * 100
nuu_duration_random$c.r.2.change[nuu_duration_random$random_effect=="speaker"] = (nuu_duration_c.r.2 - r.squaredGLMM(nuu_duration_model.no_speaker)[2]) / nuu_duration_c.r.2 * 100

# Compare model without text as a random effect to full model and calculate log-likelihood ratio test as well as percentage change in marginal and conditional R²
comparison = anova(nuu_duration_model,nuu_duration_model.no_text,test="Chisq")
nuu_duration_random$chisq[nuu_duration_random$random_effect=="text"] = comparison$Chisq[2]
nuu_duration_random$df[nuu_duration_random$random_effect=="text"] = comparison$"Chi Df"[2]
nuu_duration_random$p[nuu_duration_random$random_effect=="text"] = comparison$"Pr(>Chisq)"[2]
nuu_duration_random$sig[nuu_duration_random$random_effect=="text"] = sub("^\\.$","n.s.",sub(" ","n.s.",stars.pval(comparison$"Pr(>Chisq)"[2])))
nuu_duration_random$m.r.2.change[nuu_duration_random$random_effect=="text"] = (nuu_duration_m.r.2 - r.squaredGLMM(nuu_duration_model.no_text)[1]) / nuu_duration_m.r.2 * 100
nuu_duration_random$c.r.2.change[nuu_duration_random$random_effect=="text"] = (nuu_duration_c.r.2 - r.squaredGLMM(nuu_duration_model.no_text)[2]) / nuu_duration_c.r.2 * 100

# Also save the number of tokens included in the model as well as the marginal and conditional R² of the complete model in the table for cross-linguistic comparison
results.word_duration.lmer$result[results.word_duration.lmer$language=="Nǁng" & results.word_duration.lmer$parameter=="n"] = summary(nuu_duration_model)$devcomp$dims[1]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Nǁng" & results.word_duration.lmer$parameter=="m.r.2"] = r.squaredGLMM(nuu_duration_model)[1]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Nǁng" & results.word_duration.lmer$parameter=="c.r.2"] = r.squaredGLMM(nuu_duration_model)[2]
```

```{r analysis_sakha, echo=FALSE}

# Make a list of the 100 most frequent word types
sakha_unique = unique(sakha[,c("word_type","word_type2","whole_word_gloss","ntvr_ps_root.simplified","word_freq","word_rel_freq","word_log_freq","word_log_rel_freq","word_freq_rank","word_freq_rank2","doc_freq","doc_freq_rank")])
sakha_100 = head(sakha_unique[order(sakha_unique$word_freq_rank,sakha_unique$doc_freq_rank),],100)

# Set row names to null
rownames(sakha_100) = NULL

# Calculate the mean and standard deviation of word duration and articulation speed per word using a for loop
# Fill these calculated values into the table of the 100 most frequent word types
sakha_100$mean_duration_per_word = NA
sakha_100$sd_duration_per_word = NA
sakha_100$mean_speed_per_word = NA
sakha_100$sd_speed_per_word = NA
for (word_type2 in unique(sakha_100$word_type2)) {
  sakha_100$mean_speed_per_word[sakha_100$word_type2==word_type2] = mean(sakha$speed_per_word[sakha$word_type2==word_type2],na.rm=T)
  sakha_100$sd_speed_per_word[sakha_100$word_type2==word_type2] = sd(sakha$speed_per_word[sakha$word_type2==word_type2],na.rm=T)
  sakha_100$mean_duration_per_word[sakha_100$word_type2==word_type2] = mean(sakha$WordLength[sakha$word_type2==word_type2],na.rm=T)
  sakha_100$sd_duration_per_word[sakha_100$word_type2==word_type2] = sd(sakha$WordLength[sakha$word_type2==word_type2],na.rm=T)
}

# Mininum frequency
sakha_min_frequency = min(sakha_100$word_freq)

# Collect a data set containing all tokens (instances) of these 100 most frequent word types in the corpus
sakha_100_word_types = paste(sakha_100$word_type,sakha_100$whole_word_gloss,sakha_100$ntvr_ps_root.simplified,sep="|")
sakha_100_tokens = droplevels(subset(sakha,subset=word_type2 %in% sakha_100_word_types))

# Add another column to the data preparation table ("word types manually excluded")
data_preparation_table$tokens_100_excluded[data_preparation_table$language=="Sakha"] = length(sakha_100_tokens$word)

# Exclude tokens contained in single-word annotation units
sakha_100_tokens = droplevels(subset(sakha_100_tokens,subset=words_per_record>1))
sakha_100_tokens = droplevels(subset(sakha_100_tokens,subset=chars_per_record>chars_per_word))

# Add another column to the data preparation table ("no one-word utterances (final data set)")
data_preparation_table$tokens_100_final[data_preparation_table$language=="Sakha"] = length(sakha_100_tokens$word)

# Delete column in table of the 100 most frequent words that is no longer needed
sakha_100$word_type2 = NULL

# Similarly for excluded words (that are occur on one of the blacklists)
sakha_unique_backup = unique(sakha_backup[,c("word_type","word_type2","whole_word_gloss","ntvr_ps_root.simplified","word_freq","word_rel_freq","doc_freq")])
sakha_excluded = subset(sakha_unique_backup[order(-sakha_unique_backup$word_freq,-sakha_unique_backup$doc_freq),],subset=word_type2 %in% sakha_aux_blacklist | word_type2 %in% sakha_n_blacklist)

# Just for statistics, count the number of instances (tokens) of the 100 most frequent words without excluding any specific words
sakha_100_backup = head(sakha_unique_backup[order(-sakha_unique_backup$word_freq,-sakha_unique_backup$doc_freq),],100)
sakha_100_word_types_backup = sakha_100_backup$word_type2
sakha_100_tokens_backup = droplevels(subset(sakha_backup,subset=word_type2 %in% sakha_100_word_types_backup))

# Add a column to the data preparation table ("100 most frequent word types")
data_preparation_table$tokens_100[data_preparation_table$language=="Sakha"] = length(sakha_100_tokens_backup$word)

# Only keep those excluded words that are at least as frequent as the 100th most frequent word)
sakha_excluded = subset(sakha_excluded,subset=word_freq >= sakha_min_frequency)

# Set rownames to null
rownames(sakha_excluded) = NULL

# Calculate the mean and standard deviation of word duration and articulation speed per word using a for loop
# Fill these calculated values into the table of excluded word types
sakha_excluded$mean_duration_per_word = NA
sakha_excluded$sd_duration_per_word = NA
sakha_excluded$mean_speed_per_word = NA
sakha_excluded$sd_speed_per_word = NA
for (word_type2 in unique(sakha_excluded$word_type2)) {
  sakha_excluded$mean_speed_per_word[sakha_excluded$word_type2==word_type2] = mean(sakha_backup$speed_per_word[sakha_backup$word_type2==word_type2],na.rm=T)
  sakha_excluded$sd_speed_per_word[sakha_excluded$word_type2==word_type2] = sd(sakha_backup$speed_per_word[sakha_backup$word_type2==word_type2],na.rm=T)
  sakha_excluded$mean_duration_per_word[sakha_excluded$word_type2==word_type2] = mean(sakha_backup$WordLength[sakha_backup$word_type2==word_type2],na.rm=T)
  sakha_excluded$sd_duration_per_word[sakha_excluded$word_type2==word_type2] = sd(sakha_backup$WordLength[sakha_backup$word_type2==word_type2],na.rm=T)
}

# Delete column in table of excluded word types that is no longer needed
sakha_excluded$word_type2 = NULL
```

```{r sakha_correlation_test, echo=FALSE}

# Carry out a bivariate correlation test using Spearman's rank correlation coefficient
sakha.cor.test = cor.test(sakha_100$word_log_rel_freq,sakha_100$mean_duration_per_word,method="spearman")

# Save the results in the table prepared for the bivariate analysis results
results.word_duration$result[results.word_duration$language=="Sakha" & results.word_duration$parameter=="spearman.rho"] = sakha.cor.test$estimate
results.word_duration$result[results.word_duration$language=="Sakha" & results.word_duration$parameter=="spearman.S"] = sakha.cor.test$statistic
results.word_duration$result[results.word_duration$language=="Sakha" & results.word_duration$parameter=="spearman.p"] = sakha.cor.test$p.value
results.word_duration$result[results.word_duration$language=="Sakha" & results.word_duration$parameter=="spearman.p.stars"] = stars.pval(sakha.cor.test$p.value)
results.word_duration$result[results.word_duration$language=="Sakha" & results.word_duration$parameter=="spearman.p.stars" & results.word_duration$result==" "] = "n.s."
```

```{r sakha_duration_model, echo=FALSE, size="footnotesize"}

# Convert text names and word types into factors
sakha_100_tokens$ntvr_file = factor(sakha_100_tokens$ntvr_file)
sakha_100_tokens$word_type2 = factor(sakha_100_tokens$word_type2)

# Train a linear mixed-effects model of log word duration predicted by log relative frequency, word length in characters, the number of morphemes per word, its position within the utterance,
# its part-of-speech (noun or verb), the articulation speed in the surrounding utterance and random factors for speaker (ELANParticipant) and text (ntvr_file)
sakha_duration_model = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=sakha_100_tokens, REML=TRUE)

# Create a table of model parameters for the Baure linear mixed-effects model
# Include raw coefficients
sakha_duration_result = as.data.frame(coef(summary(sakha_duration_model)))

# standardized beta coefficients
sakha_duration_result$beta = c(NA, lm.beta.lmer(sakha_duration_model))

# prepare table cells for change in marginal and conditional R² as well as semi-partial R²
sakha_duration_result$m.r.2.change = NA
sakha_duration_result$c.r.2.change = NA
sakha_duration_result$semi.partial.r2 = NA

# Add sensible names to the coefficients
rownames(sakha_duration_result) = c("(Intercept)","log word frequency","word length","number of morphemes","position","word class = verb (vs. noun)","log speech rate")

# Perform drop1 log-likelihood ratio tests for all fixed factors in the model
sakha_duration_result_drop1 = drop1(sakha_duration_model,test="Chisq")
sakha_duration_result$chisq = sakha_duration_result_drop1$LRT
sakha_duration_result$df = sakha_duration_result_drop1$Df
sakha_duration_result$p = sakha_duration_result_drop1$"Pr(Chi)"

# Add significance stars to the results of the log-likelihood ratio tests
sakha_duration_result$stars = stars.pval(sakha_duration_result$p)
sakha_duration_result$stars[1] = NA
sakha_duration_result$stars = sub(" ", "n.s.", sakha_duration_result$stars)
sakha_duration_result$stars = sub("^\\.$", "n.s.", sakha_duration_result$stars)

# Train a similar model but exclude word frequency as a fixed effect
sakha_duration_model.no_word_freq = lmer(log_word_length ~ chars_per_word + morphs_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=sakha_100_tokens, REML=TRUE)

# Train a similar model but exclude word length as a fixed effect
sakha_duration_model.no_word_length = lmer(log_word_length ~ word_log_rel_freq + morphs_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=sakha_100_tokens, REML=TRUE)

# Train a similar model but exclude the number of morphemes as a fixed effect
sakha_duration_model.no_morphs = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=sakha_100_tokens, REML=TRUE)

# Train a similar model but exclude word position as a fixed effect
sakha_duration_model.no_position = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=sakha_100_tokens, REML=TRUE)

# Train a similar model but exclude part-of-speech as a fixed effect
sakha_duration_model.no_pos = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + position_percentage + log_speed_per_record2 + (1|ELANParticipant) + (1|ntvr_file), data=sakha_100_tokens, REML=TRUE)

# Train a similar model but exclude articulation rate in the context as a fixed effect
sakha_duration_model.no_speech_rate = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + position_percentage + ntvr_ps_root.simplified + (1|ELANParticipant) + (1|ntvr_file), data=sakha_100_tokens, REML=TRUE)

# Calculate the marginal and conditional R² of the complete model
sakha_duration_m.r.2 = r.squaredGLMM(sakha_duration_model)[1]
sakha_duration_c.r.2 = r.squaredGLMM(sakha_duration_model)[2]

# Calculate semi-partial R² for the fixed effects in the complete model
sakha_duration_model.r2beta = r2beta(model=sakha_duration_model,method="nsj",partial=T)

# Calculate the percentage change in marginal and conditional R² as well as the semi-partial R² for each fixed effect in the model
sakha_duration_result["log word frequency","m.r.2.change"] = (sakha_duration_m.r.2 - r.squaredGLMM(sakha_duration_model.no_word_freq)[1]) / sakha_duration_m.r.2 * 100
sakha_duration_result["log word frequency","c.r.2.change"] = (sakha_duration_c.r.2 - r.squaredGLMM(sakha_duration_model.no_word_freq)[2]) / sakha_duration_c.r.2 * 100
sakha_duration_result["log word frequency","semi.partial.r2"] = sakha_duration_model.r2beta$Rsq[sakha_duration_model.r2beta$Effect=="word_log_rel_freq"]
sakha_duration_result["word length","m.r.2.change"] = (sakha_duration_m.r.2 - r.squaredGLMM(sakha_duration_model.no_word_length)[1]) / sakha_duration_m.r.2 * 100
sakha_duration_result["word length","c.r.2.change"] = (sakha_duration_c.r.2 - r.squaredGLMM(sakha_duration_model.no_word_length)[2]) / sakha_duration_c.r.2 * 100
sakha_duration_result["word length","semi.partial.r2"] = sakha_duration_model.r2beta$Rsq[sakha_duration_model.r2beta$Effect=="chars_per_word"]
sakha_duration_result["number of morphemes","m.r.2.change"] = (sakha_duration_m.r.2 - r.squaredGLMM(sakha_duration_model.no_morphs)[1]) / sakha_duration_m.r.2 * 100
sakha_duration_result["number of morphemes","c.r.2.change"] = (sakha_duration_c.r.2 - r.squaredGLMM(sakha_duration_model.no_morphs)[2]) / sakha_duration_c.r.2 * 100
sakha_duration_result["number of morphemes","semi.partial.r2"] = sakha_duration_model.r2beta$Rsq[sakha_duration_model.r2beta$Effect=="morphs_per_word"]
sakha_duration_result["position","m.r.2.change"] = (sakha_duration_m.r.2 - r.squaredGLMM(sakha_duration_model.no_position)[1]) / sakha_duration_m.r.2 * 100
sakha_duration_result["position","c.r.2.change"] = (sakha_duration_c.r.2 - r.squaredGLMM(sakha_duration_model.no_position)[2]) / sakha_duration_c.r.2 * 100
sakha_duration_result["position","semi.partial.r2"] = sakha_duration_model.r2beta$Rsq[sakha_duration_model.r2beta$Effect=="position_percentage"]
sakha_duration_result["word class = verb (vs. noun)","m.r.2.change"] = (sakha_duration_m.r.2 - r.squaredGLMM(sakha_duration_model.no_pos)[1]) / sakha_duration_m.r.2 * 100
sakha_duration_result["word class = verb (vs. noun)","c.r.2.change"] = (sakha_duration_c.r.2 - r.squaredGLMM(sakha_duration_model.no_pos)[2]) / sakha_duration_c.r.2 * 100
sakha_duration_result["word class = verb (vs. noun)","semi.partial.r2"] = sakha_duration_model.r2beta$Rsq[sakha_duration_model.r2beta$Effect=="ntvr_ps_root.simplifiedV"]
sakha_duration_result["log speech rate","m.r.2.change"] = (sakha_duration_m.r.2 - r.squaredGLMM(sakha_duration_model.no_speech_rate)[1]) / sakha_duration_m.r.2 * 100
sakha_duration_result["log speech rate","c.r.2.change"] = (sakha_duration_c.r.2 - r.squaredGLMM(sakha_duration_model.no_speech_rate)[2]) / sakha_duration_c.r.2 * 100
sakha_duration_result["log speech rate","semi.partial.r2"] = sakha_duration_model.r2beta$Rsq[sakha_duration_model.r2beta$Effect=="log_speed_per_record2"]

# Save the results concerning the main predictor of interest, namely, word frequency, in table for cross-linguistic comparison
results.word_duration.lmer$result[results.word_duration.lmer$language=="Sakha" & results.word_duration.lmer$parameter=="coefficient"] = sakha_duration_result["log word frequency","Estimate"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Sakha" & results.word_duration.lmer$parameter=="beta"] = sakha_duration_result["log word frequency","beta"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Sakha" & results.word_duration.lmer$parameter=="chisq"] = sakha_duration_result["log word frequency","chisq"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Sakha" & results.word_duration.lmer$parameter=="df"] = sakha_duration_result["log word frequency","df"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Sakha" & results.word_duration.lmer$parameter=="p"] = sakha_duration_result["log word frequency","p"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Sakha" & results.word_duration.lmer$parameter=="p.stars"] = sakha_duration_result["log word frequency","stars"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Sakha" & results.word_duration.lmer$parameter=="m.r.2.change"] = sakha_duration_result["log word frequency","m.r.2.change"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Sakha" & results.word_duration.lmer$parameter=="c.r.2.change"] = sakha_duration_result["log word frequency","c.r.2.change"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Sakha" & results.word_duration.lmer$parameter=="semi.partial.r2"] = sakha_duration_result["log word frequency","semi.partial.r2"]

# Create a table of the random effects in the model
sakha_duration_random = as.data.frame(summary(sakha_duration_model)$varcor)
colnames(sakha_duration_random)[1] = "random_effect"
sakha_duration_random$groups = c(summary(sakha_duration_model)$ngrps,NA)
sakha_duration_random = sakha_duration_random[c("random_effect","groups","sdcor")]

# Give the random effects sensible names
sakha_duration_random$random_effect[sakha_duration_random$random_effect=="ELANParticipant"] = "speaker"
sakha_duration_random$random_effect[sakha_duration_random$random_effect=="ntvr_file"] = "text"

# And order them consistently
ordering = c(order(sakha_duration_random$random_effect)[2:3],order(sakha_duration_random$random_effect)[1])
sakha_duration_random = sakha_duration_random[ordering,]

# Add columns for various tests of the random effects
sakha_duration_random$m.r.2.change = NA
sakha_duration_random$c.r.2.change = NA
sakha_duration_random$chisq = NA
sakha_duration_random$df = NA
sakha_duration_random$p = NA
sakha_duration_random$sig = NA
  
# Train a similar model but exclude speaker as a random effect
sakha_duration_model.no_speaker = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ntvr_file), data=sakha_100_tokens, REML=TRUE)

# Train a similar model but exclude text as a random effect
sakha_duration_model.no_text = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ELANParticipant), data=sakha_100_tokens, REML=TRUE)

# Compare model without speaker as a random effect to full model and calculate log-likelihood ratio test as well as percentage change in marginal and conditional R²
comparison = anova(sakha_duration_model,sakha_duration_model.no_speaker,test="Chisq")
sakha_duration_random$chisq[sakha_duration_random$random_effect=="speaker"] = comparison$Chisq[2]
sakha_duration_random$df[sakha_duration_random$random_effect=="speaker"] = comparison$"Chi Df"[2]
sakha_duration_random$p[sakha_duration_random$random_effect=="speaker"] = comparison$"Pr(>Chisq)"[2]
sakha_duration_random$sig[sakha_duration_random$random_effect=="speaker"] = sub("^\\.$","n.s.",sub(" ","n.s.",stars.pval(comparison$"Pr(>Chisq)"[2])))
sakha_duration_random$m.r.2.change[sakha_duration_random$random_effect=="speaker"] = (sakha_duration_m.r.2 - r.squaredGLMM(sakha_duration_model.no_speaker)[1]) / sakha_duration_m.r.2 * 100
sakha_duration_random$c.r.2.change[sakha_duration_random$random_effect=="speaker"] = (sakha_duration_c.r.2 - r.squaredGLMM(sakha_duration_model.no_speaker)[2]) / sakha_duration_c.r.2 * 100

# Compare model without text as a random effect to full model and calculate log-likelihood ratio test as well as percentage change in marginal and conditional R²
comparison = anova(sakha_duration_model,sakha_duration_model.no_text,test="Chisq")
sakha_duration_random$chisq[sakha_duration_random$random_effect=="text"] = comparison$Chisq[2]
sakha_duration_random$df[sakha_duration_random$random_effect=="text"] = comparison$"Chi Df"[2]
sakha_duration_random$p[sakha_duration_random$random_effect=="text"] = comparison$"Pr(>Chisq)"[2]
sakha_duration_random$sig[sakha_duration_random$random_effect=="text"] = sub("^\\.$","n.s.",sub(" ","n.s.",stars.pval(comparison$"Pr(>Chisq)"[2])))
sakha_duration_random$m.r.2.change[sakha_duration_random$random_effect=="text"] = (sakha_duration_m.r.2 - r.squaredGLMM(sakha_duration_model.no_text)[1]) / sakha_duration_m.r.2 * 100
sakha_duration_random$c.r.2.change[sakha_duration_random$random_effect=="text"] = (sakha_duration_c.r.2 - r.squaredGLMM(sakha_duration_model.no_text)[2]) / sakha_duration_c.r.2 * 100

# Also save the number of tokens included in the model as well as the marginal and conditional R² of the complete model in the table for cross-linguistic comparison
results.word_duration.lmer$result[results.word_duration.lmer$language=="Sakha" & results.word_duration.lmer$parameter=="n"] = summary(sakha_duration_model)$devcomp$dims[1]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Sakha" & results.word_duration.lmer$parameter=="m.r.2"] = r.squaredGLMM(sakha_duration_model)[1]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Sakha" & results.word_duration.lmer$parameter=="c.r.2"] = r.squaredGLMM(sakha_duration_model)[2]
```

```{r analysis_texistepec, echo=FALSE}

# Make a list of the 100 most frequent word types
texistepec_unique = unique(texistepec[,c("word_type","word_type2","whole_word_gloss","ntvr_ps_root.simplified","word_freq","word_rel_freq","word_log_freq","word_log_rel_freq","word_freq_rank","word_freq_rank2","doc_freq","doc_freq_rank")])
texistepec_100 = head(texistepec_unique[order(texistepec_unique$word_freq_rank,texistepec_unique$doc_freq_rank),],100)

# Set row names to null
rownames(texistepec_100) = NULL

# Calculate the mean and standard deviation of word duration and articulation speed per word using a for loop
# Fill these calculated values into the table of the 100 most frequent word types
texistepec_100$mean_duration_per_word = NA
texistepec_100$sd_duration_per_word = NA
texistepec_100$mean_speed_per_word = NA
texistepec_100$sd_speed_per_word = NA
for (word_type2 in unique(texistepec_100$word_type2)) {
  texistepec_100$mean_speed_per_word[texistepec_100$word_type2==word_type2] = mean(texistepec$speed_per_word[texistepec$word_type2==word_type2],na.rm=T)
  texistepec_100$sd_speed_per_word[texistepec_100$word_type2==word_type2] = sd(texistepec$speed_per_word[texistepec$word_type2==word_type2],na.rm=T)
  texistepec_100$mean_duration_per_word[texistepec_100$word_type2==word_type2] = mean(texistepec$WordLength[texistepec$word_type2==word_type2],na.rm=T)
  texistepec_100$sd_duration_per_word[texistepec_100$word_type2==word_type2] = sd(texistepec$WordLength[texistepec$word_type2==word_type2],na.rm=T)
}

# Mininum frequency
texistepec_min_frequency = min(texistepec_100$word_freq)

# Collect a data set containing all tokens (instances) of these 100 most frequent word types in the corpus
texistepec_100_word_types = paste(texistepec_100$word_type,texistepec_100$whole_word_gloss,texistepec_100$ntvr_ps_root.simplified,sep="|")
texistepec_100_tokens = droplevels(subset(texistepec,subset=word_type2 %in% texistepec_100_word_types))

# Add another column to the data preparation table ("word types manually excluded")
data_preparation_table$tokens_100_excluded[data_preparation_table$language=="Texistepec"] = length(texistepec_100_tokens$word)

# Exclude tokens contained in single-word annotation units
texistepec_100_tokens = droplevels(subset(texistepec_100_tokens,subset=words_per_record>1))
texistepec_100_tokens = droplevels(subset(texistepec_100_tokens,subset=chars_per_record>chars_per_word))

# Add another column to the data preparation table ("no one-word utterances (final data set)")
data_preparation_table$tokens_100_final[data_preparation_table$language=="Texistepec"] = length(texistepec_100_tokens$word)

# Delete column in table of the 100 most frequent words that is no longer needed
texistepec_100$word_type2 = NULL

# Similarly for excluded words (that are occur on one of the blacklists)
texistepec_unique_backup = unique(texistepec_backup[,c("word_type","word_type2","whole_word_gloss","ntvr_ps_root.simplified","word_freq","word_rel_freq","doc_freq")])
texistepec_excluded = subset(texistepec_unique_backup[order(-texistepec_unique_backup$word_freq,-texistepec_unique_backup$doc_freq),],subset=word_type2 %in% texistepec_aux_blacklist | word_type2 %in% texistepec_n_blacklist)

# Just for statistics, count the number of instances (tokens) of the 100 most frequent words without excluding any specific words
texistepec_100_backup = head(texistepec_unique_backup[order(-texistepec_unique_backup$word_freq,-texistepec_unique_backup$doc_freq),],100)
texistepec_100_word_types_backup = texistepec_100_backup$word_type2
texistepec_100_tokens_backup = droplevels(subset(texistepec_backup,subset=word_type2 %in% texistepec_100_word_types_backup))

# Add a column to the data preparation table ("100 most frequent word types")
data_preparation_table$tokens_100[data_preparation_table$language=="Texistepec"] = length(texistepec_100_tokens_backup$word)

# Only keep those excluded words that are at least as frequent as the 100th most frequent word)
texistepec_excluded = subset(texistepec_excluded,subset=word_freq >= texistepec_min_frequency)

# Set rownames to null
rownames(texistepec_excluded) = NULL

# Calculate the mean and standard deviation of word duration and articulation speed per word using a for loop
# Fill these calculated values into the table of excluded word types
texistepec_excluded$mean_duration_per_word = NA
texistepec_excluded$sd_duration_per_word = NA
texistepec_excluded$mean_speed_per_word = NA
texistepec_excluded$sd_speed_per_word = NA
for (word_type2 in unique(texistepec_excluded$word_type2)) {
  texistepec_excluded$mean_speed_per_word[texistepec_excluded$word_type2==word_type2] = mean(texistepec_backup$speed_per_word[texistepec_backup$word_type2==word_type2],na.rm=T)
  texistepec_excluded$sd_speed_per_word[texistepec_excluded$word_type2==word_type2] = sd(texistepec_backup$speed_per_word[texistepec_backup$word_type2==word_type2],na.rm=T)
  texistepec_excluded$mean_duration_per_word[texistepec_excluded$word_type2==word_type2] = mean(texistepec_backup$WordLength[texistepec_backup$word_type2==word_type2],na.rm=T)
  texistepec_excluded$sd_duration_per_word[texistepec_excluded$word_type2==word_type2] = sd(texistepec_backup$WordLength[texistepec_backup$word_type2==word_type2],na.rm=T)
}

# Delete column in table of excluded word types that is no longer needed
texistepec_excluded$word_type2 = NULL
```

```{r texistepec_correlation_test, echo=FALSE}

# Carry out a bivariate correlation test using Spearman's rank correlation coefficient
texistepec.cor.test = cor.test(texistepec_100$word_log_rel_freq,texistepec_100$mean_duration_per_word,method="spearman")

# Save the results in the table prepared for the bivariate analysis results
results.word_duration$result[results.word_duration$language=="Texistepec" & results.word_duration$parameter=="spearman.rho"] = texistepec.cor.test$estimate
results.word_duration$result[results.word_duration$language=="Texistepec" & results.word_duration$parameter=="spearman.S"] = texistepec.cor.test$statistic
results.word_duration$result[results.word_duration$language=="Texistepec" & results.word_duration$parameter=="spearman.p"] = texistepec.cor.test$p.value
results.word_duration$result[results.word_duration$language=="Texistepec" & results.word_duration$parameter=="spearman.p.stars"] = stars.pval(texistepec.cor.test$p.value)
results.word_duration$result[results.word_duration$language=="Texistepec" & results.word_duration$parameter=="spearman.p.stars" & results.word_duration$result==" "] = "n.s."
```

```{r texistepec_duration_model, echo=FALSE, size="footnotesize"}

# Convert text names and word types into factors
texistepec_100_tokens$ntvr_file = factor(texistepec_100_tokens$ntvr_file)
texistepec_100_tokens$word_type2 = factor(texistepec_100_tokens$word_type2)

# Train a linear mixed-effects model of log word duration predicted by log relative frequency, word length in characters, the number of morphemes per word, its position within the utterance,
# its part-of-speech (noun or verb), the articulation speed in the surrounding utterance and random factors for speaker (ELANParticipant) and text (ntvr_file)
texistepec_duration_model = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ntvr_file), data=texistepec_100_tokens, REML=TRUE)

# Create a table of model parameters for the Baure linear mixed-effects model
# Include raw coefficients
texistepec_duration_result = as.data.frame(coef(summary(texistepec_duration_model)))

# standardized beta coefficients
texistepec_duration_result$beta = c(NA, lm.beta.lmer(texistepec_duration_model))

# prepare table cells for change in marginal and conditional R² as well as semi-partial R²
texistepec_duration_result$m.r.2.change = NA
texistepec_duration_result$c.r.2.change = NA
texistepec_duration_result$semi.partial.r2 = NA

# Add sensible names to the coefficients
rownames(texistepec_duration_result) = c("(Intercept)","log word frequency","word length","number of morphemes","position","word class = verb (vs. noun)","log speech rate")

# Perform drop1 log-likelihood ratio tests for all fixed factors in the model
texistepec_duration_result_drop1 = drop1(texistepec_duration_model,test="Chisq")
texistepec_duration_result$chisq = texistepec_duration_result_drop1$LRT
texistepec_duration_result$df = texistepec_duration_result_drop1$Df
texistepec_duration_result$p = texistepec_duration_result_drop1$"Pr(Chi)"

# Add significance stars to the results of the log-likelihood ratio tests
texistepec_duration_result$stars = stars.pval(texistepec_duration_result$p)
texistepec_duration_result$stars[1] = NA
texistepec_duration_result$stars = sub(" ", "n.s.", texistepec_duration_result$stars)
texistepec_duration_result$stars = sub("^\\.$", "n.s.", texistepec_duration_result$stars)

# Train a similar model but exclude word frequency as a fixed effect
texistepec_duration_model.no_word_freq = lmer(log_word_length ~ chars_per_word + morphs_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ntvr_file), data=texistepec_100_tokens, REML=TRUE)

# Train a similar model but exclude word length as a fixed effect
texistepec_duration_model.no_word_length = lmer(log_word_length ~ word_log_rel_freq + morphs_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ntvr_file), data=texistepec_100_tokens, REML=TRUE)

# Train a similar model but exclude the number of morphemes as a fixed effect
texistepec_duration_model.no_morphs = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ntvr_file), data=texistepec_100_tokens, REML=TRUE)

# Train a similar model but exclude word position as a fixed effect
texistepec_duration_model.no_position = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + ntvr_ps_root.simplified + log_speed_per_record2 + (1|ntvr_file), data=texistepec_100_tokens, REML=TRUE)

# Train a similar model but exclude part-of-speech as a fixed effect
texistepec_duration_model.no_pos = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + position_percentage + log_speed_per_record2 + (1|ntvr_file), data=texistepec_100_tokens, REML=TRUE)

# Train a similar model but exclude articulation rate in the context as a fixed effect
texistepec_duration_model.no_speech_rate = lmer(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + position_percentage + ntvr_ps_root.simplified + (1|ntvr_file), data=texistepec_100_tokens, REML=TRUE)

# Calculate the marginal and conditional R² of the complete model
texistepec_duration_m.r.2 = r.squaredGLMM(texistepec_duration_model)[1]
texistepec_duration_c.r.2 = r.squaredGLMM(texistepec_duration_model)[2]

# Calculate semi-partial R² for the fixed effects in the complete model
texistepec_duration_model.r2beta = r2beta(model=texistepec_duration_model,method="nsj",partial=T)

# Calculate the percentage change in marginal and conditional R² as well as the semi-partial R² for each fixed effect in the model
texistepec_duration_result["log word frequency","m.r.2.change"] = (texistepec_duration_m.r.2 - r.squaredGLMM(texistepec_duration_model.no_word_freq)[1]) / texistepec_duration_m.r.2 * 100
texistepec_duration_result["log word frequency","c.r.2.change"] = (texistepec_duration_c.r.2 - r.squaredGLMM(texistepec_duration_model.no_word_freq)[2]) / texistepec_duration_c.r.2 * 100
texistepec_duration_result["log word frequency","semi.partial.r2"] = texistepec_duration_model.r2beta$Rsq[texistepec_duration_model.r2beta$Effect=="word_log_rel_freq"]
texistepec_duration_result["word length","m.r.2.change"] = (texistepec_duration_m.r.2 - r.squaredGLMM(texistepec_duration_model.no_word_length)[1]) / texistepec_duration_m.r.2 * 100
texistepec_duration_result["word length","c.r.2.change"] = (texistepec_duration_c.r.2 - r.squaredGLMM(texistepec_duration_model.no_word_length)[2]) / texistepec_duration_c.r.2 * 100
texistepec_duration_result["word length","semi.partial.r2"] = texistepec_duration_model.r2beta$Rsq[texistepec_duration_model.r2beta$Effect=="chars_per_word"]
texistepec_duration_result["number of morphemes","m.r.2.change"] = (texistepec_duration_m.r.2 - r.squaredGLMM(texistepec_duration_model.no_morphs)[1]) / texistepec_duration_m.r.2 * 100
texistepec_duration_result["number of morphemes","c.r.2.change"] = (texistepec_duration_c.r.2 - r.squaredGLMM(texistepec_duration_model.no_morphs)[2]) / texistepec_duration_c.r.2 * 100
texistepec_duration_result["number of morphemes","semi.partial.r2"] = texistepec_duration_model.r2beta$Rsq[texistepec_duration_model.r2beta$Effect=="morphs_per_word"]
texistepec_duration_result["position","m.r.2.change"] = (texistepec_duration_m.r.2 - r.squaredGLMM(texistepec_duration_model.no_position)[1]) / texistepec_duration_m.r.2 * 100
texistepec_duration_result["position","c.r.2.change"] = (texistepec_duration_c.r.2 - r.squaredGLMM(texistepec_duration_model.no_position)[2]) / texistepec_duration_c.r.2 * 100
texistepec_duration_result["position","semi.partial.r2"] = texistepec_duration_model.r2beta$Rsq[texistepec_duration_model.r2beta$Effect=="position_percentage"]
texistepec_duration_result["word class = verb (vs. noun)","m.r.2.change"] = (texistepec_duration_m.r.2 - r.squaredGLMM(texistepec_duration_model.no_pos)[1]) / texistepec_duration_m.r.2 * 100
texistepec_duration_result["word class = verb (vs. noun)","c.r.2.change"] = (texistepec_duration_c.r.2 - r.squaredGLMM(texistepec_duration_model.no_pos)[2]) / texistepec_duration_c.r.2 * 100
texistepec_duration_result["word class = verb (vs. noun)","semi.partial.r2"] = texistepec_duration_model.r2beta$Rsq[texistepec_duration_model.r2beta$Effect=="ntvr_ps_root.simplifiedV"]
texistepec_duration_result["log speech rate","m.r.2.change"] = (texistepec_duration_m.r.2 - r.squaredGLMM(texistepec_duration_model.no_speech_rate)[1]) / texistepec_duration_m.r.2 * 100
texistepec_duration_result["log speech rate","c.r.2.change"] = (texistepec_duration_c.r.2 - r.squaredGLMM(texistepec_duration_model.no_speech_rate)[2]) / texistepec_duration_c.r.2 * 100
texistepec_duration_result["log speech rate","semi.partial.r2"] = texistepec_duration_model.r2beta$Rsq[texistepec_duration_model.r2beta$Effect=="log_speed_per_record2"]

# Save the results concerning the main predictor of interest, namely, word frequency, in table for cross-linguistic comparison
results.word_duration.lmer$result[results.word_duration.lmer$language=="Texistepec" & results.word_duration.lmer$parameter=="coefficient"] = texistepec_duration_result["log word frequency","Estimate"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Texistepec" & results.word_duration.lmer$parameter=="beta"] = texistepec_duration_result["log word frequency","beta"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Texistepec" & results.word_duration.lmer$parameter=="chisq"] = texistepec_duration_result["log word frequency","chisq"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Texistepec" & results.word_duration.lmer$parameter=="df"] = texistepec_duration_result["log word frequency","df"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Texistepec" & results.word_duration.lmer$parameter=="p"] = texistepec_duration_result["log word frequency","p"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Texistepec" & results.word_duration.lmer$parameter=="p.stars"] = texistepec_duration_result["log word frequency","stars"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Texistepec" & results.word_duration.lmer$parameter=="m.r.2.change"] = texistepec_duration_result["log word frequency","m.r.2.change"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Texistepec" & results.word_duration.lmer$parameter=="c.r.2.change"] = texistepec_duration_result["log word frequency","c.r.2.change"]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Texistepec" & results.word_duration.lmer$parameter=="semi.partial.r2"] = texistepec_duration_result["log word frequency","semi.partial.r2"]

# Create a table of the random effects in the model
texistepec_duration_random = as.data.frame(summary(texistepec_duration_model)$varcor)
colnames(texistepec_duration_random)[1] = "random_effect"
texistepec_duration_random$groups = c(summary(texistepec_duration_model)$ngrps,NA)
texistepec_duration_random = texistepec_duration_random[c("random_effect","groups","sdcor")]

# Give the random effects sensible names
# (The Texistepec subcorpus only contains recordings of one speaker, therefore there is no random effect for speaker for Texistepec)
texistepec_duration_random$random_effect[texistepec_duration_random$random_effect=="ntvr_file"] = "text"

# And order them consistently
ordering = c(order(texistepec_duration_random$random_effect)[2],order(texistepec_duration_random$random_effect)[1])
texistepec_duration_random = texistepec_duration_random[ordering,]

# Add columns for various tests of the random effects
texistepec_duration_random$m.r.2.change = NA
texistepec_duration_random$c.r.2.change = NA
texistepec_duration_random$chisq = NA
texistepec_duration_random$df = NA
texistepec_duration_random$p = NA
texistepec_duration_random$sig = NA

# Train a similar model but exclude text as a random effect
texistepec_duration_model.no_text = lm(log_word_length ~ word_log_rel_freq + chars_per_word + morphs_per_word + position_percentage + ntvr_ps_root.simplified + log_speed_per_record2, data=texistepec_100_tokens)

# Compare model without text as a random effect to full model and calculate log-likelihood ratio test as well as percentage change in marginal and conditional R²
comparison = anova(texistepec_duration_model,texistepec_duration_model.no_text,test="Chisq")
texistepec_duration_random$chisq[texistepec_duration_random$random_effect=="text"] = comparison$Chisq[2]
texistepec_duration_random$df[texistepec_duration_random$random_effect=="text"] = comparison$"Chi Df"[2]
texistepec_duration_random$p[texistepec_duration_random$random_effect=="text"] = comparison$"Pr(>Chisq)"[2]
texistepec_duration_random$sig[texistepec_duration_random$random_effect=="text"] = sub("^\\.$","n.s.",sub(" ","n.s.",stars.pval(comparison$"Pr(>Chisq)"[2])))
texistepec_duration_random$m.r.2.change[texistepec_duration_random$random_effect=="text"] = (texistepec_duration_m.r.2 - r.squaredGLMM(texistepec_duration_model.no_text)[1]) / texistepec_duration_m.r.2 * 100
texistepec_duration_random$c.r.2.change[texistepec_duration_random$random_effect=="text"] = (texistepec_duration_c.r.2 - r.squaredGLMM(texistepec_duration_model.no_text)[2]) / texistepec_duration_c.r.2 * 100

# Also save the number of tokens included in the model as well as the marginal and conditional R² of the complete model in the table for cross-linguistic comparison
results.word_duration.lmer$result[results.word_duration.lmer$language=="Texistepec" & results.word_duration.lmer$parameter=="n"] = summary(texistepec_duration_model)$devcomp$dims[1]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Texistepec" & results.word_duration.lmer$parameter=="m.r.2"] = r.squaredGLMM(texistepec_duration_model)[1]
results.word_duration.lmer$result[results.word_duration.lmer$language=="Texistepec" & results.word_duration.lmer$parameter=="c.r.2"] = r.squaredGLMM(texistepec_duration_model)[2]
```

```{r assemble_data, echo=FALSE}

# Convert language names in data frames containing the tokens of the 100 most frequent word types for each langauge into strings in order to be able to change them
baure_100_tokens$language = as.character(baure_100_tokens$language)
bora_100_tokens$language = as.character(bora_100_tokens$language)
chintang_100_tokens$language = as.character(chintang_100_tokens$language)
chintang2_100_tokens$language = as.character(chintang2_100_tokens$language)
dutch_100_tokens$language = as.character(dutch_100_tokens$language)
english_100_tokens$language = as.character(english_100_tokens$language)
even_100_tokens$language = as.character(even_100_tokens$language)
hoocak_100_tokens$language = as.character(hoocak_100_tokens$language)
nuu_100_tokens$language = as.character(nuu_100_tokens$language)
sakha_100_tokens$language = as.character(sakha_100_tokens$language)
texistepec_100_tokens$language = as.character(texistepec_100_tokens$language)

chintang2_100_tokens$language = "Chintang (no pear stories)"

# Combine all data frames for the subcorpora into one large data frame for plotting purposes
all_100_tokens = rbind(baure_100_tokens,bora_100_tokens,chintang_100_tokens,chintang2_100_tokens,dutch_100_tokens,english_100_tokens,even_100_tokens,hoocak_100_tokens,nuu_100_tokens,sakha_100_tokens,texistepec_100_tokens)

# Add a new column word class with clearer part-of-speech designations (converted into a factor)
all_100_tokens$word_class  = NA
all_100_tokens$word_class[all_100_tokens$ntvr_ps_root.simplified=="N"] = "nouns"
all_100_tokens$word_class[all_100_tokens$ntvr_ps_root.simplified=="V"] = "verbs"
all_100_tokens$word_class = factor(all_100_tokens$word_class)

# Convert all language names into a factor (with a alphabetically ordered levels)
all_100_tokens$language = factor(all_100_tokens$language,levels=c("Baure","Bora","Chintang","Chintang (no pear stories)","Dutch","English","Even","Hoocąk","Nǁng","Sakha","Texistepec"))

# Create a combined data frame for the 100 most frequent word types for each language
all_100_types = rbind(cbind(language="Baure",baure_100[c("word_freq","word_log_rel_freq","word_freq_rank2","mean_duration_per_word")],word_log_rel_freq_rank=rank(baure_100$word_log_rel_freq),mean_duration_per_word_rank=rank(baure_100$mean_duration_per_word)),cbind(language="Bora",bora_100[c("word_freq","word_log_rel_freq","word_freq_rank2","mean_duration_per_word")],word_log_rel_freq_rank=rank(bora_100$word_log_rel_freq),mean_duration_per_word_rank=rank(bora_100$mean_duration_per_word)),cbind(language="Chintang",chintang_100[c("word_freq","word_log_rel_freq","word_freq_rank2","mean_duration_per_word")],word_log_rel_freq_rank=rank(chintang_100$word_log_rel_freq),mean_duration_per_word_rank=rank(chintang_100$mean_duration_per_word)),cbind(language="Chintang (no pear stories)",chintang2_100[c("word_freq","word_log_rel_freq","word_freq_rank2","mean_duration_per_word")],word_log_rel_freq_rank=rank(chintang2_100$word_log_rel_freq),mean_duration_per_word_rank=rank(chintang2_100$mean_duration_per_word)),cbind(language="Dutch",dutch_100[c("word_freq","word_log_rel_freq","word_freq_rank2","mean_duration_per_word")],word_log_rel_freq_rank=rank(dutch_100$word_log_rel_freq),mean_duration_per_word_rank=rank(dutch_100$mean_duration_per_word)),cbind(language="English",english_100[c("word_freq","word_log_rel_freq","word_freq_rank2","mean_duration_per_word")],word_log_rel_freq_rank=rank(english_100$word_log_rel_freq),mean_duration_per_word_rank=rank(english_100$mean_duration_per_word)),cbind(language="Even",even_100[c("word_freq","word_log_rel_freq","word_freq_rank2","mean_duration_per_word")],word_log_rel_freq_rank=rank(even_100$word_log_rel_freq),mean_duration_per_word_rank=rank(even_100$mean_duration_per_word)),cbind(language="Hoocąk",hoocak_100[c("word_freq","word_log_rel_freq","word_freq_rank2","mean_duration_per_word")],word_log_rel_freq_rank=rank(hoocak_100$word_log_rel_freq),mean_duration_per_word_rank=rank(hoocak_100$mean_duration_per_word)),cbind(language="Nǁng",nuu_100[c("word_freq","word_log_rel_freq","word_freq_rank2","mean_duration_per_word")],word_log_rel_freq_rank=rank(nuu_100$word_log_rel_freq),mean_duration_per_word_rank=rank(nuu_100$mean_duration_per_word)),cbind(language="Sakha",sakha_100[c("word_freq","word_log_rel_freq","word_freq_rank2","mean_duration_per_word")],word_log_rel_freq_rank=rank(sakha_100$word_log_rel_freq),mean_duration_per_word_rank=rank(sakha_100$mean_duration_per_word)),cbind(language="Texistepec",texistepec_100[c("word_freq","word_log_rel_freq","word_freq_rank2","mean_duration_per_word")],word_log_rel_freq_rank=rank(texistepec_100$word_log_rel_freq),mean_duration_per_word_rank=rank(texistepec_100$mean_duration_per_word)))
```


# Introduction {#introduction_section}

```{r autotyp_map, echo=FALSE, fig.cap="Map of languages included in the study\\label{fig:autotyp_map}"}

# Create a map of the languages considered in the paper using AUTOTYP data
load("ntvr.autotyp.RData")
load("autotyp.map.RData")
autotyp.map(ntvr.autotyp, color='red', labels='language', label.pos=c(4,2,3,3,1,3,1,3,3,3), cex=1.4, label.cex=1.6, label.offset=0.6)
```


## Data preparation {#data_preparation_section}

```{r language_table, echo=FALSE}

# Output the data preparation table (after calculating column sums)
data_preparation_table$tokens_100[data_preparation_table$language=="\\textbf{total}"] = sum(data_preparation_table$tokens_100,na.rm=TRUE)
data_preparation_table$tokens_100_excluded[data_preparation_table$language=="\\textbf{total}"] = sum(data_preparation_table$tokens_100_excluded,na.rm=TRUE)
data_preparation_table$tokens_100_final[data_preparation_table$language=="\\textbf{total}"] = sum(data_preparation_table$tokens_100_final,na.rm=TRUE)
kable(data_preparation_table[,c(1,2,3,4,5,7,8,9)],col.names=c("language","all words","no disfluencies","nouns and verbs only","no ambiguous words (data set for word lists)","100 most frequent word types","word types manually excluded","no one-word utterances (final data set)"),caption="Data selection and final data sets per language (number of word tokens)\\label{tab:language_table}",format.args=list(big.mark=" "),escape=FALSE,format="latex",booktabs=T) %>% kable_styling(position="center",latex_options=c("scale_down","hold_position")) %>% column_spec(4, width = "8em") %>% column_spec(5, width = "8em") %>% column_spec(6, width = "8em") %>% column_spec(7, width = "7em") %>% column_spec(8, width = "8em")
```

```{r aus_table, echo=FALSE}

# Output a table about the number of words in annotation units 
kable(annotation_units)
```


```{r word_length_table, echo=FALSE}

# Output a table with statistics about word length and morphological complexity for each language
kable(word.length.table,col.names=c("language","mean","std. dev.","median","mean","std. dev.","median"),caption="Word length and morphological complexity per language\\label{tab:word_length_table}",digits=2,format="latex",booktabs=TRUE) %>% kable_styling(full_width=FALSE,position="center",latex_options=c("hold_position")) %>% add_header_above(c(" " = 1, "length in segments" = 3, "length in morphemes" = 3)) %>% footnote(general=c("The Dutch corpus does not provide information about","the morphological segmentation of words. Therefore, the number of","morphemes per word is always one for Dutch. The multivariate model","for Dutch therefore does not include the factor number of morphemes."),general_title="Notes:")
```


# Results {#results_section}

```{r bivariate_correlation_plots, echo=FALSE, fig.height=10, fig.cap="Bivariate correlation between word frequency and average word duration based on Spearman's nonparametric rank correlation coefficient (plots based on ranks of word frequency and duration, correlation coefficients and test results\\label{fig:correlation_plot} are given beneath the strip titles)"}

results.word_duration$result = as.numeric(results.word_duration$result)

# Cast bivariate word duration results into a different table format
results.word_duration.cast = dcast(results.word_duration, language ~ parameter)

# Add significance stars
results.word_duration.cast$spearman.p.stars = stars.pval(results.word_duration.cast$spearman.p)
results.word_duration.cast$spearman.p.stars[results.word_duration.cast$spearman.p.stars==" "] = "n.s."
results.word_duration.cast$spearman.p.stars[results.word_duration.cast$spearman.p.stars=="."] = "n.s."

all_100_types$language_label = as.character(all_100_types$language)

# Create labels with the results of a correlation test (using Spearman's rank correlation coefficient) for each language
for (language in all_100_types$language) {
all_100_types$language_label[all_100_types$language==language] = paste(language,"\n(ρ = ", round(results.word_duration$result[results.word_duration$language==language & results.word_duration$parameter=="spearman.rho"],2), ", S = ", round(results.word_duration$result[results.word_duration$language==language & results.word_duration$parameter=="spearman.S"]), ", ", ifelse(results.word_duration$result[results.word_duration$language==language & results.word_duration$parameter=="spearman.p"] < 0.001, "p < 0.001", paste("p = ", round(results.word_duration$result[results.word_duration$language==language & results.word_duration$parameter=="spearman.p"],3), sep="")), ")", sep="")
}

# Convert these labels to a factor (with a sensible ordering of factor levels)
all_100_types$language_label = factor(all_100_types$language_label)
levels(all_100_types$language_label) <- c(levels(all_100_types$language_label)[1:2], levels(all_100_types$language_label)[4], levels(all_100_types$language_label)[3], levels(all_100_types$language_label)[5:11])

# Create plots of the bivariate correlation between word frequency rank and word duration rank
ggplot(all_100_types,aes(x=word_log_rel_freq_rank,y=mean_duration_per_word_rank)) + geom_point() + geom_smooth(method="lm",color="blue") + geom_smooth(method="loess",color="darkgray", se=FALSE) + ylab("rank of mean word duration") + xlab("rank of log word frequency") + facet_wrap(~ language_label, ncol=3, scales="free") + theme(legend.title=element_text(size=15),legend.text=element_text(size=15),axis.text=element_text(size=12),axis.title=element_text(size=15),plot.title=element_text(size=17), strip.text=element_text(size=16))
```


```{r summary_word_duration_lmer, echo=FALSE, size="footnotesize"}

# Create a table summarizing the effect of word frequency in the linear mixed-effects models for the different languages
results.word_duration.lmer$result = as.numeric(results.word_duration.lmer$result)
results.word_duration.lmer.cast = dcast(results.word_duration.lmer, language ~ parameter)

# Add significance stars
results.word_duration.lmer.cast$p.stars = stars.pval(results.word_duration.lmer.cast$p)
results.word_duration.lmer.cast$p.stars[results.word_duration.lmer.cast$p.stars==" "] = "n.s."
results.word_duration.lmer.cast$p.stars[results.word_duration.lmer.cast$p.stars=="."] = "n.s."

# Output table summarizing the effect of word frequency in the linear mixed-effects models for the different languages
kable(results.word_duration.lmer.cast,col.names=c("language","n","$R^2_{(m)}$","$R^2_{(c)}$","coefficient","$\\beta$","$\\Delta R^2_{(m)}$\\%","$\\Delta R^2_{(c)}$\\%","s.-p. $R^2$","$\\chi^2$","$df$","$p$",""),caption="Summary of results on the word frequency effect on word duration (based on multivariate models)\\label{tab:word_frequency_effect}",digits=4,row.names=FALSE,format.args=list(big.mark=" "),format="latex",booktabs=TRUE,escape=FALSE) %>% kable_styling(position="center",latex_options=c("scale_down","hold_position"))
```


```{r effect_word_duration_lmer, echo=FALSE}

# Prepare data for an effects plot of the effect of word frequency on word duration based on the linear mixed-effects models for each language

# Derive predictions from the model for Baure
frequency.effect.word_duration.baure = as.data.frame(effect("word_log_rel_freq",baure_duration_model,xlevels=20,transformation=list(link=log, inverse=exp)))
frequency.effect.word_duration.baure$language = "Baure"
frequency.effect.word_duration.baure$middle_x = (max(exp(frequency.effect.word_duration.baure$word_log_rel_freq)) + min(exp(frequency.effect.word_duration.baure$word_log_rel_freq))) / 2
frequency.effect.word_duration.baure$middle_y = (max(frequency.effect.word_duration.baure$fit) + min(frequency.effect.word_duration.baure$fit)) / 2

# Derive predictions from the model for Bora
frequency.effect.word_duration.bora = as.data.frame(effect("word_log_rel_freq",bora_duration_model,xlevels=20,transformation=list(link=log, inverse=exp)))
frequency.effect.word_duration.bora$language = "Bora"
frequency.effect.word_duration.bora$middle_x = (max(exp(frequency.effect.word_duration.bora$word_log_rel_freq)) + min(exp(frequency.effect.word_duration.bora$word_log_rel_freq))) / 2
frequency.effect.word_duration.bora$middle_y = (max(frequency.effect.word_duration.bora$fit) + min(frequency.effect.word_duration.bora$fit)) / 2

# Derive predictions from the model for Chintang
frequency.effect.word_duration.chintang = as.data.frame(effect("word_log_rel_freq",chintang_duration_model,xlevels=20,transformation=list(link=log, inverse=exp)))
frequency.effect.word_duration.chintang$language = "Chintang"
frequency.effect.word_duration.chintang$middle_x = (max(exp(frequency.effect.word_duration.chintang$word_log_rel_freq)) + min(exp(frequency.effect.word_duration.chintang$word_log_rel_freq))) / 2
frequency.effect.word_duration.chintang$middle_y = (max(frequency.effect.word_duration.chintang$fit) + min(frequency.effect.word_duration.chintang$fit)) / 2

# Derive predictions from the model for Chintang (without pear stories)
frequency.effect.word_duration.chintang2 = as.data.frame(effect("word_log_rel_freq",chintang2_duration_model,xlevels=20,transformation=list(link=log, inverse=exp)))
frequency.effect.word_duration.chintang2$language = "Chintang (no pear stories)"
frequency.effect.word_duration.chintang2$middle_x = (max(exp(frequency.effect.word_duration.chintang2$word_log_rel_freq)) + min(exp(frequency.effect.word_duration.chintang2$word_log_rel_freq))) / 2
frequency.effect.word_duration.chintang2$middle_y = (max(frequency.effect.word_duration.chintang2$fit) + min(frequency.effect.word_duration.chintang2$fit)) / 2

# Derive predictions from the model for Dutch
frequency.effect.word_duration.dutch = as.data.frame(effect("word_log_rel_freq",dutch_duration_model,xlevels=20,transformation=list(link=log, inverse=exp)))
frequency.effect.word_duration.dutch$language = "Dutch"
frequency.effect.word_duration.dutch$middle_x = (max(exp(frequency.effect.word_duration.dutch$word_log_rel_freq)) + min(exp(frequency.effect.word_duration.dutch$word_log_rel_freq))) / 2
frequency.effect.word_duration.dutch$middle_y = (max(frequency.effect.word_duration.dutch$fit) + min(frequency.effect.word_duration.dutch$fit)) / 2

# Derive predictions from the model for English
frequency.effect.word_duration.english = as.data.frame(effect("word_log_rel_freq",english_duration_model,xlevels=20,transformation=list(link=log, inverse=exp)))
frequency.effect.word_duration.english$language = "English"
frequency.effect.word_duration.english$middle_x = (max(exp(frequency.effect.word_duration.english$word_log_rel_freq)) + min(exp(frequency.effect.word_duration.english$word_log_rel_freq))) / 2
frequency.effect.word_duration.english$middle_y = (max(frequency.effect.word_duration.english$fit) + min(frequency.effect.word_duration.english$fit)) / 2

# Derive predictions from the model for Even
frequency.effect.word_duration.even = as.data.frame(effect("word_log_rel_freq",even_duration_model,xlevels=20,transformation=list(link=log, inverse=exp)))
frequency.effect.word_duration.even$language = "Even"
frequency.effect.word_duration.even$middle_x = (max(exp(frequency.effect.word_duration.even$word_log_rel_freq)) + min(exp(frequency.effect.word_duration.even$word_log_rel_freq))) / 2
frequency.effect.word_duration.even$middle_y = (max(frequency.effect.word_duration.even$fit) + min(frequency.effect.word_duration.even$fit)) / 2

# Derive predictions from the model for Hoocąk
frequency.effect.word_duration.hoocak = as.data.frame(effect("word_log_rel_freq",hoocak_duration_model,xlevels=20,transformation=list(link=log, inverse=exp)))
frequency.effect.word_duration.hoocak$language = "Hoocąk"
frequency.effect.word_duration.hoocak$middle_x = (max(exp(frequency.effect.word_duration.hoocak$word_log_rel_freq)) + min(exp(frequency.effect.word_duration.hoocak$word_log_rel_freq))) / 2
frequency.effect.word_duration.hoocak$middle_y = (max(frequency.effect.word_duration.hoocak$fit) + min(frequency.effect.word_duration.hoocak$fit)) / 2

# Derive predictions from the model for Nǁng
frequency.effect.word_duration.nuu = as.data.frame(effect("word_log_rel_freq",nuu_duration_model,xlevels=20,transformation=list(link=log, inverse=exp)))
frequency.effect.word_duration.nuu$language = "Nǁng"
frequency.effect.word_duration.nuu$middle_x = (max(exp(frequency.effect.word_duration.nuu$word_log_rel_freq)) + min(exp(frequency.effect.word_duration.nuu$word_log_rel_freq))) / 2
frequency.effect.word_duration.nuu$middle_y = (max(frequency.effect.word_duration.nuu$fit) + min(frequency.effect.word_duration.nuu$fit)) / 2

# Derive predictions from the model for Sakha
frequency.effect.word_duration.sakha = as.data.frame(effect("word_log_rel_freq",sakha_duration_model,xlevels=20,transformation=list(link=log, inverse=exp)))
frequency.effect.word_duration.sakha$language = "Sakha"
frequency.effect.word_duration.sakha$middle_x = (max(exp(frequency.effect.word_duration.sakha$word_log_rel_freq)) + min(exp(frequency.effect.word_duration.sakha$word_log_rel_freq))) / 2
frequency.effect.word_duration.sakha$middle_y = (max(frequency.effect.word_duration.sakha$fit) + min(frequency.effect.word_duration.sakha$fit)) / 2

# Derive predictions from the model for Texistepec Popoluca
frequency.effect.word_duration.texistepec = as.data.frame(effect("word_log_rel_freq",texistepec_duration_model,xlevels=20,transformation=list(link=log, inverse=exp)))
frequency.effect.word_duration.texistepec$language = "Texistepec"
frequency.effect.word_duration.texistepec$middle_x = (max(exp(frequency.effect.word_duration.texistepec$word_log_rel_freq)) + min(exp(frequency.effect.word_duration.texistepec$word_log_rel_freq))) / 2
frequency.effect.word_duration.texistepec$middle_y = (max(frequency.effect.word_duration.texistepec$fit) + min(frequency.effect.word_duration.texistepec$fit)) / 2

# Combine the predictions for all languages into one big table
frequency.effect.word_duration = rbind(frequency.effect.word_duration.baure,frequency.effect.word_duration.bora,frequency.effect.word_duration.chintang,frequency.effect.word_duration.chintang2,frequency.effect.word_duration.dutch,frequency.effect.word_duration.english,frequency.effect.word_duration.even,frequency.effect.word_duration.hoocak,frequency.effect.word_duration.nuu,frequency.effect.word_duration.sakha,frequency.effect.word_duration.texistepec)

# Add significance stars
frequency.effect.word_duration$significance[frequency.effect.word_duration$language=="Baure"] = unique(results.word_duration.lmer.cast$p.stars[results.word_duration.lmer.cast$language=="Baure"])
frequency.effect.word_duration$significance[frequency.effect.word_duration$language=="Bora"] = unique(results.word_duration.lmer.cast$p.stars[results.word_duration.lmer.cast$language=="Bora"])
frequency.effect.word_duration$significance[frequency.effect.word_duration$language=="Chintang"] = unique(results.word_duration.lmer.cast$p.stars[results.word_duration.lmer.cast$language=="Chintang"])
frequency.effect.word_duration$significance[frequency.effect.word_duration$language=="Chintang (no pear stories)"] = unique(results.word_duration.lmer.cast$p.stars[results.word_duration.lmer.cast$language=="Chintang (no pear stories)"])
frequency.effect.word_duration$significance[frequency.effect.word_duration$language=="Dutch"] = unique(results.word_duration.lmer.cast$p.stars[results.word_duration.lmer.cast$language=="Dutch"])
frequency.effect.word_duration$significance[frequency.effect.word_duration$language=="English"] = unique(results.word_duration.lmer.cast$p.stars[results.word_duration.lmer.cast$language=="English"])
frequency.effect.word_duration$significance[frequency.effect.word_duration$language=="Even"] = unique(results.word_duration.lmer.cast$p.stars[results.word_duration.lmer.cast$language=="Even"])
frequency.effect.word_duration$significance[frequency.effect.word_duration$language=="Hoocąk"] = unique(results.word_duration.lmer.cast$p.stars[results.word_duration.lmer.cast$language=="Hoocąk"])
frequency.effect.word_duration$significance[frequency.effect.word_duration$language=="Nǁng"] = unique(results.word_duration.lmer.cast$p.stars[results.word_duration.lmer.cast$language=="Nǁng"])
frequency.effect.word_duration$significance[frequency.effect.word_duration$language=="Sakha"] = unique(results.word_duration.lmer.cast$p.stars[results.word_duration.lmer.cast$language=="Sakha"])
frequency.effect.word_duration$significance[frequency.effect.word_duration$language=="Texistepec"] = unique(results.word_duration.lmer.cast$p.stars[results.word_duration.lmer.cast$language=="Texistepec"])
```

```{r effect_plot_word_duration_lmer, echo=FALSE, fig.height=10, fig.cap="Effect display of the effect of word frequency on word duration in the individual languages\\label{fig:effect_plot_word_duration_lmer}"}

# Create a facetted effects plot of the effect of word frequency on word duration in the different languages
ggplot(frequency.effect.word_duration,aes(exp(word_log_rel_freq),fit,fit)) + geom_line(color="black") + geom_ribbon(aes(ymin = lower, ymax = upper),fill="gray",alpha=0.2,show.legend=FALSE) + facet_wrap(~language, ncol=3, scales="free") + ylab("Word duration (in seconds)") + xlab("Word frequency") + ylim(0.18,0.6) + theme(legend.title=element_text(size=15),legend.text=element_text(size=15),axis.text=element_text(size=12),axis.title=element_text(size=15),plot.title=element_text(size=17),strip.text = element_text(size=16)) + geom_text(aes(x=middle_x,y=middle_y*1.10,label=significance),color="black")
```


```{r comparison_of_coefficients, echo=FALSE}

# Add additional missing row to Dutch table (number of morphemes) since there is no segmentation into morphemes in our Dutch corpus
dutch_duration_result = rbind(dutch_duration_result[1:3,],NA,dutch_duration_result[4:6,])
rownames(dutch_duration_result)[4] = "number of morphemes"

# Combine information about the coefficients in the linear mixed-effects models for all languages in one big table
duration_results = rbind(cbind(as.vector(baure_duration_result[2,])[c(4,5,11)],as.vector(baure_duration_result[3,])[c(4,5,11)],as.vector(baure_duration_result[4,])[c(4,5,11)],as.vector(baure_duration_result[5,])[c(4,5,11)],as.vector(baure_duration_result[6,])[c(4,5,11)],as.vector(baure_duration_result[7,])[c(4,5,11)]),cbind(as.vector(bora_duration_result[2,])[c(4,5,11)],as.vector(bora_duration_result[3,])[c(4,5,11)],as.vector(bora_duration_result[4,])[c(4,5,11)],as.vector(bora_duration_result[5,])[c(4,5,11)],as.vector(bora_duration_result[6,])[c(4,5,11)],as.vector(bora_duration_result[7,])[c(4,5,11)]),cbind(as.vector(chintang_duration_result[2,])[c(4,5,11)],as.vector(chintang_duration_result[3,])[c(4,5,11)],as.vector(chintang_duration_result[4,])[c(4,5,11)],as.vector(chintang_duration_result[5,])[c(4,5,11)],as.vector(chintang_duration_result[6,])[c(4,5,11)],as.vector(chintang_duration_result[7,])[c(4,5,11)]),cbind(as.vector(chintang2_duration_result[2,])[c(4,5,11)],as.vector(chintang2_duration_result[3,])[c(4,5,11)],as.vector(chintang2_duration_result[4,])[c(4,5,11)],as.vector(chintang2_duration_result[5,])[c(4,5,11)],as.vector(chintang2_duration_result[6,])[c(4,5,11)],as.vector(chintang2_duration_result[7,])[c(4,5,11)]),cbind(as.vector(dutch_duration_result[2,])[c(4,5,11)],as.vector(dutch_duration_result[3,])[c(4,5,11)],as.vector(dutch_duration_result[4,])[c(4,5,11)],as.vector(dutch_duration_result[5,])[c(4,5,11)],as.vector(dutch_duration_result[6,])[c(4,5,11)],as.vector(dutch_duration_result[7,])[c(4,5,11)]),cbind(as.vector(english_duration_result[2,])[c(4,5,11)],as.vector(english_duration_result[3,])[c(4,5,11)],as.vector(english_duration_result[4,])[c(4,5,11)],as.vector(english_duration_result[5,])[c(4,5,11)],as.vector(english_duration_result[6,])[c(4,5,11)],as.vector(english_duration_result[7,])[c(4,5,11)]),cbind(as.vector(even_duration_result[2,])[c(4,5,11)],as.vector(even_duration_result[3,])[c(4,5,11)],as.vector(even_duration_result[4,])[c(4,5,11)],as.vector(even_duration_result[5,])[c(4,5,11)],as.vector(even_duration_result[6,])[c(4,5,11)],as.vector(even_duration_result[7,])[c(4,5,11)]),cbind(as.vector(hoocak_duration_result[2,])[c(4,5,11)],as.vector(hoocak_duration_result[3,])[c(4,5,11)],as.vector(hoocak_duration_result[4,])[c(4,5,11)],as.vector(hoocak_duration_result[5,])[c(4,5,11)],as.vector(hoocak_duration_result[6,])[c(4,5,11)],as.vector(hoocak_duration_result[7,])[c(4,5,11)]),cbind(as.vector(nuu_duration_result[2,])[c(4,5,11)],as.vector(nuu_duration_result[3,])[c(4,5,11)],as.vector(nuu_duration_result[4,])[c(4,5,11)],as.vector(nuu_duration_result[5,])[c(4,5,11)],as.vector(nuu_duration_result[6,])[c(4,5,11)],as.vector(nuu_duration_result[7,])[c(4,5,11)]),cbind(as.vector(sakha_duration_result[2,])[c(4,5,11)],as.vector(sakha_duration_result[3,])[c(4,5,11)],as.vector(sakha_duration_result[4,])[c(4,5,11)],as.vector(sakha_duration_result[5,])[c(4,5,11)],as.vector(sakha_duration_result[6,])[c(4,5,11)],as.vector(sakha_duration_result[7,])[c(4,5,11)]),cbind(as.vector(texistepec_duration_result[2,])[c(4,5,11)],as.vector(texistepec_duration_result[3,])[c(4,5,11)],as.vector(texistepec_duration_result[4,])[c(4,5,11)],as.vector(texistepec_duration_result[5,])[c(4,5,11)],as.vector(texistepec_duration_result[6,])[c(4,5,11)],as.vector(texistepec_duration_result[7,])[c(4,5,11)]))

# Add sensible row names
rownames(duration_results) = c("Baure","Bora","Chintang","Chintang (no pear stories)","Dutch","English","Even","Hoocąk","Nǁng","Sakha","Texistepec")

# Output the table (in two parts)
options(knitr.kable.NA = '')
kable(duration_results[,1:9],escape=FALSE,format="latex",booktabs=TRUE,digits=4,row.names=TRUE,col.names=c("$\\beta$","$\\Delta R^2_{(m)}$\\%","","$\\beta$","$\\Delta R^2_{(m)}$\\%","","$\\beta$","$\\Delta R^2_{(m)}$\\%",""),caption="Comparison of coefficients of the multivariate models for the ten individual languages\\label{tab:comparison_of_coefficients}") %>% kable_styling(position="center",latex_options=c("scale_down","hold_position")) %>% add_header_above(c(" " = 1, "word frequency (log)" = 3, "word length" = 3, "number of morphemes" = 3))
```
\nopagebreak
```{r comparison_of_coefficients2, echo=FALSE}
kable(duration_results[,10:18],escape=FALSE,format="latex",booktabs=TRUE,digits=4,row.names=TRUE,col.names=c("$\\beta$","$\\Delta R^2_{(m)}$\\%","","$\\beta$","$\\Delta R^2_{(m)}$\\%","","$\\beta$","$\\Delta R^2_{(m)}$\\%","")) %>% kable_styling(position="center",latex_options=c("scale_down","hold_position")) %>% add_header_above(c(" " = 1, "position" = 3, "word class = verb (vs. noun)" = 3, "local speech rate (log)" = 3)) %>% footnote(general=c("Significance stars are based on likelihood ratio tests."),general_title="Notes:")
options(knitr.kable.NA = 'NA')
```


```{r plot_of_coefficients, echo=FALSE, fig.width=16, fig.height=11, fig.cap="Comparison of standardized β coefficients for the ten languages (with significance stars based on likelihood ratio tests)\\label{fig:coefficient_comparison}"}

# Create a long format table of the word duration results, specifically containing the standardized beta coefficients of all fixed factors in all linear mixed-effects models
colnames(duration_results) = c("word_freq_beta","word_freq_r2","word_freq_sig","word_length_beta","word_length_r2","word_length_sig","morphs_beta","morphs_r2","morphs_sig","position_beta","position_r2","position_sig","word_class_beta","word_class_r2","word_class_sig","speech_rate_beta","speech_rate_r2","speech_rate_sig")
duration_results$language = rownames(duration_results)
duration_results_molten = melt(duration_results,id="language",variable.name="variable",value.name="value")
duration_results_molten$value = as.numeric(duration_results_molten$value)
duration_results_molten_betas = subset(duration_results_molten,subset=grepl("beta",variable))
duration_results_molten_betas$variable = as.character(duration_results_molten_betas$variable)
duration_results_molten_betas$variable = rep(c("word frequency (log)","word length","number of morphemes","position","word class = verb","local speech rate (log)"),each=11)
duration_results_molten_betas$variable = factor(duration_results_molten_betas$variable,levels=c("word length","word frequency (log)","number of morphemes","position","word class = verb","local speech rate (log)"))
duration_results_molten_betas$stars = c(duration_results$word_freq_sig,duration_results$word_length_sig,duration_results$morphs_sig,duration_results$position_sig,duration_results$word_class_sig,duration_results$speech_rate_sig)

# Add an empty row for the number of morphemes in the Dutch corpus (which does not contain morphological annotation)
duration_results_molten_betas$stars[duration_results_molten_betas$language=="Dutch" & duration_results_molten_betas$variable=="number of morphemes"] = ""

# Output a plot with bars representing the magnitude and size of the standardized effect of the different fixed factors (word frequency, word length, number of morphemes, position, part-of-speech,
# articulation speed in the context
ggplot(duration_results_molten_betas,aes(variable,value)) + geom_bar(stat="identity",aes(fill=variable),color="black") + facet_wrap(~language) + ylab("standardized β coefficient") + scale_x_discrete(breaks=NULL) + scale_y_continuous(breaks=c(-0.4,-0.2,0,0.2,0.4,0.6)) + geom_hline(yintercept = 0) + labs(fill="explanatory variable") + theme(axis.title.x = element_blank(),legend.position = c(0.99, 0.11), legend.justification = c(1, 0)) + scale_fill_manual(values=cbPalette) + geom_text(aes(label = stars,vjust = ifelse(value >= 0, -0.8, 1.8)), color="black", size=5) + coord_cartesian(ylim=c(-0.35,0.77))
```


```{r effect_sizes, echo=FALSE}

# Create a table about the size of the word frequency effect in all languages
effect_sizes = data.frame("language" = c("Baure","Bora","Chintang","Chintang (no pear stories)","Dutch","English","Even","Hoocąk","Nǁng","Sakha","Texistepec"),"max_word_log_rel_freq" = NA, "max_word_duration" = NA,"min_word_log_rel_freq" = NA, "min_word_duration" = NA,"difference" = NA,"ratio" = NA, "percentage_change" = NA)

# Calculate the mininum and maximum word frequency (in the table with the 100 most frequent word types) and get predictions of average word duration at these two extremes using the R library effects
# based on the linear mixed-effects model for each language
# Combine the results in a cross-linguistic table

# Baure
baure.min = min(baure_100_tokens$word_log_rel_freq)
baure.max = max(baure_100_tokens$word_log_rel_freq)
frequency.effect.word_duration.extreme.baure = as.data.frame(effect("word_log_rel_freq",baure_duration_model,xlevels=list(word_log_rel_freq=c(baure.min,baure.max)),transformation=list(link=log, inverse=exp)))
frequency.effect.word_duration.extreme.baure$language = "Baure"
effect_sizes$min_word_log_rel_freq[effect_sizes$language=="Baure"] = baure.min
effect_sizes$min_word_duration[effect_sizes$language=="Baure"] = frequency.effect.word_duration.extreme.baure$fit[1]
effect_sizes$max_word_log_rel_freq[effect_sizes$language=="Baure"] = baure.max
effect_sizes$max_word_duration[effect_sizes$language=="Baure"] = frequency.effect.word_duration.extreme.baure$fit[2]

# Bora
bora.min = min(bora_100_tokens$word_log_rel_freq)
bora.max = max(bora_100_tokens$word_log_rel_freq)
frequency.effect.word_duration.extreme.bora = as.data.frame(effect("word_log_rel_freq",bora_duration_model,xlevel=list(word_log_rel_freq=c(bora.min,bora.max)),transformation=list(link=log, inverse=exp)))
frequency.effect.word_duration.extreme.bora$language = "Bora"
effect_sizes$min_word_log_rel_freq[effect_sizes$language=="Bora"] = bora.min
effect_sizes$min_word_duration[effect_sizes$language=="Bora"] = frequency.effect.word_duration.extreme.bora$fit[1]
effect_sizes$max_word_log_rel_freq[effect_sizes$language=="Bora"] = bora.max
effect_sizes$max_word_duration[effect_sizes$language=="Bora"] = frequency.effect.word_duration.extreme.bora$fit[2]

# Chintang
chintang.min = min(chintang_100_tokens$word_log_rel_freq)
chintang.max = max(chintang_100_tokens$word_log_rel_freq)
frequency.effect.word_duration.extreme.chintang = as.data.frame(effect("word_log_rel_freq",chintang_duration_model,xlevels=list(word_log_rel_freq=c(chintang.min,chintang.max)),transformation=list(link=log, inverse=exp)))
frequency.effect.word_duration.extreme.chintang$language = "Chintang"
effect_sizes$min_word_log_rel_freq[effect_sizes$language=="Chintang"] = chintang.min
effect_sizes$min_word_duration[effect_sizes$language=="Chintang"] = frequency.effect.word_duration.extreme.chintang$fit[1]
effect_sizes$max_word_log_rel_freq[effect_sizes$language=="Chintang"] = chintang.max
effect_sizes$max_word_duration[effect_sizes$language=="Chintang"] = frequency.effect.word_duration.extreme.chintang$fit[2]

# Chintang (without pear stories)
chintang2.min = min(chintang2_100_tokens$word_log_rel_freq)
chintang2.max = max(chintang2_100_tokens$word_log_rel_freq)
frequency.effect.word_duration.extreme.chintang2 = as.data.frame(effect("word_log_rel_freq",chintang2_duration_model,xlevels=list(word_log_rel_freq=c(chintang2.min,chintang2.max)),transformation=list(link=log, inverse=exp)))
frequency.effect.word_duration.extreme.chintang2$language = "Chintang (no pear stories)"
effect_sizes$min_word_log_rel_freq[effect_sizes$language=="Chintang (no pear stories)"] = chintang2.min
effect_sizes$min_word_duration[effect_sizes$language=="Chintang (no pear stories)"] = frequency.effect.word_duration.extreme.chintang2$fit[1]
effect_sizes$max_word_log_rel_freq[effect_sizes$language=="Chintang (no pear stories)"] = chintang2.max
effect_sizes$max_word_duration[effect_sizes$language=="Chintang (no pear stories)"] = frequency.effect.word_duration.extreme.chintang2$fit[2]

# Dutch
dutch.min = min(dutch_100_tokens$word_log_rel_freq)
dutch.max = max(dutch_100_tokens$word_log_rel_freq)
frequency.effect.word_duration.extreme.dutch = as.data.frame(effect("word_log_rel_freq",dutch_duration_model,xlevels=list(word_log_rel_freq=c(dutch.min,dutch.max)),transformation=list(link=log, inverse=exp)))
frequency.effect.word_duration.extreme.dutch$language = "Dutch"
effect_sizes$min_word_log_rel_freq[effect_sizes$language=="Dutch"] = dutch.min
effect_sizes$min_word_duration[effect_sizes$language=="Dutch"] = frequency.effect.word_duration.extreme.dutch$fit[1]
effect_sizes$max_word_log_rel_freq[effect_sizes$language=="Dutch"] = dutch.max
effect_sizes$max_word_duration[effect_sizes$language=="Dutch"] = frequency.effect.word_duration.extreme.dutch$fit[2]

# English
english.min = min(english_100_tokens$word_log_rel_freq)
english.max = max(english_100_tokens$word_log_rel_freq)
frequency.effect.word_duration.extreme.english = as.data.frame(effect("word_log_rel_freq",english_duration_model,xlevels=list(word_log_rel_freq=c(english.min,english.max)),transformation=list(link=log, inverse=exp)))
frequency.effect.word_duration.extreme.english$language = "English"
effect_sizes$min_word_log_rel_freq[effect_sizes$language=="English"] = english.min
effect_sizes$min_word_duration[effect_sizes$language=="English"] = frequency.effect.word_duration.extreme.english$fit[1]
effect_sizes$max_word_log_rel_freq[effect_sizes$language=="English"] = english.max
effect_sizes$max_word_duration[effect_sizes$language=="English"] = frequency.effect.word_duration.extreme.english$fit[2]

# Even
even.min = min(even_100_tokens$word_log_rel_freq)
even.max = max(even_100_tokens$word_log_rel_freq)
frequency.effect.word_duration.extreme.even = as.data.frame(effect("word_log_rel_freq",even_duration_model,xlevels=list(word_log_rel_freq=c(even.min,even.max)),transformation=list(link=log, inverse=exp)))
frequency.effect.word_duration.extreme.even$language = "Even"
effect_sizes$min_word_log_rel_freq[effect_sizes$language=="Even"] = even.min
effect_sizes$min_word_duration[effect_sizes$language=="Even"] = frequency.effect.word_duration.extreme.even$fit[1]
effect_sizes$max_word_log_rel_freq[effect_sizes$language=="Even"] = even.max
effect_sizes$max_word_duration[effect_sizes$language=="Even"] = frequency.effect.word_duration.extreme.even$fit[2]

# Hoocąk
hoocak.min = min(hoocak_100_tokens$word_log_rel_freq)
hoocak.max = max(hoocak_100_tokens$word_log_rel_freq)
frequency.effect.word_duration.extreme.hoocak = as.data.frame(effect("word_log_rel_freq",hoocak_duration_model,xlevels=list(word_log_rel_freq=c(hoocak.min,hoocak.max)),transformation=list(link=log, inverse=exp)))
frequency.effect.word_duration.extreme.hoocak$language = "Hoocąk"
effect_sizes$min_word_log_rel_freq[effect_sizes$language=="Hoocąk"] = hoocak.min
effect_sizes$min_word_duration[effect_sizes$language=="Hoocąk"] = frequency.effect.word_duration.extreme.hoocak$fit[1]
effect_sizes$max_word_log_rel_freq[effect_sizes$language=="Hoocąk"] = hoocak.max
effect_sizes$max_word_duration[effect_sizes$language=="Hoocąk"] = frequency.effect.word_duration.extreme.hoocak$fit[2]

# Nǁng
nuu.min = min(nuu_100_tokens$word_log_rel_freq)
nuu.max = max(nuu_100_tokens$word_log_rel_freq)
frequency.effect.word_duration.extreme.nuu = as.data.frame(effect("word_log_rel_freq",nuu_duration_model,xlevels=list(word_log_rel_freq=c(nuu.min,nuu.max)),transformation=list(link=log, inverse=exp)))
frequency.effect.word_duration.extreme.nuu$language = "Nǁng"
effect_sizes$min_word_log_rel_freq[effect_sizes$language=="Nǁng"] = nuu.min
effect_sizes$min_word_duration[effect_sizes$language=="Nǁng"] = frequency.effect.word_duration.extreme.nuu$fit[1]
effect_sizes$max_word_log_rel_freq[effect_sizes$language=="Nǁng"] = nuu.max
effect_sizes$max_word_duration[effect_sizes$language=="Nǁng"] = frequency.effect.word_duration.extreme.nuu$fit[2]

# Sakha
sakha.min = min(sakha_100_tokens$word_log_rel_freq)
sakha.max = max(sakha_100_tokens$word_log_rel_freq)
frequency.effect.word_duration.extreme.sakha = as.data.frame(effect("word_log_rel_freq",sakha_duration_model,xlevels=list(word_log_rel_freq=c(sakha.min,sakha.max)),transformation=list(link=log, inverse=exp)))
frequency.effect.word_duration.extreme.sakha$language = "Sakha"
effect_sizes$min_word_log_rel_freq[effect_sizes$language=="Sakha"] = sakha.min
effect_sizes$min_word_duration[effect_sizes$language=="Sakha"] = frequency.effect.word_duration.extreme.sakha$fit[1]
effect_sizes$max_word_log_rel_freq[effect_sizes$language=="Sakha"] = sakha.max
effect_sizes$max_word_duration[effect_sizes$language=="Sakha"] = frequency.effect.word_duration.extreme.sakha$fit[2]

# Texistepec Popoluca
texistepec.min = min(texistepec_100_tokens$word_log_rel_freq)
texistepec.max = max(texistepec_100_tokens$word_log_rel_freq)
frequency.effect.word_duration.extreme.texistepec = as.data.frame(effect("word_log_rel_freq",texistepec_duration_model,xlevels=list(word_log_rel_freq=c(texistepec.min,texistepec.max)),transformation=list(link=log, inverse=exp)))
frequency.effect.word_duration.extreme.texistepec$language = "Texistepec"
effect_sizes$min_word_log_rel_freq[effect_sizes$language=="Texistepec"] = texistepec.min
effect_sizes$min_word_duration[effect_sizes$language=="Texistepec"] = frequency.effect.word_duration.extreme.texistepec$fit[1]
effect_sizes$max_word_log_rel_freq[effect_sizes$language=="Texistepec"] = texistepec.max
effect_sizes$max_word_duration[effect_sizes$language=="Texistepec"] = frequency.effect.word_duration.extreme.texistepec$fit[2]

# Combine the different tables into one
frequency.effect.word_duration.extreme = rbind(frequency.effect.word_duration.extreme.baure,frequency.effect.word_duration.extreme.bora,frequency.effect.word_duration.extreme.chintang,frequency.effect.word_duration.extreme.chintang2,frequency.effect.word_duration.extreme.dutch,frequency.effect.word_duration.extreme.english,frequency.effect.word_duration.extreme.even,frequency.effect.word_duration.extreme.hoocak,frequency.effect.word_duration.extreme.nuu,frequency.effect.word_duration.extreme.sakha,frequency.effect.word_duration.extreme.texistepec)

# Calculate relative frequency on normal scale from log relative frequency
frequency.effect.word_duration.extreme$word_rel_freq = exp(frequency.effect.word_duration.extreme$word_log_rel_freq)
frequency.effect.word_duration.extreme$word_rel_freq = exp(frequency.effect.word_duration.extreme$word_log_rel_freq)

effect_sizes$min_word_log_rel_freq = exp(effect_sizes$min_word_log_rel_freq)
effect_sizes$max_word_log_rel_freq = exp(effect_sizes$max_word_log_rel_freq)

# Calculate the difference in predicted word durations for the most frequent and the least frequent word for each language
effect_sizes$difference = effect_sizes$max_word_duration - effect_sizes$min_word_duration

# Calculate the ratio between the predicted word durations for the most frequent and the least frequent word for each language
effect_sizes$ratio = effect_sizes$max_word_duration / effect_sizes$min_word_duration

# Calculate percentage change in the predicted word duration going from the least frequent to the most frequent word type for each language
effect_sizes$percentage_change = (effect_sizes$max_word_duration - effect_sizes$min_word_duration)*100 / effect_sizes$max_word_duration

# Output a table with effect sizes for the word frequency effect in each language
kable(effect_sizes,digits=4,col.names=c("language","word frequency","word duration","word frequency","word duration","difference","ratio","percentage change"),caption="Size of word frequency effect on word duration \\label{tab:effect_sizes}",escape=FALSE,format="latex",booktabs=TRUE) %>% kable_styling(position="center",latex_options=c("scale_down","hold_position")) %>% add_header_above(c(" " = 1, "most frequent word (1st)" = 2, "least frequent word (100th)" = 2, "word duration" = 3))
```


\appendix
\appendixpage
\addappheadtotoc

# Basic corpus statistics

```{r basic_corpus_statistics, echo=FALSE}

# Output a table about basic corpus characteristics for the ten languages
languages = data.frame(language=c("Baure","Bora","Chintang","Dutch","English","Even","Hoocąk","Nǁng","Sakha","Texistepec"),iso_code=c("brg","boa","ctn","nld","eng","eve","win","ngh","sah","poq"),glottocode=c("baur1253","bora1263","chhi1245","dutc1256","stan1293","lamu1253","hoch1243","nuuu1241","yaku1245","texi1237"),family=c("Arawakan","Boran","Sino-Tibetan","Indo-European","Indo-European","Tungusic","Siouan","ǃUi-Taa","Turkic","Mixe-Zoquean"),area=c("Amazonia","Amazonia","Himalayas","Europe","North America","Siberia","North America","Southern Africa","Siberia","Central America"),speakers=as.vector(by(combined.wordtimes.words$ELANParticipant,INDICES=list(combined.wordtimes.words$language),FUN=function (x) length(unique(x)))),texts=as.vector(by(combined.wordtimes.words$ntvr_file,INDICES=list(combined.wordtimes.words$language),FUN=function (x) length(unique(x)))),words=as.vector(by(combined.wordtimes.words$word.id,INDICES=list(combined.wordtimes.words$language),FUN=function (x) length(unique(x)))),reference=c("\\citet{Danielsen2009}","\\citet{Seifart2009}","\\citet{Bickel2011}","\\citet{CGN2003}","\\citet{Calhoun2009}","\\citet{Pakendorf2010}","\\citet{Hartmann2013}","\\citet{Gueldemann2011}","","\\citet{Wichmann1996}"))

kable(languages,col.names = c("language","ISO 639-3 code","glottocode","family","area","speakers","texts","words","reference"),format="latex",escape=FALSE,booktabs=TRUE,caption="Basic information about languages and corpora\\label{tab:corpora_table}",format.args=list(big.mark=" ")) %>% kable_styling(full_width=FALSE,position="center",latex_options=c("scale_down","hold_position")) %>% footnote(general=c("The corpus of Even documents the Lamunkhin variety of Even.","The Texistepec language is also sometimes ambiguously referred to as (Texistepec) Popoluca.","Unambiguous	language codes are provided with ISO 639-3/Ethnologue \\\\citep{Simons2018} and Glottolog \\\\citep{Hammarstroem2018} standards."),general_title="Notes:",escape=FALSE) %>% add_header_above(c(" " = 1, "language information" = 4, "corpus information" = 4))
```

# Lists of the 100 most frequent word types in the ten languages {#word_lists}

```{r baure_frequency_list, echo=FALSE}

# Output a table with the 100 most frequent (non-excluded) word types in Baure together with some statistics about them
kable(baure_100[,c(-6,-7,-9,-11)],col.names=c("word type","gloss","word class","abs. freq.","rel. freq.","rank","doc. freq.","mean duration","std. dev.","mean art. rate","std. dev."),caption="The 100 most frequent word types in Baure\\label{tab:baure_100_list}",digits=4,row.names=TRUE,format="latex",booktabs=TRUE,longtable=TRUE) %>% kable_styling(position="center",font_size=7,latex_options=c("scale_down","hold_position","repeat_header")) %>% landscape()
```

```{r bora_frequency_list, echo=FALSE}

# Output a table with the 100 most frequent (non-excluded) word types in Bora together with some statistics about them
kable(bora_100[,c(-6,-7,-9,-11)],col.names=c("word type","gloss","word class","abs. freq.","rel. freq.","rank","doc. freq.","mean duration","std. dev.","mean art. rate","std. dev."),caption="The 100 most frequent word types in Bora",digits=4,row.names=TRUE,format="latex",booktabs=TRUE,longtable=TRUE) %>% kable_styling(position="center",font_size=7,latex_options=c("scale_down","hold_position","repeat_header")) %>% landscape()
```

```{r chintang_frequency_list, echo=FALSE}

# Output a table with the 100 most frequent (non-excluded) word types in Chintang together with some statistics about them
kable(chintang_100[,c(-6,-7,-9,-11)],col.names=c("word type","gloss","word class","abs. freq.","rel. freq.","rank","doc. freq.","mean duration","std. dev.","mean art. rate","std. dev."),caption="The 100 most frequent word types in Chintang",digits=4,row.names=TRUE,format="latex",booktabs=TRUE,longtable=TRUE) %>% kable_styling(position="center",font_size=7,latex_options=c("scale_down","hold_position","repeat_header")) %>% landscape()
```

```{r chintang2_frequency_list, echo=FALSE}

# Output a table with the 100 most frequent (non-excluded) word types in Chintang (without pear stories) together with some statistics about them
kable(chintang2_100[,c(-6,-7,-9,-11)],col.names=c("word type","gloss","word class","abs. freq.","rel. freq.","rank","doc. freq.","mean duration","std. dev.","mean art. rate","std. dev."),caption="The 100 most frequent word types in Chintang (no pear stories)",digits=4,row.names=TRUE,format="latex",booktabs=TRUE,longtable=TRUE) %>% kable_styling(position="center",font_size=7,latex_options=c("scale_down","hold_position","repeat_header")) %>% landscape()
```

```{r dutch_frequency_list, echo=FALSE, size="tiny"}

# Output a table with the 100 most frequent (non-excluded) word types in Dutch together with some statistics about them
kable(dutch_100[,c(-6,-7,-9,-11)],col.names=c("word type","gloss","word class","abs. freq.","rel. freq.","rank","doc. freq.","mean duration","std. dev.","mean art. rate","std. dev."),caption="The 100 most frequent word types in Dutch",digits=4,row.names=TRUE,format="latex",booktabs=TRUE,longtable=TRUE) %>% kable_styling(position="center",font_size=7,latex_options=c("scale_down","hold_position","repeat_header")) %>% landscape()
```

```{r english_frequency_list, echo=FALSE, size="tiny"}

# Output a table with the 100 most frequent (non-excluded) word types in English together with some statistics about them
kable(english_100[,c(-6,-7,-9,-11)],col.names=c("word type","gloss","word class","abs. freq.","rel. freq.","rank","doc. freq.","mean duration","std. dev.","mean art. rate","std. dev."),caption="The 100 most frequent word types in English",digits=4,row.names=TRUE,format="latex",booktabs=TRUE,longtable=TRUE) %>% kable_styling(position="center",font_size=7,latex_options=c("scale_down","hold_position","repeat_header")) %>% landscape()
```

```{r even_frequency_list, echo=FALSE, size="tiny"}

# Output a table with the 100 most frequent (non-excluded) word types in Even together with some statistics about them
kable(even_100[,c(-6,-7,-9,-11)],col.names=c("word type","gloss","word class","abs. freq.","rel. freq.","rank","doc. freq.","mean duration","std. dev.","mean art. rate","std. dev."),caption="The 100 most frequent word types in Even",digits=4,row.names=TRUE,format="latex",booktabs=TRUE,longtable=TRUE) %>% kable_styling(position="center",font_size=7,latex_options=c("scale_down","hold_position","repeat_header")) %>% landscape()
```

```{r hoocak_frequency_list, echo=FALSE, size="tiny"}

# Output a table with the 100 most frequent (non-excluded) word types in Hoocąk together with some statistics about them
kable(hoocak_100[,c(-6,-7,-9,-11)],col.names=c("word type","gloss","word class","abs. freq.","rel. freq.","rank","doc. freq.","mean duration","std. dev.","mean art. rate","std. dev."),caption="The 100 most frequent word types in Hoocąk",digits=4,row.names=TRUE,format="latex",booktabs=TRUE,longtable=TRUE) %>% kable_styling(position="center",font_size=7,latex_options=c("scale_down","hold_position","repeat_header")) %>% landscape()
```

```{r nuu_frequency_list, echo=FALSE, size="tiny"}

# Output a table with the 100 most frequent (non-excluded) word types in Nǁng together with some statistics about them
kable(nuu_100[,c(-6,-7,-9,-11)],col.names=c("word type","gloss","word class","abs. freq.","rel. freq.","rank","doc. freq.","mean duration","std. dev.","mean art. rate","std. dev."),caption="The 100 most frequent word types in Nǁng",digits=4,row.names=TRUE,format="latex",booktabs=TRUE,longtable=TRUE) %>% kable_styling(position="center",font_size=7,latex_options=c("scale_down","hold_position","repeat_header")) %>% landscape()
```

```{r sakha_frequency_list, echo=FALSE, size="tiny"}

# Output a table with the 100 most frequent (non-excluded) word types in Sakha together with some statistics about them
kable(sakha_100[,c(-6,-7,-9,-11)],col.names=c("word type","gloss","word class","abs. freq.","rel. freq.","rank","doc. freq.","mean duration","std. dev.","mean art. rate","std. dev."),caption="The 100 most frequent word types in Sakha",digits=4,row.names=TRUE,format="latex",booktabs=TRUE,longtable=TRUE) %>% kable_styling(position="center",font_size=7,latex_options=c("scale_down","hold_position","repeat_header")) %>% landscape()
```

```{r texistepec_frequency_list, echo=FALSE, size="tiny"}

# Output a table with the 100 most frequent (non-excluded) word types in Texistepec Popoluca together with some statistics about them
kable(texistepec_100[,c(-6,-7,-9,-11)],col.names=c("word type","gloss","word class","abs. freq.","rel. freq.","rank","doc. freq.","mean duration","std. dev.","mean art. rate","std. dev."),caption="The 100 most frequent word types in Texistepec",digits=4,row.names=TRUE,format="latex",booktabs=TRUE,longtable=TRUE) %>% kable_styling(position="center",font_size=7,latex_options=c("scale_down","hold_position","repeat_header")) %>% landscape()
```


# Lists of excluded word types in the ten languages {#excluded_lists}

```{r baure_excluded_list, echo=FALSE}

# Output a table with those word types excluded from the list of the 100 most frequent word types in Baure together with some statistics about them
kable(baure_excluded,col.names=c("word type","gloss","word class","abs. freq","rel. freq.","doc. freq.","mean duration","std. dev.","mean art. rate","std. dev."),caption="Excluded word types in Baure\\label{tab:baure_excluded_list}",digits=4,row.names=TRUE,format="latex",booktabs=TRUE,longtable=TRUE) %>% kable_styling(position="center",font_size=7,latex_options =c("scale_down","hold_position","repeat_header")) %>% landscape()
```

```{r bora_excluded_list, echo=FALSE}

# Output a table with those word types excluded from the list of the 100 most frequent word types in Bora together with some statistics about them
kable(bora_excluded,col.names=c("word type","gloss","word class","abs. freq","rel. freq.","doc. freq.","mean duration","std. dev.","mean art. rate","std. dev."),caption="Excluded word types in Bora\\label{tab:bora_excluded_list}",digits=4,row.names=TRUE,format="latex",booktabs=TRUE,longtable=TRUE) %>% kable_styling(position="center",font_size=7,latex_options =c("scale_down","hold_position","repeat_header")) %>% landscape()
```

```{r chintang_excluded_list, echo=FALSE}

# Output a table with those word types excluded from the list of the 100 most frequent word types in Chintang together with some statistics about them
kable(chintang_excluded,col.names=c("word type","gloss","word class","abs. freq","rel. freq.","doc. freq.","mean duration","std. dev.","mean art. rate","std. dev."),caption="Excluded word types in Chintang\\label{tab:chintang_excluded_list}",digits=4,row.names=TRUE,format="latex",booktabs=TRUE,longtable=TRUE) %>% kable_styling(position="center",font_size=7,latex_options =c("scale_down","hold_position","repeat_header")) %>% landscape()
```

```{r chintang2_excluded_list, echo=FALSE}

# Output a table with those word types excluded from the list of the 100 most frequent word types in Chintang (without pear stories) together with some statistics about them
kable(chintang2_excluded,col.names=c("word type","gloss","word class","abs. freq","rel. freq.","doc. freq.","mean duration","std. dev.","mean art. rate","std. dev."),caption="Excluded word types in Chintang (no pear stories)\\label{tab:chintang2_excluded_list}",digits=4,row.names=TRUE,format="latex",booktabs=TRUE,longtable=TRUE) %>% kable_styling(position="center",font_size=7,latex_options =c("scale_down","hold_position","repeat_header")) %>% landscape()
```

```{r dutch_excluded_list, echo=FALSE}

# Output a table with those word types excluded from the list of the 100 most frequent word types in Dutch together with some statistics about them
kable(dutch_excluded,col.names=c("word type","gloss","word class","abs. freq","rel. freq.","doc. freq.","mean duration","std. dev.","mean art. rate","std. dev."),caption="Excluded word types in Dutch\\label{tab:dutch_excluded_list}",digits=4,row.names=TRUE,format="latex",booktabs=TRUE,longtable=TRUE) %>% kable_styling(position="center",font_size=7,latex_options =c("scale_down","hold_position","repeat_header")) %>% landscape()
```

```{r english_excluded_list, echo=FALSE}

# Output a table with those word types excluded from the list of the 100 most frequent word types in English together with some statistics about them
kable(english_excluded,col.names=c("word type","gloss","word class","abs. freq","rel. freq.","doc. freq.","mean duration","std. dev.","mean art. rate","std. dev."),caption="Excluded word types in English\\label{tab:english_excluded_list}",digits=4,row.names=TRUE,format="latex",booktabs=TRUE,longtable=TRUE) %>% kable_styling(position="center",font_size=7,latex_options =c("scale_down","hold_position","repeat_header")) %>% landscape()
```

```{r even_excluded_list, echo=FALSE}

# Output a table with those word types excluded from the list of the 100 most frequent word types in Even together with some statistics about them
kable(even_excluded,col.names=c("word type","gloss","word class","abs. freq","rel. freq.","doc. freq.","mean duration","std. dev.","mean art. rate","std. dev."),caption="Excluded word types in Even\\label{tab:even_excluded_list}",digits=4,row.names=TRUE,format="latex",booktabs=TRUE,longtable=TRUE) %>% kable_styling(position="center",font_size=7,latex_options =c("scale_down","hold_position","repeat_header")) %>% landscape()
```

```{r hoocak_excluded_list, echo=FALSE}

# Output a table with those word types excluded from the list of the 100 most frequent word types in Hoocąk together with some statistics about them
kable(hoocak_excluded,col.names=c("word type","gloss","word class","abs. freq","rel. freq.","doc. freq.","mean duration","std. dev.","mean art. rate","std. dev."),caption="Excluded word types in Hoocąk\\label{tab:hoocak_excluded_list}",digits=4,row.names=TRUE,format="latex",booktabs=TRUE,longtable=TRUE) %>% kable_styling(position="center",font_size=7,latex_options =c("scale_down","hold_position","repeat_header")) %>% landscape()
```

There were no excluded auxiliaries or other, specifically excluded words in Nǁng.
<!-- No excluded words for Nǁng
```{r nuu_excluded_list, echo=FALSE}
kable(baure_excluded,col.names=c("word type","gloss","word class","abs. freq","rel. freq.","doc. freq.","mean duration","std. dev.","mean art. rate","std. dev."),caption="Excluded word types in Baure\\label{tab:baure_excluded_list}",digits=4,row.names=TRUE,format="latex",booktabs=TRUE,longtable=TRUE) %>% kable_styling(position="center",font_size=7,latex_options =c("scale_down","hold_position","repeat_header")) %>% landscape()
```
-->

```{r sakha_excluded_list, echo=FALSE}

# Output a table with those word types excluded from the list of the 100 most frequent word types in Sakha together with some statistics about them
kable(sakha_excluded,col.names=c("word type","gloss","word class","abs. freq","rel. freq.","doc. freq.","mean duration","std. dev.","mean art. rate","std. dev."),caption="Excluded word types in Sakha\\label{tab:sakha_excluded_list}",digits=4,row.names=TRUE,format="latex",booktabs=TRUE,longtable=TRUE) %>% kable_styling(position="center",font_size=7,latex_options =c("scale_down","hold_position","repeat_header")) %>% landscape()
```

```{r texistepec_excluded_list, echo=FALSE}

# Output a table with those word types excluded from the list of the 100 most frequent word types in Texistepec Popoluca together with some statistics about them
kable(texistepec_excluded,col.names=c("word type","gloss","word class","abs. freq","rel. freq.","doc. freq.","mean duration","std. dev.","mean art. rate","std. dev."),caption="Excluded word types in Texistepec\\label{tab:texistepec_excluded_list}",digits=4,row.names=TRUE,format="latex",booktabs=TRUE,longtable=TRUE) %>% kable_styling(position="center",font_size=7,latex_options =c("scale_down","hold_position","repeat_header")) %>% landscape()
```

# Distribution of word frequencies (Zipf plots) {#zipf_plots}

```{r zipf_plots, echo=FALSE, fig.cap="Rank vs. frequency for content words in the ten languages\\label{fig:zipf_plots}"}

# Create a facetted Zipf plot for all languages, plotting rank vs. frequency
ggplot(all_100_types,aes(x=word_freq_rank2,y=word_freq)) + geom_point() + geom_line() + ylab("Absolute Frequency") + coord_cartesian(xlim=c(1,100)) + xlab("Rank") + facet_wrap(~ language, scales="free") + theme(legend.title=element_text(size=15),legend.text=element_text(size=15),axis.text=element_text(size=12),axis.title=element_text(size=15),plot.title=element_text(size=17))
```


# Bivariate rank correlation between word frequency and word duration {#bivariate_correlations}

```{r summary_word_duration, echo=FALSE}

# Output a table about the results of bivariate correlation tests using Spearman's rank correlation coefficient between word frequency and word duration
kable(results.word_duration.cast,col.names=c("language","$\\rho$","$S$","$p$",""),caption="Summary of results on bivariate correlations between word frequency and word duration",digits=4,row.names=FALSE)
```


\pagebreak

# Details on the individual mixed-effects models for the ten languages {#individual_models}

## Baure {-}

```{r baure_model, echo=FALSE}

# Output a table of fixed effects for the Baure linear mixed-effects model (including columns for standard error, t values, the standardized beta coefficients, percentage change
# in marginal and conditional R², respectively, as well semi-partial R² and the results of a log-likelihood ratio test comparing models with and without the factor in question)
options(knitr.kable.NA = '')
kable(baure_duration_result,digits=4,row.names=TRUE,col.names=c("Coefficient","Std. Error","$t$","$\\beta$","$\\Delta R^2_{(m)}$\\%","$\\Delta R^2_{(c)}$\\%","s.-p. $R^2$","$\\chi^2$","$df$","$p$",""),escape=FALSE,caption="Summary of fixed effects of the linear mixed-effects model of word duration in Baure\\label{tab:baure_duration_fixed_effects}",format="latex",booktabs=TRUE) %>% kable_styling(position="center",latex_options=c("scale_down","hold_position")) %>% footnote(general=paste("n = ", format(summary(baure_duration_model)$devcomp$dims[1],big.mark=" "), ", $R^2_{(m)}$ = ",round(r.squaredGLMM(baure_duration_model)[1],4), ", $R^2_{(c)}$ = ",round(r.squaredGLMM(baure_duration_model)[2],4),sep=""),general_title="",escape=FALSE)
options(knitr.kable.NA = 'NA')

# Output a table of random effects for the Baure linear mixed-effects model (including columns for the number of groups, standard deviation, percentage change in marginal and conditional
# R², respectively, as well as the results of a log-likelihood test comparing models with and without the random effect in question)
options(knitr.kable.NA = '')
kable(baure_duration_random,digits=4,row.names=FALSE,col.names=c("Random effect","Groups","Std. Dev.","$\\Delta R^2_{(m)}$\\%","$\\Delta R^2_{(c)}$\\%","$\\chi^2$","$df$","$p$",""),escape=FALSE,caption="Summary of random effects of the linear mixed-effects model of word duration in Baure\\label{tab:baure_duration_random_effects}",format="latex",booktabs=TRUE) %>% kable_styling(position="center",latex_options=c("hold_position"))
options(knitr.kable.NA = 'NA')
```

## Bora {-}

```{r bora_model, echo=FALSE}

# Output a table of fixed effects for the Bora linear mixed-effects model (including columns for standard error, t values, the standardized beta coefficients, percentage change
# in marginal and conditional R², respectively, as well semi-partial R² and the results of a log-likelihood ratio test comparing models with and without the factor in question)
options(knitr.kable.NA = '')
kable(bora_duration_result,digits=4,row.names=TRUE,col.names=c("Coefficient","Std. Error","$t$","$\\beta$","$\\Delta R^2_{(m)}$\\%","$\\Delta R^2_{(c)}$\\%","s.-p. $R^2$","$\\chi^2$","$df$","$p$",""),escape=FALSE,caption="Summary of fixed effects of the linear mixed-effects model of word duration in Bora\\label{tab:bora_duration_fixed_effects}",format="latex",booktabs=TRUE) %>% kable_styling(position="center",latex_options=c("scale_down","hold_position")) %>% footnote(general=paste("n = ", format(summary(bora_duration_model)$devcomp$dims[1],big.mark=" "), ", $R^2_{(m)}$ = ",round(r.squaredGLMM(bora_duration_model)[1],4), ", $R^2_{(c)}$ = ",round(r.squaredGLMM(bora_duration_model)[2],4),sep=""),general_title="",escape=FALSE)
options(knitr.kable.NA = 'NA')

# Output a table of random effects for the Bora linear mixed-effects model (including columns for the number of groups, standard deviation, percentage change in marginal and conditional
# R², respectively, as well as the results of a log-likelihood test comparing models with and without the random effect in question)
options(knitr.kable.NA = '')
kable(bora_duration_random,digits=4,row.names=FALSE,col.names=c("Random effect","Groups","Std. Dev.","$\\Delta R^2_{(m)}$%","$\\Delta R^2_{(c)}$%","$\\chi^2$","$df$","$p$",""),caption="Summary of random effects of the linear mixed-effects model of word duration in Bora\\label{tab:bora_duration_random_effects}")
options(knitr.kable.NA = 'NA')
```

\pagebreak

## Chintang {-}

```{r chintang_model, echo=FALSE}

# Output a table of fixed effects for the Chintang linear mixed-effects model (including columns for standard error, t values, the standardized beta coefficients, percentage change
# in marginal and conditional R², respectively, as well semi-partial R² and the results of a log-likelihood ratio test comparing models with and without the factor in question)
options(knitr.kable.NA = '')
kable(chintang_duration_result,digits=4,row.names=TRUE,col.names=c("Coefficient","Std. Error","$t$","$\\beta$","$\\Delta R^2_{(m)}$\\%","$\\Delta R^2_{(c)}$\\%","s.-p. $R^2$","$\\chi^2$","$df$","$p$",""),escape=FALSE,caption="Summary of fixed effects of the linear mixed-effects model of word duration in Chintang\\label{tab:chintang_duration_fixed_effects}",format="latex",booktabs=TRUE) %>% kable_styling(position="center",latex_options=c("scale_down","hold_position")) %>% footnote(general=paste("n = ", format(summary(chintang_duration_model)$devcomp$dims[1],big.mark=" "), ", $R^2_{(m)}$ = ",round(r.squaredGLMM(chintang_duration_model)[1],4), ", $R^2_{(c)}$ = ",round(r.squaredGLMM(chintang_duration_model)[2],4),sep=""),general_title="",escape=FALSE)
options(knitr.kable.NA = 'NA')

# Output a table of random effects for the Chintang linear mixed-effects model (including columns for the number of groups, standard deviation, percentage change in marginal and conditional
# R², respectively, as well as the results of a log-likelihood test comparing models with and without the random effect in question)
options(knitr.kable.NA = '')
kable(chintang_duration_random,digits=4,row.names=FALSE,col.names=c("Random effect","Groups","Std. Dev.","$\\Delta R^2_{(m)}$%","$\\Delta R^2_{(c)}$%","$\\chi^2$","$df$","$p$",""),caption="Summary of random effects of the linear mixed-effects model of word duration in Chintang\\label{tab:chintang_duration_random_effects}")
options(knitr.kable.NA = 'NA')
```

## Chintang (no pear stories) {-}

```{r chintang2_model, echo=FALSE}

# Output a table of fixed effects for the linear mixed-effects model for Chintang without pear stories (including columns for standard error, t values, the standardized beta coefficients, percentage change
# in marginal and conditional R², respectively, as well semi-partial R² and the results of a log-likelihood ratio test comparing models with and without the factor in question)
options(knitr.kable.NA = '')
kable(chintang2_duration_result,digits=4,row.names=TRUE,col.names=c("Coefficient","Std. Error","$t$","$\\beta$","$\\Delta R^2_{(m)}$\\%","$\\Delta R^2_{(c)}$\\%","s.-p. $R^2$","$\\chi^2$","$df$","$p$",""),escape=FALSE,caption="Summary of fixed effects of the linear mixed-effects model of word duration in Chintang (no pear stories)\\label{tab:chintang2_duration_fixed_effects}",format="latex",booktabs=TRUE) %>% kable_styling(position="center",latex_options=c("scale_down","hold_position")) %>% footnote(general=paste("n = ", format(summary(chintang2_duration_model)$devcomp$dims[1],big.mark=" "), ", $R^2_{(m)}$ = ",round(r.squaredGLMM(chintang2_duration_model)[1],4), ", $R^2_{(c)}$ = ",round(r.squaredGLMM(chintang2_duration_model)[2],4),sep=""),general_title="",escape=FALSE)
options(knitr.kable.NA = 'NA')

# Output a table of random effects for the linear mixed-effects model for Chintang without pear stories (including columns for the number of groups, standard deviation, percentage change in marginal and conditional
# R², respectively, as well as the results of a log-likelihood test comparing models with and without the random effect in question)
options(knitr.kable.NA = '')
kable(chintang2_duration_random,digits=4,row.names=FALSE,col.names=c("Random effect","Groups","Std. Dev.","$\\Delta R^2_{(m)}$%","$\\Delta R^2_{(c)}$%","$\\chi^2$","$df$","$p$",""),caption="Summary of random effects of the linear mixed-effects model of word duration in Chintang (no pear stories)\\label{tab:chintang2_duration_random_effects}")
options(knitr.kable.NA = 'NA')
```

\pagebreak

## Dutch {-}

```{r dutch_model, echo=FALSE}

# Output a table of fixed effects for the Dutch linear mixed-effects model (including columns for standard error, t values, the standardized beta coefficients, percentage change
# in marginal and conditional R², respectively, as well semi-partial R² and the results of a log-likelihood ratio test comparing models with and without the factor in question)
options(knitr.kable.NA = '')
kable(dutch_duration_result[-4,],digits=4,row.names=TRUE,col.names=c("Coefficient","Std. Error","$t$","$\\beta$","$\\Delta R^2_{(m)}$\\%","$\\Delta R^2_{(c)}$\\%","s.-p. $R^2$","$\\chi^2$","$df$","$p$",""),escape=FALSE,caption="Summary of fixed effects of the linear mixed-effects model of word duration in Dutch\\label{tab:dutch_duration_fixed_effects}",format="latex",booktabs=TRUE) %>% kable_styling(position="center",latex_options=c("scale_down","hold_position")) %>% footnote(general=paste("n = ", format(summary(dutch_duration_model)$devcomp$dims[1],big.mark=" "), ", $R^2_{(m)}$ = ",round(r.squaredGLMM(dutch_duration_model)[1],4), ", $R^2_{(c)}$ = ",round(r.squaredGLMM(dutch_duration_model)[2],4),sep=""),general_title="",escape=FALSE)
options(knitr.kable.NA = 'NA')

# Output a table of random effects for the Dutch linear mixed-effects model (including columns for the number of groups, standard deviation, percentage change in marginal and conditional
# R², respectively, as well as the results of a log-likelihood test comparing models with and without the random effect in question)
options(knitr.kable.NA = '')
kable(dutch_duration_random,digits=4,row.names=FALSE,col.names=c("Random effect","Groups","Std. Dev.","$\\Delta R^2_{(m)}$%","$\\Delta R^2_{(c)}$%","$\\chi^2$","$df$","$p$",""),caption="Summary of random effects of the linear mixed-effects model of word duration in Dutch\\label{tab:dutch_duration_random_effects}")
options(knitr.kable.NA = 'NA')
```

## English {-}

```{r english_model, echo=FALSE}

# Output a table of fixed effects for the English linear mixed-effects model (including columns for standard error, t values, the standardized beta coefficients, percentage change
# in marginal and conditional R², respectively, as well semi-partial R² and the results of a log-likelihood ratio test comparing models with and without the factor in question)
options(knitr.kable.NA = '')
kable(english_duration_result,digits=4,row.names=TRUE,col.names=c("Coefficient","Std. Error","$t$","$\\beta$","$\\Delta R^2_{(m)}$\\%","$\\Delta R^2_{(c)}$\\%","s.-p. $R^2$","$\\chi^2$","$df$","$p$",""),escape=FALSE,caption="Summary of fixed effects of the linear mixed-effects model of word duration in English\\label{tab:english_duration_fixed_effects}",format="latex",booktabs=TRUE) %>% kable_styling(position="center",latex_options=c("scale_down","hold_position")) %>% footnote(general=paste("n = ", format(summary(english_duration_model)$devcomp$dims[1],big.mark=" "), ", $R^2_{(m)}$ = ",round(r.squaredGLMM(english_duration_model)[1],4), ", $R^2_{(c)}$ = ",round(r.squaredGLMM(english_duration_model)[2],4),sep=""),general_title="",escape=FALSE)
options(knitr.kable.NA = 'NA')

# Output a table of random effects for the English linear mixed-effects model (including columns for the number of groups, standard deviation, percentage change in marginal and conditional
# R², respectively, as well as the results of a log-likelihood test comparing models with and without the random effect in question)
options(knitr.kable.NA = '')
kable(english_duration_random,digits=4,row.names=FALSE,col.names=c("Random effect","Groups","Std. Dev.","$\\Delta R^2_{(m)}$%","$\\Delta R^2_{(c)}$%","$\\chi^2$","$df$","$p$",""),caption="Summary of random effects of the linear mixed-effects model of word duration in English\\label{tab:bora_duration_random_effects}")
options(knitr.kable.NA = 'NA')
```

\pagebreak

## Even {-}

```{r even_model, echo=FALSE}

# Output a table of fixed effects for the Even linear mixed-effects model (including columns for standard error, t values, the standardized beta coefficients, percentage change
# in marginal and conditional R², respectively, as well semi-partial R² and the results of a log-likelihood ratio test comparing models with and without the factor in question)
options(knitr.kable.NA = '')
kable(even_duration_result,digits=4,row.names=TRUE,col.names=c("Coefficient","Std. Error","$t$","$\\beta$","$\\Delta R^2_{(m)}$\\%","$\\Delta R^2_{(c)}$\\%","s.-p. $R^2$","$\\chi^2$","$df$","$p$",""),escape=FALSE,caption="Summary of fixed effects of the linear mixed-effects model of word duration in Even\\label{tab:even_duration_fixed_effects}",format="latex",booktabs=TRUE) %>% kable_styling(position="center",latex_options=c("scale_down","hold_position")) %>% footnote(general=paste("n = ", format(summary(even_duration_model)$devcomp$dims[1],big.mark=" "), ", $R^2_{(m)}$ = ",round(r.squaredGLMM(even_duration_model)[1],4), ", $R^2_{(c)}$ = ",round(r.squaredGLMM(even_duration_model)[2],4),sep=""),general_title="",escape=FALSE)
options(knitr.kable.NA = 'NA')

# Output a table of random effects for the Even linear mixed-effects model (including columns for the number of groups, standard deviation, percentage change in marginal and conditional
# R², respectively, as well as the results of a log-likelihood test comparing models with and without the random effect in question)
options(knitr.kable.NA = '')
kable(even_duration_random,digits=4,row.names=FALSE,col.names=c("Random effect","Groups","Std. Dev.","$\\Delta R^2_{(m)}$%","$\\Delta R^2_{(c)}$%","$\\chi^2$","$df$","$p$",""),caption="Summary of random effects of the linear mixed-effects model of word duration in Even\\label{tab:even_duration_random_effects}")
options(knitr.kable.NA = 'NA')
```

## Hoocąk {-}

```{r hoocak_model, echo=FALSE}

# Output a table of fixed effects for the Hoocąk linear mixed-effects model (including columns for standard error, t values, the standardized beta coefficients, percentage change
# in marginal and conditional R², respectively, as well semi-partial R² and the results of a log-likelihood ratio test comparing models with and without the factor in question)
options(knitr.kable.NA = '')
kable(hoocak_duration_result,digits=4,row.names=TRUE,col.names=c("Coefficient","Std. Error","$t$","$\\beta$","$\\Delta R^2_{(m)}$\\%","$\\Delta R^2_{(c)}$\\%","s.-p. $R^2$","$\\chi^2$","$df$","$p$",""),escape=FALSE,caption="Summary of fixed effects of the linear mixed-effects model of word duration in Hoocąk\\label{tab:hoocak_duration_fixed_effects}",format="latex",booktabs=TRUE) %>% kable_styling(position="center",latex_options=c("scale_down","hold_position")) %>% footnote(general=paste("n = ", format(summary(hoocak_duration_model)$devcomp$dims[1],big.mark=" "), ", $R^2_{(m)}$ = ",round(r.squaredGLMM(hoocak_duration_model)[1],4), ", $R^2_{(c)}$ = ",round(r.squaredGLMM(hoocak_duration_model)[2],4),sep=""),general_title="",escape=FALSE)
options(knitr.kable.NA = 'NA')

# Output a table of random effects for the Hoocąk linear mixed-effects model (including columns for the number of groups, standard deviation, percentage change in marginal and conditional
# R², respectively, as well as the results of a log-likelihood test comparing models with and without the random effect in question)
options(knitr.kable.NA = '')
kable(hoocak_duration_random,digits=4,row.names=FALSE,col.names=c("Random effect","Groups","Std. Dev.","$\\Delta R^2_{(m)}$%","$\\Delta R^2_{(c)}$%","$\\chi^2$","$df$","$p$",""),caption="Summary of random effects of the linear mixed-effects model of word duration in Hoocąk\\label{tab:hoocak_duration_random_effects}")
options(knitr.kable.NA = 'NA')
```

\pagebreak

## Nǁng {-}

```{r nuu_model, echo=FALSE}

# Output a table of fixed effects for the Nǁng linear mixed-effects model (including columns for standard error, t values, the standardized beta coefficients, percentage change
# in marginal and conditional R², respectively, as well semi-partial R² and the results of a log-likelihood ratio test comparing models with and without the factor in question)
options(knitr.kable.NA = '')
kable(nuu_duration_result,digits=4,row.names=TRUE,col.names=c("Coefficient","Std. Error","$t$","$\\beta$","$\\Delta R^2_{(m)}$\\%","$\\Delta R^2_{(c)}$\\%","s.-p. $R^2$","$\\chi^2$","$df$","$p$",""),escape=FALSE,caption="Summary of fixed effects of the linear mixed-effects model of word duration in Nǁng\\label{tab:nuu_duration_fixed_effects}",format="latex",booktabs=TRUE) %>% kable_styling(position="center",latex_options=c("scale_down","hold_position")) %>% footnote(general=paste("n = ", format(summary(nuu_duration_model)$devcomp$dims[1],big.mark=" "), ", $R^2_{(m)}$ = ",round(r.squaredGLMM(nuu_duration_model)[1],4), ", $R^2_{(c)}$ = ",round(r.squaredGLMM(nuu_duration_model)[2],4),sep=""),general_title="",escape=FALSE)
options(knitr.kable.NA = 'NA')

# Output a table of random effects for the Nǁng linear mixed-effects model (including columns for the number of groups, standard deviation, percentage change in marginal and conditional
# R², respectively, as well as the results of a log-likelihood test comparing models with and without the random effect in question)
options(knitr.kable.NA = '')
kable(nuu_duration_random,digits=4,row.names=FALSE,col.names=c("Random effect","Groups","Std. Dev.","$\\Delta R^2_{(m)}$%","$\\Delta R^2_{(c)}$%","$\\chi^2$","$df$","$p$",""),caption="Summary of random effects of the linear mixed-effects model of word duration in Nǁng\\label{tab:nuu_duration_random_effects}")
options(knitr.kable.NA = 'NA')
```

## Sakha {-}

```{r sakha_model, echo=FALSE}

# Output a table of fixed effects for the Sakha linear mixed-effects model (including columns for standard error, t values, the standardized beta coefficients, percentage change
# in marginal and conditional R², respectively, as well semi-partial R² and the results of a log-likelihood ratio test comparing models with and without the factor in question)
options(knitr.kable.NA = '')
kable(sakha_duration_result,digits=4,row.names=TRUE,col.names=c("Coefficient","Std. Error","$t$","$\\beta$","$\\Delta R^2_{(m)}$\\%","$\\Delta R^2_{(c)}$\\%","s.-p. $R^2$","$\\chi^2$","$df$","$p$",""),escape=FALSE,caption="Summary of fixed effects of the linear mixed-effects model of word duration in Sakha\\label{tab:sakha_duration_fixed_effects}",format="latex",booktabs=TRUE) %>% kable_styling(position="center",latex_options=c("scale_down","hold_position")) %>% footnote(general=paste("n = ", format(summary(sakha_duration_model)$devcomp$dims[1],big.mark=" "), ", $R^2_{(m)}$ = ",round(r.squaredGLMM(sakha_duration_model)[1],4), ", $R^2_{(c)}$ = ",round(r.squaredGLMM(sakha_duration_model)[2],4),sep=""),general_title="",escape=FALSE)
options(knitr.kable.NA = 'NA')

# Output a table of random effects for the Sakha linear mixed-effects model (including columns for the number of groups, standard deviation, percentage change in marginal and conditional
# R², respectively, as well as the results of a log-likelihood test comparing models with and without the random effect in question)
options(knitr.kable.NA = '')
kable(sakha_duration_random,digits=4,row.names=FALSE,col.names=c("Random effect","Groups","Std. Dev.","$\\Delta R^2_{(m)}$%","$\\Delta R^2_{(c)}$%","$\\chi^2$","$df$","$p$",""),caption="Summary of random effects of the linear mixed-effects model of word duration in Sakha\\label{tab:sakha_duration_random_effects}")
options(knitr.kable.NA = 'NA')
```

\pagebreak

## Texistepec {-}

```{r texistepec_model, echo=FALSE}

# Output a table of fixed effects for the Texistepec Popoluca linear mixed-effects model (including columns for standard error, t values, the standardized beta coefficients, percentage change
# in marginal and conditional R², respectively, as well semi-partial R² and the results of a log-likelihood ratio test comparing models with and without the factor in question)
options(knitr.kable.NA = '')
kable(texistepec_duration_result,digits=4,row.names=TRUE,col.names=c("Coefficient","Std. Error","$t$","$\\beta$","$\\Delta R^2_{(m)}$\\%","$\\Delta R^2_{(c)}$\\%","s.-p. $R^2$","$\\chi^2$","$df$","$p$",""),escape=FALSE,caption="Summary of fixed effects of the linear mixed-effects model of word duration in Texistepec\\label{tab:texistepec_duration_fixed_effects}",format="latex",booktabs=TRUE) %>% kable_styling(position="center",latex_options=c("scale_down","hold_position")) %>% footnote(general=paste("n = ", format(summary(texistepec_duration_model)$devcomp$dims[1],big.mark=" "), ", $R^2_{(m)}$ = ",round(r.squaredGLMM(texistepec_duration_model)[1],4), ", $R^2_{(c)}$ = ",round(r.squaredGLMM(texistepec_duration_model)[2],4),sep=""),general_title="",escape=FALSE)
options(knitr.kable.NA = 'NA')

# Output a table of random effects for the Texistepec Popoluca linear mixed-effects model (including columns for the number of groups, standard deviation, percentage change in marginal and conditional
# R², respectively, as well as the results of a log-likelihood test comparing models with and without the random effect in question)
options(knitr.kable.NA = '')
kable(texistepec_duration_random,digits=4,row.names=FALSE,col.names=c("Random effect","Groups","Std. Dev.","$\\Delta R^2_{(m)}$%","$\\Delta R^2_{(c)}$%","$\\chi^2$","$df$","$p$",""),caption="Summary of random effects of the linear mixed-effects model of word duration in Texistepec\\label{tab:texistepec_duration_random_effects}")
options(knitr.kable.NA = 'NA')
```


# Distribution of the dependent variable word duration {#appendix_dependent_variable}

```{r word_duration_distribution, echo=FALSE, fig.cap="Distribution of word duration in the ten languages\\label{fig:word_duration_distribution}"}

# Plot the distribution of word duration
ggplot(all_100_tokens,aes(WordLength)) + geom_density() + facet_wrap(~ language, scales="fixed") + ylab("Density") + xlab("Word duration (in seconds)") + theme(legend.title=element_text(size=15),legend.text=element_text(size=15),axis.text=element_text(size=12),axis.title=element_text(size=15),plot.title=element_text(size=17))
```

```{r log_word_duration_distribution, echo=FALSE, fig.cap="Distribution of log word duration in the ten languages\\label{fig:log_word_duration_distribution}"}

# Plot the distribution of log word duration
ggplot(all_100_tokens,aes(log_word_length)) + geom_density() + facet_wrap(~ language, scales="fixed") + ylab("Density") + xlab("Log word duration (in seconds)") + theme(legend.title=element_text(size=15),legend.text=element_text(size=15),axis.text=element_text(size=12),axis.title=element_text(size=15),plot.title=element_text(size=17))
```


# Distribution of independent variables {#appendix_independent_variables}

## Word frequency

```{r absolute_word_frequency_distribution, echo=FALSE, fig.cap="Distribution of absolute word frequencies in the ten languages\\label{fig:absolute_word_frequency_distribution}"}

# Plot the distribution of word frequency
ggplot(all_100_tokens,aes(word_freq)) + geom_density() + facet_wrap(~ language, scales="fixed") + ylab("Density") + xlab("Absolute word frequency") + theme(legend.title=element_text(size=15),legend.text=element_text(size=15),axis.text=element_text(size=12),axis.title=element_text(size=15),plot.title=element_text(size=17))
```

```{r log_absolute_word_frequency_distribution, echo=FALSE, fig.cap="Distribution of log absolute word frequencies in the ten languages\\label{fig:log_absolute_word_frequency_distribution}"}

# Plot the distribution of log word frequency
ggplot(all_100_tokens,aes(log(word_freq))) + geom_density() + facet_wrap(~ language, scales="fixed") + ylab("Density") + xlab("Log absolute word frequency") + theme(legend.title=element_text(size=15),legend.text=element_text(size=15),axis.text=element_text(size=12),axis.title=element_text(size=15),plot.title=element_text(size=17))
```

## Word length

```{r word_length_distribution, echo=FALSE, fig.cap="Distribution of word lengths in the ten languages\\label{fig:word_length_distribution}"}

# Plot the distribution of word length in orthographic characters
ggplot(all_100_tokens,aes(chars_per_word)) + geom_density() + facet_wrap(~ language, scales="fixed") + ylab("Density") + xlab("Word length (in segments)") + theme(legend.title=element_text(size=15),legend.text=element_text(size=15),axis.text=element_text(size=12),axis.title=element_text(size=15),plot.title=element_text(size=17))
```

```{r log_word_length_distribution, echo=FALSE, fig.cap="Distribution of log word lengths in the ten languages\\label{fig:word_length_distribution}"}

# Plot the distribution of log word length in orthographic characters
ggplot(all_100_tokens,aes(log(chars_per_word))) + geom_density() + facet_wrap(~ language, scales="fixed") + ylab("Density") + xlab("Log word length (in segments)") + theme(legend.title=element_text(size=15),legend.text=element_text(size=15),axis.text=element_text(size=12),axis.title=element_text(size=15),plot.title=element_text(size=17))
```

## Number of morphemes per word

```{r morphemes_distribution, echo=FALSE, fig.cap="Distribution of the number of morphemes per word in the ten languages\\label{fig:morpheme_distribution}"}

# Plot the distribution of the number of morphs per word
ggplot(all_100_tokens,aes(morphs_per_word)) + geom_density() + facet_wrap(~ language, scales="fixed") + ylab("Density") + xlab("Morphemes per word") + theme(legend.title=element_text(size=15),legend.text=element_text(size=15),axis.text=element_text(size=12),axis.title=element_text(size=15),plot.title=element_text(size=17))
```

```{r log_morphemes_distribution, echo=FALSE, fig.cap="Distribution of the log number of morphemes per word in the ten languages\\label{fig:log_morpheme_distribution}"}
# Plot the distribution of the log number of morphs per word
ggplot(all_100_tokens,aes(log(morphs_per_word))) + geom_density() + facet_wrap(~ language, scales="fixed") + ylab("Density") + xlab("Log morphemes per word") + theme(legend.title=element_text(size=15),legend.text=element_text(size=15),axis.text=element_text(size=12),axis.title=element_text(size=15),plot.title=element_text(size=17))
```

## Position

```{r position_distribution, echo=FALSE, fig.cap="Distribution of word position in the ten languages\\label{fig:position_distribution}"}

# Plot the distribution of word position
ggplot(all_100_tokens,aes(position_percentage)) + geom_density() + facet_wrap(~ language, scales="fixed") + ylab("Density") + xlab("Word position (0 - 1)") + theme(legend.title=element_text(size=15),legend.text=element_text(size=15),axis.text=element_text(size=12),axis.title=element_text(size=15),plot.title=element_text(size=17))
```

```{r log_position_distribution, echo=FALSE, fig.cap="Distribution of log word position in the ten languages\\label{fig:log_position_distribution}"}

# Plot the distribution of log word position
ggplot(all_100_tokens,aes(log(position_percentage))) + geom_density() + facet_wrap(~ language, scales="fixed") + ylab("Density") + xlab("Log word position") + theme(legend.title=element_text(size=15),legend.text=element_text(size=15),axis.text=element_text(size=12),axis.title=element_text(size=15),plot.title=element_text(size=17))
```

## Word class

```{r word_class_distribution, echo=FALSE, fig.cap="Distribution of word classes in the ten languages\\label{fig:word_class_distribution}"}

# Plot the frequency of parts-of-speech
ggplot(all_100_tokens,aes(word_class,fill=word_class)) + geom_bar() + facet_wrap(~ language, scales="fixed") + ylab("Density") + xlab("Word class") + theme(legend.title=element_text(size=15),legend.text=element_text(size=15),axis.text=element_text(size=12),axis.title=element_text(size=15),plot.title=element_text(size=17)) + pos_fill_scale2 + guides(fill=guide_legend(title="Word class"))
```

## Local speech rate

```{r speech_rate_distribution, echo=FALSE, fig.cap="Distribution of local speech rate in the ten languages\\label{fig:speech_rate_distribution}"}

# Plot the distribution of local articulation speed
ggplot(all_100_tokens,aes(speed_per_record2)) + geom_density() + facet_wrap(~ language, scales="fixed") + ylab("Density") + xlab("Local speech rate") + theme(legend.title=element_text(size=15),legend.text=element_text(size=15),axis.text=element_text(size=12),axis.title=element_text(size=15),plot.title=element_text(size=17))
```

```{r log_speech_rate_distribution, echo=FALSE, fig.cap="Distribution of log local speech rate in the ten languages\\label{fig:log_speech_rate_distribution}"}

# Plot the distribution of local log articulation speed
ggplot(all_100_tokens,aes(log_speed_per_record2)) + geom_density() + facet_wrap(~ language, scales="fixed") + ylab("Density") + xlab("Log local speech rate") + theme(legend.title=element_text(size=15),legend.text=element_text(size=15),axis.text=element_text(size=12),axis.title=element_text(size=15),plot.title=element_text(size=17))
```
